# HPC å¿«é€Ÿå‚è€ƒ

**KAVA åœ¨ HPC é›†ç¾¤ä¸Šçš„å¸¸ç”¨å‘½ä»¤å’Œå·¥ä½œæµç¨‹**

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè®¾ç½®ï¼ˆä»…éœ€ä¸€æ¬¡ï¼‰

**æ–¹æ³• A: ä½¿ç”¨ç³»ç»Ÿ Module CUDA**

```bash
# åˆå§‹åŒ– module
. /usr/share/modules/init/bash
module use --append /home/share/modules/modulefiles

# æ·»åŠ åˆ° ~/.bashrc
cat >> ~/.bashrc << 'EOF'
# KAVA Environment
. /usr/share/modules/init/bash
module use --append /home/share/modules/modulefiles
alias load-kava='module load cuda/11.8.0 anaconda3 && conda activate kava'
EOF

source ~/.bashrc
```

**æ–¹æ³• B: ä½¿ç”¨ Conda å®‰è£… CUDAï¼ˆæ›´çµæ´»ï¼‰**

```bash
# ä¸€é”®åˆ›å»ºç¯å¢ƒ
conda create -n kava python=3.10 \
    cudatoolkit=11.8 \
    pytorch torchvision torchaudio pytorch-cuda=11.8 \
    -c pytorch -c nvidia -y

conda activate kava

# é…ç½®ç¯å¢ƒå˜é‡ï¼ˆè‡ªåŠ¨æ¿€æ´»ï¼‰
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
cat > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh << 'EOF'
#!/bin/bash
export CUDA_HOME=$CONDA_PREFIX
export CUDA_PATH=$CONDA_PREFIX
export PATH=$CONDA_PREFIX/bin:$PATH
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH
EOF
chmod +x $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh

# åˆ›å»º lib64 é“¾æ¥
cd $CONDA_PREFIX && ln -s lib lib64

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
pip install peft wandb bitsandbytes

# é…ç½® HPC å…¬å…±æ¨¡å‹åº“
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
export HF_DATASETS_CACHE=/home/share/models
echo 'export HF_HOME=/home/share/models' >> ~/.bashrc
echo 'export TRANSFORMERS_CACHE=/home/share/models' >> ~/.bashrc
echo 'export HF_DATASETS_CACHE=/home/share/models' >> ~/.bashrc

# éªŒè¯
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
nvcc -V
ls /home/share/models/models--meta-llama--Llama-3.2-1B-Instruct

# æ·»åŠ åˆ«å
echo "alias load-kava='conda activate kava'" >> ~/.bashrc
```

---

### 2. åŠ è½½ç¯å¢ƒ

```bash
load-kava  # ä½¿ç”¨åˆ«å
# æˆ–
module load cuda/11.8.0 anaconda3
conda activate kava

# HPC å…¬å…±æ¨¡å‹åº“ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœæœªå†™å…¥ ~/.bashrcï¼‰
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
export HF_DATASETS_CACHE=/home/share/models
```

### 3. æäº¤ä»»åŠ¡

```bash
# å•ä¸ªé…ç½®
sbatch --export=CONFIG=llama1b_aug submit_multi_seed.slurm

# æ‰¹é‡æäº¤
./hpc_run_all.sh llama1b_aug qwen05b_aug
```

---

## ï¿½ HPC å…¬å…±æ¨¡å‹åº“

### ç®€ä»‹

HPC é›†ç¾¤ç»´æŠ¤äº†ä¸€ä¸ªå…±äº«æ¨¡å‹åº“ï¼Œä½äº `/home/share/models`ï¼ŒåŒ…å«ä» HuggingFace ä¸‹è½½çš„å¸¸ç”¨å¼€æºæ¨¡å‹ã€‚

**ä¼˜åŠ¿**ï¼š
- âœ… **å¿«é€Ÿå¯åŠ¨**ï¼šæ— éœ€ç­‰å¾…æ¨¡å‹ä¸‹è½½ï¼Œç«‹å³å¼€å§‹è®­ç»ƒ
- âœ… **èŠ‚çœç©ºé—´**ï¼šå¤šç”¨æˆ·å…±äº«ï¼Œæ— éœ€æ¯äººä¸‹è½½
- âœ… **æŒç»­æ›´æ–°**ï¼šç®¡ç†å‘˜å®šæœŸæ›´æ–°æœ€æ–°æ¨¡å‹
- âœ… **ç¨³å®šå¯é **ï¼šé¿å…ç½‘ç»œè¶…æ—¶é—®é¢˜

---

### é…ç½®æ–¹æ³•

#### æ–¹æ³• 1: æ°¸ä¹…é…ç½®ï¼ˆæ¨èï¼‰

```bash
# æ·»åŠ åˆ° ~/.bashrcï¼ˆä»…éœ€æ‰§è¡Œä¸€æ¬¡ï¼‰
cat >> ~/.bashrc << 'EOF'
# HuggingFace å…¬å…±æ¨¡å‹åº“
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
export HF_DATASETS_CACHE=/home/share/models
EOF

# ç«‹å³ç”Ÿæ•ˆ
source ~/.bashrc
```

#### æ–¹æ³• 2: åœ¨ SLURM è„šæœ¬ä¸­é…ç½®

```bash
# åœ¨ submit_*.slurm è„šæœ¬ä¸­æ·»åŠ 
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
export HF_DATASETS_CACHE=/home/share/models

# æˆ–å†™å…¥ Conda ç¯å¢ƒæ¿€æ´»è„šæœ¬
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
cat > $CONDA_PREFIX/etc/conda/activate.d/hf_models.sh << 'EOF'
#!/bin/bash
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
export HF_DATASETS_CACHE=/home/share/models
EOF
chmod +x $CONDA_PREFIX/etc/conda/activate.d/hf_models.sh
```

---

### å¯ç”¨æ¨¡å‹åˆ—è¡¨

```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹
ls -lh /home/share/models/models--*

# æŸ¥çœ‹ KAVA é¡¹ç›®æ‰€éœ€æ¨¡å‹
ls -lh /home/share/models/models--meta-llama--Llama-3.2-1B-Instruct
ls -lh /home/share/models/models--meta-llama--Llama-3.2-3B-Instruct
ls -lh /home/share/models/models--Qwen--Qwen2.5-0.5B-Instruct

# æŸ¥çœ‹æ¨¡å‹è¯¦æƒ…
tree -L 2 /home/share/models/models--meta-llama--Llama-3.2-1B-Instruct
```

**å¸¸ç”¨æ¨¡å‹**ï¼š
- `meta-llama/Llama-3.2-1B-Instruct` - LLaMA 1Bï¼ˆKAVA è®ºæ–‡ï¼‰
- `meta-llama/Llama-3.2-3B-Instruct` - LLaMA 3Bï¼ˆKAVA è®ºæ–‡ï¼‰
- `Qwen/Qwen2.5-0.5B-Instruct` - Qwen 0.5Bï¼ˆKAVA è®ºæ–‡ï¼‰

---

### ä½¿ç”¨ç¤ºä¾‹

#### åœ¨ Python ä»£ç ä¸­ä½¿ç”¨

```python
import os
from transformers import AutoModelForCausalLM, AutoTokenizer

# æ–¹æ³• 1: ç¯å¢ƒå˜é‡å·²é…ç½®ï¼ˆæ¨èï¼‰
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
# è‡ªåŠ¨ä» /home/share/models åŠ è½½

# æ–¹æ³• 2: æ˜¾å¼æŒ‡å®šç¼“å­˜ç›®å½•
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-1B-Instruct",
    cache_dir="/home/share/models"
)

# æ–¹æ³• 3: ç›´æ¥ä½¿ç”¨æœ¬åœ°è·¯å¾„
model = AutoModelForCausalLM.from_pretrained(
    "/home/share/models/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/..."
)
```

#### åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨

```bash
# é…ç½®ç¯å¢ƒå˜é‡åç›´æ¥è¿è¡Œ
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models

# è®­ç»ƒï¼ˆè‡ªåŠ¨ä»å…±äº«è·¯å¾„åŠ è½½ï¼‰
python train.py --config configs/llama1b_aug.yaml

# æ¨ç†
python inference.py --model_path outputs/llama1b_aug_seed_42
```

---

### éªŒè¯é…ç½®

```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo "HF_HOME=$HF_HOME"
echo "TRANSFORMERS_CACHE=$TRANSFORMERS_CACHE"
echo "HF_DATASETS_CACHE=$HF_DATASETS_CACHE"

# éªŒè¯æ¨¡å‹å¯è®¿é—®
python -c "
import os
print('HF_HOME:', os.environ.get('HF_HOME'))
print('Model exists:', os.path.exists('/home/share/models/models--meta-llama--Llama-3.2-1B-Instruct'))
"

# æµ‹è¯•åŠ è½½æ¨¡å‹ï¼ˆä¸ä¼šä¸‹è½½ï¼‰
python -c "
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B-Instruct')
print('âœ… Successfully loaded from shared cache')
"
```

---

### æ•…éšœæ’é™¤

**é—®é¢˜ 1: ä»ç„¶å°è¯•ä¸‹è½½æ¨¡å‹**

```bash
# ç—‡çŠ¶ï¼šçœ‹åˆ° "Downloading model..." æç¤º

# è§£å†³ï¼šç¡®è®¤ç¯å¢ƒå˜é‡å·²è®¾ç½®
echo $HF_HOME  # åº”è¯¥è¾“å‡º /home/share/models

# å¦‚æœä¸ºç©ºï¼Œé‡æ–°é…ç½®
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
```

**é—®é¢˜ 2: æƒé™æ‹’ç»**

```bash
# ç—‡çŠ¶ï¼šPermission denied

# è§£å†³ï¼šç¡®è®¤è·¯å¾„å¯è®¿é—®
ls -ld /home/share/models  # åº”è¯¥æ˜¾ç¤º drwxr-xr-x

# å¦‚æœæ— æƒé™ï¼Œè”ç³»ç®¡ç†å‘˜
```

**é—®é¢˜ 3: æ¨¡å‹ä¸å­˜åœ¨**

```bash
# ç—‡çŠ¶ï¼šModel not found in /home/share/models

# è§£å†³ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
ls /home/share/models/models--*/

# å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·æ±‚ç®¡ç†å‘˜æ·»åŠ 
# æˆ–ä¸´æ—¶ä½¿ç”¨ HF_ENDPOINT é•œåƒä¸‹è½½åˆ°ä¸ªäººç›®å½•
export HF_ENDPOINT=https://hf-mirror.com
export HF_HOME=$HOME/.cache/huggingface
```

---

## ï¿½ğŸ“‹ SLURM å¸¸ç”¨å‘½ä»¤

### SLURM æ¶æ„è¯´æ˜

```
ç™»å½•è·³æ¿æœº â†’ æäº¤ä½œä¸š â†’ SLURM è°ƒåº¦å™¨ â†’ åˆ†é…åˆ° GPU èŠ‚ç‚¹
```

**é‡è¦**: 
- âš ï¸ å¤§éƒ¨åˆ†èŠ‚ç‚¹ç¦ç”¨ SSHï¼ˆé˜²æ­¢èµ„æºæŠ¢å ï¼‰
- âœ… å¼€æ”¾ SSH çš„èŠ‚ç‚¹ï¼š`gpu10~gpu14`
- âš ï¸ ç¦æ­¢ä½¿ç”¨ `sleep 7 day` ç­‰æ–¹å¼æŠ¢å èµ„æº

---

### é›†ç¾¤çŠ¶æ€æŸ¥è¯¢

#### sinfo - æŸ¥çœ‹é›†ç¾¤çŠ¶æ€

```bash
# åŸºæœ¬æŸ¥çœ‹
sinfo

# è¯¦ç»†ä¿¡æ¯ï¼ˆæ¨èï¼‰
# èŠ‚ç‚¹åç§° | çŠ¶æ€ | CPU(å·²åˆ†é…/å¯ç”¨/å…¶ä»–/æ€») | CPUè´Ÿè½½ | å¯ç”¨å†…å­˜ | æ€»å†…å­˜ | GPU
sinfo -N -o "%5N  %5t  %13C  %8O  %8e  %7m  %G"

# æŸ¥çœ‹ç‰¹å®šåˆ†åŒº
sinfo -p gpu
sinfo -p compute

# æŸ¥çœ‹èŠ‚ç‚¹è¯¦æƒ…
scontrol show node gpu06
```

**èŠ‚ç‚¹çŠ¶æ€è¯´æ˜**ï¼š

| çŠ¶æ€ | è¯´æ˜ | æ˜¯å¦å¯ç”¨ |
|------|------|---------|
| `idle` | èŠ‚ç‚¹ç©ºé—² | âœ… å¯æäº¤ |
| `mix` | èµ„æºéƒ¨åˆ†åˆ†é… | âœ… å¯æäº¤ |
| `alloc` | èµ„æºå®Œå…¨åˆ†é… | âŒ ç­‰å¾… |
| `down` | èŠ‚ç‚¹ä¸‹çº¿ | âŒ ä¸å¯ç”¨ |
| `drain` | èŠ‚ç‚¹æ•…éšœ | âŒ ä¸å¯ç”¨ |
| `drng` | æ•…éšœä½†ä½œä¸šç»§ç»­ | âš ï¸ ä¸å»ºè®® |
| `comp` | æ­£åœ¨æ¸…ç† | âš ï¸ ç­‰å¾… |

#### scir-watch - æŸ¥çœ‹ GPU çŠ¶æ€ï¼ˆæ¨èï¼‰

```bash
# æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹ GPU çŠ¶æ€
scir-watch -s

# è¾“å‡ºï¼šGPUåç§° | è´¹ç”¨ | ç©ºé—²å¡æ•° | æ‰€åœ¨èŠ‚ç‚¹

# æŸ¥çœ‹ç‰¹å®šèŠ‚ç‚¹çš„ GPU ä½¿ç”¨æƒ…å†µ
scir-watch gpu06 gpustat
scir-watch gpu10 gpustat
```

---

### ä»»åŠ¡æäº¤

#### srun - äº¤äº’å¼ä½œä¸šï¼ˆå®æ—¶æ‰§è¡Œï¼‰

```bash
# åŸºæœ¬ç”¨æ³•
srun <å‘½ä»¤>

# åœ¨è®¡ç®—èŠ‚ç‚¹è¿è¡Œå‘½ä»¤
srun nvidia-smi
srun python --version

# ç”³è¯· GPU å¹¶å¯åŠ¨äº¤äº’å¼ Shell
srun --gres=gpu:a100-sxm4-80gb:1 --pty bash -i

# ç”³è¯· 4 å¡ A100 80GB
srun --gres=gpu:a100-sxm4-80gb:4 --pty bash -i

# æŒ‡å®šèŠ‚ç‚¹ï¼ˆgpu10-gpu14 æ”¯æŒ SSHï¼‰
srun -w gpu10 --gres=gpu:a100-sxm4-80gb:2 --pty bash -i

# å®Œæ•´å‚æ•°ç¤ºä¾‹
srun -p compute \                    # åˆ†åŒº
     -N 1 \                          # èŠ‚ç‚¹æ•°
     -w gpu12 \                      # æŒ‡å®šèŠ‚ç‚¹
     --gres=gpu:a100-sxm4-80gb:4 \   # GPU ç±»å‹å’Œæ•°é‡
     --mem=128G \                    # å†…å­˜
     --cpus-per-task=16 \            # CPU æ•°é‡
     --time=2:00:00 \                # æ—¶é—´é™åˆ¶
     --pty bash -i                   # äº¤äº’å¼ Shell
```

**GPU ç±»å‹è¯´æ˜**ï¼š
- `gpu:a100-sxm4-80gb:N` - A100 80GBï¼ˆN ä¸ºæ•°é‡ï¼‰
- `gpu:a100-pcie-40gb:N` - A100 40GB
- `gpu:v100:N` - V100
- æŸ¥çœ‹å¯ç”¨ç±»å‹ï¼š`sinfo -o "%G"` æˆ– `scir-watch -s`

#### sbatch - æ‰¹é‡ä½œä¸šï¼ˆè„šæœ¬æäº¤ï¼‰

```bash
# åŸºæœ¬æäº¤
sbatch run.sh

# å¸¦å‚æ•°æäº¤
sbatch --export=CONFIG=llama1b_aug,SEED=42 run.sh

# æ•°ç»„ä½œä¸šï¼ˆæ‰¹é‡è¿è¡Œï¼‰
sbatch --array=0-9 run.sh         # 10 ä¸ªä»»åŠ¡
sbatch --array=0-2%1 run.sh       # 3 ä¸ªä»»åŠ¡ï¼Œæ¯æ¬¡åªè¿è¡Œ 1 ä¸ª

# ä¾èµ–å…³ç³»
JOB1=$(sbatch --parsable train.sh)
sbatch --dependency=afterok:$JOB1 eval.sh
```

**æ ‡å‡† SLURM è„šæœ¬æ¨¡æ¿**ï¼š

```bash
#!/bin/bash
#SBATCH -J kava-train              # ä½œä¸šå
#SBATCH -o logs/train_%j.out       # stdout è¾“å‡ºï¼ˆ%j = job IDï¼‰
#SBATCH -e logs/train_%j.err       # stderr è¾“å‡º
#SBATCH -p compute                 # åˆ†åŒº
#SBATCH -N 1                       # èŠ‚ç‚¹æ•°
#SBATCH -n 1                       # ä»»åŠ¡æ•°
#SBATCH --cpus-per-task=8          # æ¯ä»»åŠ¡ CPU æ•°
#SBATCH --mem=64G                  # å†…å­˜
#SBATCH -t 48:00:00                # æ—¶é—´é™åˆ¶ï¼ˆ48å°æ—¶ï¼‰
#SBATCH --gres=gpu:a100-sxm4-80gb:1  # GPU èµ„æº
# #SBATCH -w gpu10                 # æŒ‡å®šèŠ‚ç‚¹ï¼ˆå¯é€‰ï¼‰

# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p logs

# åŠ è½½ç¯å¢ƒ
. $HOME/miniconda3/etc/profile.d/conda.sh
conda activate kava

# éªŒè¯ç¯å¢ƒ
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Date: $(date)"
nvidia-smi

# è¿è¡Œç¨‹åº
python train.py --config configs/llama1b_aug.yaml

echo "Job finished at $(date)"
```

#### salloc - èµ„æºé¢„åˆ†é…

```bash
# é¢„åˆ†é…èµ„æº
salloc --gres=gpu:a100-sxm4-80gb:4 --time=2:00:00

# åœ¨åˆ†é…çš„èµ„æºä¸Šè¿è¡Œå‘½ä»¤
srun <å‘½ä»¤>

# é‡Šæ”¾èµ„æº
exit
```

---

### ä»»åŠ¡ç›‘æ§

#### squeue - æŸ¥çœ‹ä½œä¸šé˜Ÿåˆ—

```bash
# æŸ¥çœ‹æ‰€æœ‰ä½œä¸š
squeue

# ä»…æŸ¥çœ‹è‡ªå·±çš„ä½œä¸šï¼ˆæ¨èï¼‰
squeue --me
squeue -u $USER

# è¯¦ç»†è¾“å‡ºæ ¼å¼
squeue -u $USER -o "%.18i %.9P %.30j %.8T %.10M %.6D %R"
# è¾“å‡ºï¼šJobID | åˆ†åŒº | ä½œä¸šå | çŠ¶æ€ | è¿è¡Œæ—¶é—´ | èŠ‚ç‚¹æ•° | åŸå› 

# æŒç»­ç›‘æ§ï¼ˆæ¯ 10 ç§’åˆ·æ–°ï¼‰
watch -n 10 'squeue --me'

# æŸ¥çœ‹ç‰¹å®šä½œä¸š
squeue -j <JOB_ID>

# æŒ‰çŠ¶æ€è¿‡æ»¤
squeue --me --state=RUNNING
squeue --me --state=PENDING
```

**ä½œä¸šçŠ¶æ€**ï¼š`PD`(ç­‰å¾…) | `R`(è¿è¡Œ) | `CG`(å®Œæˆä¸­) | `CD`(å·²å®Œæˆ) | `F`(å¤±è´¥) | `CA`(å–æ¶ˆ)

#### æŸ¥çœ‹ä½œä¸šè¯¦æƒ…

```bash
# è¯¦ç»†ä¿¡æ¯
scontrol show job <JOB_ID>

# æŸ¥çœ‹è¾“å‡º
tail -f logs/train_<JOB_ID>.out
tail -f logs/train_<JOB_ID>.err

# ç›‘æ§ GPU ä½¿ç”¨
scir-watch gpu06 gpustat
```

---
squeue -u $USER

# è¯¦ç»†è¾“å‡º
squeue -u $USER -o "%.18i %.9P %.30j %.8T %.10M %.6D %R"

# æŒç»­ç›‘æ§
watch -n 10 'squeue -u $USER'

# æŸ¥çœ‹ç‰¹å®šä»»åŠ¡
scontrol show job <JOB_ID>

# æŸ¥çœ‹æ•°ç»„ä»»åŠ¡
squeue -j <ARRAY_JOB_ID>
```

### ä»»åŠ¡ç®¡ç†

```bash
# å–æ¶ˆä»»åŠ¡
scancel <JOB_ID>

# å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
scancel -u $USER

# å–æ¶ˆç‰¹å®šåç§°çš„ä»»åŠ¡
scancel -n kava-train

# å–æ¶ˆæ•°ç»„ä»»åŠ¡çš„ç‰¹å®šå­ä»»åŠ¡
scancel <ARRAY_JOB_ID>_<INDEX>

# æš‚åœä»»åŠ¡
scontrol hold <JOB_ID>

# æ¢å¤ä»»åŠ¡
scontrol release <JOB_ID>
```

### å†å²æŸ¥è¯¢

```bash
# æŸ¥çœ‹å·²å®Œæˆä»»åŠ¡
sacct -u $USER

# è¯¦ç»†ä¿¡æ¯
sacct -j <JOB_ID> --format=JobID,JobName,Partition,State,ExitCode,Elapsed,MaxRSS,MaxVMSize

# æœ€è¿‘ 24 å°æ—¶
sacct -S $(date -d '1 day ago' +%Y-%m-%d) -u $USER

# ç‰¹å®šæ—¶é—´èŒƒå›´
sacct -S 2024-01-01 -E 2024-01-31 -u $USER
```

### èµ„æºæŸ¥è¯¢

```bash
# æŸ¥çœ‹åˆ†åŒºä¿¡æ¯
sinfo
sinfo -p gpu

# æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€
sinfo -N
scontrol show node <NODE_NAME>

# æŸ¥çœ‹é…é¢
sacctmgr show user $USER
sacctmgr show association user=$USER
```

---

## ğŸ”§ Module å‘½ä»¤

### åŸºæœ¬æ“ä½œ

```bash
# æŸ¥çœ‹å¯ç”¨æ¨¡å—
module avail

# æœç´¢æ¨¡å—
module avail cuda
module spider pytorch

# åŠ è½½æ¨¡å—
module load cuda/11.8.0
module load anaconda3

# å¸è½½æ¨¡å—
module unload cuda

# åˆ‡æ¢ç‰ˆæœ¬
module swap cuda/11.8.0 cuda/12.1.1

# æŸ¥çœ‹å·²åŠ è½½æ¨¡å—
module list

# æ¸…é™¤æ‰€æœ‰æ¨¡å—
module purge
```

### CUDA æ¨¡å—

```bash
# æŸ¥çœ‹å¯ç”¨ CUDA ç‰ˆæœ¬
module avail cuda

# åŠ è½½æŒ‡å®šç‰ˆæœ¬
module load cuda/11.8.0

# éªŒè¯
nvcc -V
nvidia-smi

# æŸ¥çœ‹æ¨¡å—è¯¦æƒ…
module show cuda/11.8.0
```

---

## ğŸ“Š ç›‘æ§å’Œè°ƒè¯•

### å®æ—¶ç›‘æ§

```bash
# ç›‘æ§ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# ç›‘æ§ä»»åŠ¡è¾“å‡º
tail -f logs/kava_*.out

# ç›‘æ§é”™è¯¯æ—¥å¿—
tail -f logs/kava_*.err

# åŒæ—¶ç›‘æ§å¤šä¸ªæ–‡ä»¶
tail -f logs/kava_{12345,12346,12347}.out
```

### æ€§èƒ½åˆ†æ

```bash
# æŸ¥çœ‹ä»»åŠ¡èµ„æºä½¿ç”¨
sacct -j <JOB_ID> --format=JobID,JobName,MaxRSS,MaxVMSize,Elapsed

# æŸ¥çœ‹ GPU ä½¿ç”¨å†å²
ssh <NODE_NAME>  # ç™»å½•åˆ°è®¡ç®—èŠ‚ç‚¹
nvidia-smi dmon -i 0 -s puc  # ç›‘æ§ GPU 0
```

### è°ƒè¯•ä»»åŠ¡

```bash
# äº¤äº’å¼ä¼šè¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
srun --pty --gres=gpu:1 --mem=32G bash

# åœ¨äº¤äº’ä¼šè¯ä¸­æµ‹è¯•
python train.py --config configs/llama1b_aug.yaml --quick_test

# æŸ¥çœ‹ä»»åŠ¡æ ‡å‡†è¾“å‡º
cat logs/kava_<JOB_ID>.out

# æŸ¥çœ‹ä»»åŠ¡é”™è¯¯è¾“å‡º
cat logs/kava_<JOB_ID>.err

# æœç´¢é”™è¯¯
grep -i error logs/kava_*.err
grep -i "out of memory" logs/kava_*.err
```

---

## ğŸ“ æ–‡ä»¶ç®¡ç†

### æ•°æ®ä¼ è¾“

```bash
# ä»æœ¬åœ°ä¸Šä¼ åˆ° HPC
scp -r kava/ username@hpc.example.edu:~/

# ä» HPC ä¸‹è½½åˆ°æœ¬åœ°
scp -r username@hpc.example.edu:~/kava/outputs/ ./

# ä½¿ç”¨ rsyncï¼ˆå¢é‡åŒæ­¥ï¼‰
rsync -avz --progress kava/ username@hpc.example.edu:~/kava/
rsync -avz --progress username@hpc.example.edu:~/kava/outputs/ ./outputs/
```

### ç£ç›˜é…é¢

```bash
# æŸ¥çœ‹é…é¢
quota -s

# æŸ¥çœ‹ç›®å½•å¤§å°
du -sh outputs/
du -h --max-depth=1 outputs/

# æ¸…ç†ç©ºé—´
# åˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹
find outputs/ -name "checkpoint-*" -type d -mtime +30 -exec rm -rf {} +

# å‹ç¼©ç»“æœ
tar -czf outputs_backup.tar.gz outputs/
```

---

## ğŸ”„ å·¥ä½œæµç¨‹ç¤ºä¾‹

### å®Œæ•´å®éªŒæµç¨‹

```bash
# 1. ç™»å½• HPC
ssh username@hpc.example.edu

# 2. è¿›å…¥é¡¹ç›®ç›®å½•
cd ~/kava

# 3. åŠ è½½ç¯å¢ƒ
load-kava

# 4. æäº¤è®­ç»ƒä»»åŠ¡
./hpc_run_all.sh llama1b_aug

# 5. ç›‘æ§ä»»åŠ¡
watch -n 10 'squeue -u $USER'

# 6. æŸ¥çœ‹æ—¥å¿—
tail -f logs/kava_*.out

# 7. ä»»åŠ¡å®ŒæˆåæŸ¥çœ‹ç»“æœ
cat outputs/llama1b_aug_multi_seed/aggregated_results.yaml

# 8. ç”Ÿæˆè¡¨æ ¼
python format_results.py --input_dir outputs/

# 9. ä¸‹è½½ç»“æœåˆ°æœ¬åœ°
exit  # é€€å‡º HPC
scp -r username@hpc.example.edu:~/kava/outputs/ ./
scp username@hpc.example.edu:~/kava/kava_tables.tex ./
```

### æ‰¹é‡æäº¤å¤šä¸ªé…ç½®

```bash
# æ–¹æ³• 1: ä½¿ç”¨è„šæœ¬
./hpc_run_all.sh llama1b_aug llama1b_aug_nl qwen05b_aug llama3b_aug

# æ–¹æ³• 2: å¾ªç¯æäº¤
for config in llama1b_aug llama1b_aug_nl qwen05b_aug llama3b_aug; do
    echo "Submitting $config"
    sbatch --export=CONFIG=$config submit_multi_seed.slurm
    sleep 1
done

# æ–¹æ³• 3: ä»»åŠ¡é“¾
JOB1=$(sbatch --parsable --export=CONFIG=llama1b_aug submit_multi_seed.slurm)
JOB2=$(sbatch --parsable --export=CONFIG=llama1b_aug_nl submit_multi_seed.slurm)
JOB3=$(sbatch --parsable --export=CONFIG=qwen05b_aug submit_multi_seed.slurm)

# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆåèšåˆ
sbatch --dependency=afterok:$JOB1:$JOB2:$JOB3 submit_aggregate_all.slurm
```

---

## âš™ï¸ é…ç½®ä¼˜åŒ–

### èµ„æºè¯·æ±‚ä¼˜åŒ–

```bash
# LLaMA-1B (å°æ¨¡å‹)
#SBATCH --gres=gpu:1
#SBATCH --mem=64G
#SBATCH --time=24:00:00

# LLaMA-3B (ä¸­ç­‰æ¨¡å‹)
#SBATCH --gres=gpu:1
#SBATCH --mem=96G
#SBATCH --time=48:00:00

# Qwen-0.5B (æœ€å°æ¨¡å‹)
#SBATCH --gres=gpu:1
#SBATCH --mem=32G
#SBATCH --time=12:00:00
```

### å¹¶è¡Œç­–ç•¥

```bash
# ç­–ç•¥ 1: æ•°æ®å¹¶è¡Œï¼ˆå•æœºå¤šå¡ï¼‰
#SBATCH --gres=gpu:4
python -m torch.distributed.launch --nproc_per_node=4 train.py

# ç­–ç•¥ 2: æ•°ç»„ä½œä¸šï¼ˆå¤šä¸ªç‹¬ç«‹ä»»åŠ¡ï¼‰
#SBATCH --array=0-11  # 4 configs Ã— 3 seeds

# ç­–ç•¥ 3: ç®¡é“å¹¶è¡Œï¼ˆå¤§æ¨¡å‹ï¼‰
#SBATCH --gres=gpu:2
# åœ¨ä»£ç ä¸­ä½¿ç”¨ model.parallelize()
```

---

## ğŸ“ è·å–å¸®åŠ©

```bash
# SLURM å¸®åŠ©
man sbatch
man squeue
man sacct

# Module å¸®åŠ©
module help
module show <MODULE>

# è”ç³»ç®¡ç†å‘˜
# æŸ¥çœ‹é›†ç¾¤å…¬å‘Š
cat /etc/motd

# æäº¤æ”¯æŒå·¥å•
# support@hpc.example.edu
```

---

## ğŸŒ SSH ç«¯å£æ˜ å°„

### æ­£å‘æ˜ å°„ (Local Port Forwarding)

å°†è¿œç¨‹ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ï¼Œæ–¹ä¾¿è®¿é—®è¿œç¨‹æœåŠ¡ï¼ˆå¦‚ TensorBoardã€Jupyterï¼‰ã€‚

#### åŸºæœ¬ç”¨æ³•

```bash
# è¯­æ³•
ssh -L [æœ¬åœ°ç«¯å£]:localhost:[è¿œç¨‹ç«¯å£] [è¿œç¨‹ä¸»æœº]

# ç¤ºä¾‹ï¼šæ˜ å°„ HPC TensorBoard (6006) åˆ°æœ¬åœ° 22222
ssh -L 22222:localhost:6006 hpc

# ç„¶ååœ¨æœ¬åœ°æµè§ˆå™¨è®¿é—®
# http://localhost:22222
```

#### åå°è¿è¡Œï¼ˆä¸ç™»å½•ï¼‰

```bash
# ä½¿ç”¨ -N é€‰é¡¹ä»…åšç«¯å£æ˜ å°„ï¼Œä¸æ‰“å¼€äº¤äº’å¼ shell
ssh -N -L 22222:localhost:6006 hpc

# åå°è¿è¡Œ
ssh -N -L 22222:localhost:6006 hpc &

# æˆ–ä½¿ç”¨ nohup
nohup ssh -N -L 22222:localhost:6006 hpc > /dev/null 2>&1 &
```

#### å¸¸è§åº”ç”¨åœºæ™¯

```bash
# 1. TensorBoard
ssh -N -L 6006:localhost:6006 hpc
# æœ¬åœ°è®¿é—®: http://localhost:6006

# 2. Jupyter Notebook (å‡è®¾è¿œç¨‹è¿è¡Œåœ¨ 8888)
ssh -N -L 8888:localhost:8888 hpc
# æœ¬åœ°è®¿é—®: http://localhost:8888

# 3. Jupyter æ˜ å°„åˆ°ä¸åŒæœ¬åœ°ç«¯å£
ssh -N -L 9999:localhost:8888 hpc
# æœ¬åœ°è®¿é—®: http://localhost:9999

# 4. WandB Local Server
ssh -N -L 8080:localhost:8080 hpc
# æœ¬åœ°è®¿é—®: http://localhost:8080

# 5. VS Code Server
ssh -N -L 8000:localhost:8000 hpc
```

#### å¤šç«¯å£æ˜ å°„

```bash
# åŒæ—¶æ˜ å°„å¤šä¸ªç«¯å£
ssh -N \
    -L 6006:localhost:6006 \
    -L 8888:localhost:8888 \
    -L 8080:localhost:8080 \
    hpc
```

#### VSCode Remote SSH è‡ªåŠ¨æ˜ å°„

VSCode çš„ Remote SSH æ’ä»¶å…·æœ‰**è‡ªåŠ¨ç«¯å£è½¬å‘**åŠŸèƒ½ï¼š

1. åœ¨è¿œç¨‹è¿è¡ŒæœåŠ¡ï¼ˆå¦‚ `tensorboard --logdir runs --port 6006`ï¼‰
2. VSCode è‡ªåŠ¨æ£€æµ‹å¹¶æç¤ºè½¬å‘ç«¯å£
3. ç‚¹å‡»é€šçŸ¥ä¸­çš„"è½¬å‘ç«¯å£"æˆ–åœ¨"ç«¯å£"é¢æ¿æ‰‹åŠ¨æ·»åŠ 
4. è‡ªåŠ¨æ˜ å°„åˆ°æœ¬åœ°ç›¸åŒç«¯å£

---

### åå‘æ˜ å°„ (Remote Port Forwarding)

å°†æœ¬åœ°ç«¯å£æ˜ å°„åˆ°è¿œç¨‹ï¼Œè®©è¿œç¨‹è®¿é—®æœ¬åœ°æœåŠ¡ï¼ˆå¦‚ä»£ç†ã€æ•°æ®åº“ï¼‰ã€‚

#### åŸºæœ¬ç”¨æ³•

```bash
# è¯­æ³•
ssh -R [è¿œç¨‹ç«¯å£]:localhost:[æœ¬åœ°ç«¯å£] [è¿œç¨‹ä¸»æœº]

# ç¤ºä¾‹ï¼šå°†æœ¬åœ° Clash (7890) æ˜ å°„åˆ° HPC çš„ 55555
ssh -R 55555:localhost:7890 hpc
```

#### ä½¿ç”¨æœ¬åœ°ä»£ç†åŠ é€Ÿ HPC ä¸‹è½½

**åœºæ™¯**: HPC è®¿é—® HuggingFace/GitHub ç¼“æ…¢ï¼Œä½¿ç”¨æœ¬åœ°ä»£ç†åŠ é€Ÿã€‚

##### Step 1: é…ç½®æœ¬åœ°ä»£ç†

```bash
# Clash for Windows
1. æ‰“å¼€ Clash
2. å¯ç”¨ "Allow LAN" é€‰é¡¹
3. è®°ä½ç«¯å£å·ï¼ˆé»˜è®¤ 7890ï¼‰

# Shadowrocket (macOS)
# é»˜è®¤ç«¯å£ 1089
```

##### Step 2: å»ºç«‹åå‘éš§é“

```bash
# æ˜ å°„æœ¬åœ° Clash (7890) åˆ° HPC çš„ 55555
ssh -R 55555:localhost:7890 hpc

# æˆ–åå°è¿è¡Œ
ssh -N -R 55555:localhost:7890 hpc &

# ä½¿ç”¨ Shadowrocket
ssh -N -R 55555:localhost:1089 hpc &
```

##### Step 3: åœ¨ HPC é…ç½®ä»£ç†

```bash
# åœ¨ HPC ç»ˆç«¯è®¾ç½®ä»£ç†ç¯å¢ƒå˜é‡
export http_proxy=http://localhost:55555
export https_proxy=http://localhost:55555
export all_proxy=http://localhost:55555

# æµ‹è¯•ä»£ç†è¿æ¥
curl -I https://www.google.com
curl https://huggingface.co

# ä¸‹è½½ Google ä¸»é¡µæµ‹è¯•
wget https://www.google.com -O google.html
cat google.html

# å¦‚æœæˆåŠŸï¼Œå¯ä»¥çœ‹åˆ° Google HTML å†…å®¹
```

##### Step 4: æ°¸ä¹…é…ç½®ï¼ˆå¯é€‰ï¼‰

```bash
# å†™å…¥ ~/.bashrcï¼ˆæ¯æ¬¡ç™»å½•è‡ªåŠ¨ç”Ÿæ•ˆï¼‰
cat >> ~/.bashrc << 'EOF'
# ä»£ç†é…ç½® (éœ€è¦æœ¬åœ°å…ˆå»ºç«‹ SSH åå‘éš§é“)
# ssh -N -R 55555:localhost:7890 hpc
export http_proxy=http://localhost:55555
export https_proxy=http://localhost:55555
export all_proxy=http://localhost:55555
EOF

source ~/.bashrc
```

##### Step 5: åœ¨ SLURM ä½œä¸šä¸­ä½¿ç”¨

```bash
# åœ¨ submit_*.slurm è„šæœ¬ä¸­æ·»åŠ 
export http_proxy=http://localhost:55555
export https_proxy=http://localhost:55555
export all_proxy=http://localhost:55555

# ç„¶åæ­£å¸¸è¿è¡Œ
python train.py --config configs/llama1b_aug.yaml
```

#### æ•…éšœæ’é™¤

**é—®é¢˜ 1: ç«¯å£å·²å ç”¨**

```bash
# ç—‡çŠ¶
bind: Address already in use
channel_setup_fwd_listener_tcpip: cannot listen to port: 55555

# è§£å†³ï¼šä½¿ç”¨å…¶ä»–ç«¯å£ï¼ˆå»ºè®® 50000-65535ï¼‰
ssh -N -R 56789:localhost:7890 hpc
export all_proxy=http://localhost:56789
```

**é—®é¢˜ 2: è¿æ¥è¢«æ‹’ç»**

```bash
# ç—‡çŠ¶
curl: (7) Failed to connect to localhost port 55555: Connection refused

# è§£å†³ï¼šç¡®è®¤ SSH éš§é“ä»åœ¨è¿è¡Œ
ps aux | grep "ssh -R"

# é‡æ–°å»ºç«‹éš§é“
ssh -N -R 55555:localhost:7890 hpc &
```

**é—®é¢˜ 3: æœ¬åœ°ä»£ç†æœªå¯ç”¨ LAN**

```bash
# ç—‡çŠ¶
channel 2: open failed: connect failed: Connection refused

# è§£å†³ï¼šåœ¨ Clash ä¸­å¯ç”¨ "Allow LAN"
1. æ‰“å¼€ Clash
2. General â†’ Allow LAN â†’ å¼€å¯
3. é‡æ–°å»ºç«‹ SSH éš§é“
```

**é—®é¢˜ 4: éš§é“æ„å¤–æ–­å¼€**

```bash
# ä½¿ç”¨ autossh è‡ªåŠ¨é‡è¿ï¼ˆæœ¬åœ°å®‰è£…ï¼‰
# Linux/macOS
autossh -M 0 -N -R 55555:localhost:7890 hpc

# Windows (PowerShell)
# åˆ›å»ºé‡è¿è„šæœ¬ keep_tunnel.ps1
while ($true) {
    ssh -N -R 55555:localhost:7890 hpc
    Start-Sleep -Seconds 5
}
```

#### å®‰å…¨å»ºè®®

```bash
# 1. ä½¿ç”¨éç‰¹æƒç«¯å£ (>1024)
ssh -R 55555:localhost:7890 hpc  # âœ… æ¨è
ssh -R 80:localhost:7890 hpc     # âŒ éœ€è¦ root

# 2. é™åˆ¶ç»‘å®šåœ°å€ï¼ˆä»…å…è®¸æœ¬åœ°è¿æ¥ï¼‰
ssh -R localhost:55555:localhost:7890 hpc

# 3. ä½¿ç”¨å®Œæ¯•åæ¸…ç†ç¯å¢ƒå˜é‡
unset http_proxy https_proxy all_proxy

# 4. ä¸è¦åœ¨å…¬å…±è„šæœ¬ä¸­ç¡¬ç¼–ç ä»£ç†
```

#### é«˜çº§ç”¨æ³•

```bash
# 1. åŠ¨æ€ç«¯å£è½¬å‘ï¼ˆSOCKS5 ä»£ç†ï¼‰
ssh -D 1080 hpc
# ç„¶åé…ç½®åº”ç”¨ä½¿ç”¨ SOCKS5: localhost:1080

# 2. è·³æ¿æœºè½¬å‘
ssh -J jumphost -L 6006:localhost:6006 compute-node

# 3. å¤šçº§è½¬å‘
# æœ¬åœ° â†’ è·³æ¿æœº â†’ è®¡ç®—èŠ‚ç‚¹
ssh -L 6006:compute-node:6006 jumphost

# 4. é…ç½®æ–‡ä»¶ç®€åŒ–å‘½ä»¤
# ~/.ssh/config
Host hpc-tunnel
    HostName hpc.example.edu
    User username
    LocalForward 6006 localhost:6006
    LocalForward 8888 localhost:8888

# ä½¿ç”¨
ssh hpc-tunnel
```

---

## ï¿½ å®¹å™¨åŒ–éƒ¨ç½²

HPC æ”¯æŒä¸¤ç§å®¹å™¨æŠ€æœ¯ï¼š**Enroot**ï¼ˆæ¨èï¼‰å’Œ **Docker**ã€‚

### ä¸ºä»€ä¹ˆä½¿ç”¨å®¹å™¨ï¼Ÿ

- âœ… **ç¯å¢ƒä¸€è‡´æ€§**ï¼šé¿å…ä¾èµ–å†²çª
- âœ… **å¿«é€Ÿéƒ¨ç½²**ï¼šé¢„è£…æ‰€æœ‰ä¾èµ–
- âœ… **ç‰ˆæœ¬éš”ç¦»**ï¼šä¸åŒé¡¹ç›®ä½¿ç”¨ä¸åŒç¯å¢ƒ
- âœ… **æ˜“äºåˆ†äº«**ï¼šå¯¼å‡ºé•œåƒç»™å›¢é˜Ÿä½¿ç”¨
- âœ… **GPU æ”¯æŒ**ï¼šå®¹å™¨å†…ç›´æ¥è®¿é—® GPU

---

## ğŸš€ Enroot å®¹å™¨ï¼ˆæ¨èï¼‰

Enroot æ˜¯ NVIDIA å¼€å‘çš„è½»é‡çº§å®¹å™¨è¿è¡Œæ—¶ï¼Œä¸“ä¸º HPC è®¾è®¡ï¼Œä¸ SLURM æ·±åº¦é›†æˆã€‚

### 1. å¯¼å…¥ Docker é•œåƒ

```bash
# åŸºæœ¬è¯­æ³•
enroot import docker://<IMAGE_NAME>

# ç¤ºä¾‹ï¼šå¯¼å…¥ PyTorch é•œåƒ
enroot import docker://pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# å¦‚æœ DockerHub è®¿é—®æœ‰é—®é¢˜ï¼Œä½¿ç”¨å›½å†…é•œåƒ
enroot import docker://dockerpull.org/pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# ä½¿ç”¨ä»£ç†åŠ é€Ÿä¸‹è½½
export all_proxy=http://localhost:55555
enroot import docker://pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# å¯¼å…¥å®Œæˆåï¼Œä¼šåœ¨å½“å‰ç›®å½•ç”Ÿæˆ .sqsh é•œåƒæ–‡ä»¶
# pytorch+pytorch+2.5.1-cuda12.1-cudnn9-runtime.sqsh
```

**å¸¸ç”¨é•œåƒ**ï¼š
```bash
# PyTorch å®˜æ–¹é•œåƒ
enroot import docker://pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime
enroot import docker://pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel

# TensorFlow å®˜æ–¹é•œåƒ
enroot import docker://tensorflow/tensorflow:2.14.0-gpu

# NVIDIA CUDA åŸºç¡€é•œåƒ
enroot import docker://nvidia/cuda:12.1.0-runtime-ubuntu22.04

# è‡ªå®šä¹‰é•œåƒï¼ˆä»ç§æœ‰ä»“åº“ï¼‰
enroot import docker://myregistry.com/myproject:latest
```

### 2. åˆ›å»ºå®¹å™¨

```bash
# åŸºæœ¬è¯­æ³•
enroot create --name <CONTAINER_NAME> <SQSH_PATH>

# ç¤ºä¾‹ï¼šä» .sqsh æ–‡ä»¶åˆ›å»ºå®¹å™¨
enroot create --name torch251 pytorch+pytorch+2.5.1-cuda12.1-cudnn9-runtime.sqsh

# å®¹å™¨åˆ›å»ºåä¼šä¿å­˜åœ¨ ~/.local/share/enroot/
```

### 3. å®¹å™¨å†…æ‰§è¡Œå‘½ä»¤

```bash
# åŸºæœ¬è¯­æ³•
enroot start <CONTAINER_NAME> <COMMAND>

# ç¤ºä¾‹ï¼šæ£€æŸ¥ GPU
enroot start torch251 nvidia-smi

# ç¤ºä¾‹ï¼šè¿è¡Œ Python è„šæœ¬
enroot start torch251 python train.py

# ç¤ºä¾‹ï¼šäº¤äº’å¼ Shell
enroot start torch251 bash

# ç¤ºä¾‹ï¼šæµ‹è¯• PyTorch CUDA
enroot start torch251 python -c "import torch; print(torch.cuda.is_available())"
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
Thu Jan 17 10:30:45 2025       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03   Driver Version: 535.129.03   CUDA Version: 12.2   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   32C    P0    52W / 400W |      0MiB / 81920MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

### 4. ç›®å½•æŒ‚è½½

Enroot é»˜è®¤**ä¸æŒ‚è½½ä»»ä½•ç›®å½•**ï¼Œéœ€è¦ä½¿ç”¨ `--mount` å‚æ•°ã€‚

```bash
# åŸºæœ¬è¯­æ³•
enroot start --mount <SRC>:<DST> <CONTAINER_NAME> <COMMAND>

# ç¤ºä¾‹ï¼šæŒ‚è½½å…¬å…±æ¨¡å‹åº“
enroot start --mount /home/share/models:/models:ro torch251 ls /models

# ç¤ºä¾‹ï¼šæŒ‚è½½é¡¹ç›®ç›®å½•ï¼ˆè¯»å†™ï¼‰
enroot start --mount /home/username/kava:/workspace torch251 bash

# ç¤ºä¾‹ï¼šå¤šä¸ªæŒ‚è½½
enroot start \
    --mount /home/share/models:/models:ro \
    --mount /home/username/kava:/workspace \
    --mount /home/username/data:/data:ro \
    torch251 python /workspace/train.py

# :ro è¡¨ç¤ºåªè¯»ï¼Œ:rw è¡¨ç¤ºè¯»å†™ï¼ˆé»˜è®¤ï¼‰
```

**éªŒè¯æŒ‚è½½**ï¼š
```bash
# åœ¨å®¹å™¨å†…æŸ¥çœ‹æŒ‚è½½çš„ç›®å½•
enroot start --mount /home/share/models:/models:ro torch251 ls -lh /models

# è¾“å‡ºï¼š
# drwxr-xr-x 15 user group 4.0K Jan 17 10:00 models--meta-llama--Llama-3.2-1B-Instruct
# drwxr-xr-x 12 user group 4.0K Jan 17 10:00 models--Qwen--Qwen2.5-0.5B-Instruct
```

### 5. ä¸ SLURM é›†æˆï¼ˆæ¨èï¼‰

Enroot ä¸ SLURM æ·±åº¦é›†æˆï¼Œå¯ç›´æ¥åœ¨ `sbatch` è„šæœ¬ä¸­ä½¿ç”¨ã€‚

#### SLURM + Enroot è„šæœ¬ç¤ºä¾‹

```bash
#!/usr/bin/bash
#SBATCH --job-name=kava-enroot
#SBATCH --partition=compute
#SBATCH --gres=gpu:a100-sxm4-80gb:1
#SBATCH --cpus-per-task=8
#SBATCH --mem=64G
#SBATCH --time=48:00:00
#SBATCH --output=logs/kava_%j.out
#SBATCH --error=logs/kava_%j.err

# ========== Enroot å®¹å™¨é…ç½® ==========
#SBATCH --container-writable                              # å®¹å™¨å†…å¯å†™
#SBATCH --container-mount-home                            # æŒ‚è½½å®¶ç›®å½•
#SBATCH --container-mounts /home/share/models:/models:ro  # æŒ‚è½½å…¬å…±æ¨¡å‹åº“
#SBATCH --container-image torch251                        # å®¹å™¨åç§°æˆ– .sqsh è·¯å¾„

# ========== ä»¥ä¸‹å‘½ä»¤åœ¨å®¹å™¨å†…æ‰§è¡Œ ==========

# æ£€æµ‹ GPU
NUM_GPUS=$(nvidia-smi --query-gpu=gpu_name --format=csv,noheader | wc -l)
echo "NUM_GPUS: $NUM_GPUS"

# éªŒè¯ PyTorch CUDA
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"

# é…ç½® HuggingFace ä½¿ç”¨æŒ‚è½½çš„æ¨¡å‹åº“
export HF_HOME=/models
export TRANSFORMERS_CACHE=/models
export HF_DATASETS_CACHE=/models

# è®­ç»ƒ
cd /workspace  # å‡è®¾æŒ‚è½½äº†é¡¹ç›®ç›®å½•
python train.py --config configs/llama1b_aug.yaml
```

#### æäº¤ä½œä¸š

```bash
# å‡†å¤‡é•œåƒï¼ˆä»…é¦–æ¬¡ï¼‰
enroot import docker://pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime
enroot create --name torch251 pytorch+pytorch+2.5.1-cuda12.1-cudnn9-runtime.sqsh

# æäº¤ä½œä¸š
sbatch submit_enroot.slurm

# æŸ¥çœ‹ä½œä¸š
squeue --me
tail -f logs/kava_*.out
```

#### ä½¿ç”¨ .sqsh æ–‡ä»¶è·¯å¾„

```bash
# å¦‚æœä¸æƒ³åˆ›å»ºå‘½åå®¹å™¨ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ .sqsh è·¯å¾„
#SBATCH --container-image /home/username/pytorch+pytorch+2.5.1-cuda12.1-cudnn9-runtime.sqsh
```

### 6. Enroot å¸¸ç”¨å‘½ä»¤

```bash
# åˆ—å‡ºæ‰€æœ‰å®¹å™¨
enroot list

# åˆ é™¤å®¹å™¨
enroot remove torch251

# å¯¼å‡ºå®¹å™¨ä¸º .sqsh
enroot export --output mycontainer.sqsh mycontainer

# ä»æœ¬åœ° .sqsh åˆ›å»ºå®¹å™¨
enroot create --name newcontainer mycontainer.sqsh

# æŸ¥çœ‹å®¹å™¨ä¿¡æ¯
enroot inspect torch251

# æ¸…ç†æœªä½¿ç”¨çš„é•œåƒ
enroot remove --all
```

### 7. KAVA é¡¹ç›®å®¹å™¨åŒ–éƒ¨ç½²

#### Step 1: åˆ›å»ºè‡ªå®šä¹‰ Dockerfile

```dockerfile
# Dockerfile
FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /workspace

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    git \
    vim \
    wget \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£… Python ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt && \
    pip install peft wandb bitsandbytes

# å¤åˆ¶é¡¹ç›®ä»£ç 
COPY . .

# é»˜è®¤å‘½ä»¤
CMD ["bash"]
```

#### Step 2: æœ¬åœ°æ„å»ºå¹¶å¯¼å‡ºï¼ˆå¯é€‰ï¼‰

```bash
# åœ¨æœ¬åœ°æœºå™¨æ„å»º Docker é•œåƒ
docker build -t kava:latest .

# å¯¼å‡ºä¸º tar
docker save kava:latest -o kava-latest.tar

# ä¸Šä¼ åˆ° HPC
scp kava-latest.tar username@hpc.example.edu:~/

# åœ¨ HPC ä¸Šå¯¼å…¥
enroot import docker://kava-latest.tar
enroot create --name kava kava+latest.sqsh
```

#### Step 3: æˆ–ç›´æ¥åœ¨ HPC å¯¼å…¥åŸºç¡€é•œåƒ

```bash
# å¯¼å…¥ PyTorch åŸºç¡€é•œåƒ
enroot import docker://pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel
enroot create --name kava-base pytorch+pytorch+2.1.0-cuda12.1-cudnn8-devel.sqsh

# è¿›å…¥å®¹å™¨å®‰è£…ä¾èµ–
enroot start --mount $PWD:/workspace --writable kava-base bash

# åœ¨å®¹å™¨å†…
cd /workspace
pip install -r requirements.txt
pip install peft wandb bitsandbytes
exit

# å¯¼å‡ºä¿®æ”¹åçš„å®¹å™¨
enroot export --output kava-ready.sqsh kava-base
enroot create --name kava kava-ready.sqsh
```

#### Step 4: ä½¿ç”¨å®¹å™¨è®­ç»ƒ

```bash
# æ–¹æ³• 1: ç›´æ¥è¿è¡Œ
enroot start \
    --mount $PWD:/workspace \
    --mount /home/share/models:/models:ro \
    kava python /workspace/train.py --config /workspace/configs/llama1b_aug.yaml

# æ–¹æ³• 2: SLURM æäº¤
sbatch --container-image kava submit_multi_seed.slurm
```

---

## ğŸ‹ Docker å®¹å™¨ï¼ˆé«˜çº§ï¼‰

HPC æ”¯æŒ **rootless Docker**ï¼ˆæ— éœ€ root æƒé™ï¼‰ï¼Œé€‚åˆéœ€è¦å¯¼å‡ºé•œåƒçš„åœºæ™¯ã€‚

**æ¨è**: å¦‚æœä¸éœ€è¦å¯¼å‡ºé•œåƒï¼Œä¼˜å…ˆä½¿ç”¨ Enrootï¼

### 1. åˆæ¬¡é…ç½®ï¼ˆä»…éœ€ä¸€æ¬¡ï¼‰

#### æ–¹æ³• 1: è‡ªåŠ¨é…ç½®ï¼ˆæ¨èï¼‰

```bash
# ç™»å½•åˆ°è®¡ç®—èŠ‚ç‚¹ï¼ˆgpu10-gpu14ï¼‰
srun -w gpu10 --pty bash

# è¿è¡Œé…ç½®è„šæœ¬
dockerd-rootless-setuptool.sh install

# å¦‚æœæˆåŠŸï¼Œä¼šè¾“å‡ºç±»ä¼¼ï¼š
# [INFO] Installed dockerd-rootless-setuptool.sh
# [INFO] Make sure the following environment variables are set:
#   export PATH=/usr/bin:$PATH
#   export DOCKER_HOST=unix:///tmp/$(id -u)/docker/run/docker.sock
```

#### æ–¹æ³• 2: æ‰‹åŠ¨é…ç½®

```bash
# 1. åˆ›å»ºè¿è¡Œç›®å½•
mkdir -p /tmp/$(id -u)/docker/run

# 2. è®¾ç½®ç¯å¢ƒå˜é‡
export XDG_RUNTIME_DIR=/tmp/$(id -u)/docker/run
export DOCKER_HOST=unix:///tmp/$(id -u)/docker/run/docker.sock

# 3. å†™å…¥ ~/.bashrc
cat >> ~/.bashrc << 'EOF'
# Docker rootless
export XDG_RUNTIME_DIR=/tmp/$(id -u)/docker/run
export DOCKER_HOST=unix:///tmp/$(id -u)/docker/run/docker.sock
EOF

# 4. å¯åŠ¨ Docker æœåŠ¡
PATH=/usr/bin:/sbin:/usr/sbin:$PATH dockerd-rootless.sh &

# ç­‰å¾…æœåŠ¡å¯åŠ¨ï¼ˆçº¦ 10 ç§’ï¼‰
sleep 10
```

### 2. é…ç½®æ•°æ®ç›®å½•ï¼ˆé¿å…æƒé™é—®é¢˜ï¼‰

```bash
# åˆ›å»ºé…ç½®æ–‡ä»¶
mkdir -p ~/.config/docker
cat > ~/.config/docker/daemon.json << EOF
{
  "data-root": "/tmp/$(id -u)/docker-data"
}
EOF

# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p /tmp/$(id -u)/docker-data

# é‡å¯ Docker æœåŠ¡
systemctl --user restart docker
```

### 3. éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
systemctl --user status docker

# åº”è¯¥çœ‹åˆ°ï¼š
# â— docker.service - Docker Application Container Engine (Rootless)
#      Loaded: loaded
#      Active: active (running)

# æµ‹è¯•è¿è¡Œå®¹å™¨
docker run hello-world

# æµ‹è¯• GPU å®¹å™¨
docker run --rm --gpus 0 pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel nvidia-smi
```

### 4. ä½¿ç”¨ Docker

```bash
# æ‹‰å–é•œåƒ
docker pull pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# åˆ—å‡ºé•œåƒ
docker images

# è¿è¡Œå®¹å™¨ï¼ˆäº¤äº’å¼ï¼‰
docker run -it --rm --gpus all \
    -v $PWD:/workspace \
    -v /home/share/models:/models:ro \
    pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime bash

# è¿è¡Œè®­ç»ƒ
docker run --rm --gpus all \
    -v $PWD:/workspace \
    -v /home/share/models:/models:ro \
    -e HF_HOME=/models \
    pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime \
    python /workspace/train.py --config /workspace/configs/llama1b_aug.yaml

# åå°è¿è¡Œ
docker run -d --gpus all \
    -v $PWD:/workspace \
    --name kava-train \
    pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime \
    python /workspace/train.py --config /workspace/configs/llama1b_aug.yaml

# æŸ¥çœ‹æ—¥å¿—
docker logs -f kava-train

# åœæ­¢å®¹å™¨
docker stop kava-train
```

### 5. æ„å»ºè‡ªå®šä¹‰é•œåƒ

```bash
# ä½¿ç”¨ä¹‹å‰çš„ Dockerfile
docker build -t kava:latest .

# è¿è¡Œè‡ªå®šä¹‰é•œåƒ
docker run -it --rm --gpus all -v $PWD:/workspace kava:latest
```

### 6. å¯¼å‡ºå’Œåˆ†äº«é•œåƒ

```bash
# å¯¼å‡ºä¸º tar
docker save kava:latest -o kava-latest.tar

# åˆ†äº«ç»™å›¢é˜Ÿ
scp kava-latest.tar teammate@hpc.example.edu:~/

# å¯¼å…¥é•œåƒ
docker load -i kava-latest.tar

# æˆ–æ¨é€åˆ° Docker Hub
docker tag kava:latest myusername/kava:latest
docker push myusername/kava:latest
```

### 7. æ•…éšœæ’é™¤

**é—®é¢˜ 1: æƒé™é”™è¯¯**

```bash
# ç—‡çŠ¶
Got permission denied while trying to connect to the Docker daemon socket

# è§£å†³ï¼šæ£€æŸ¥ç¯å¢ƒå˜é‡
echo $DOCKER_HOST  # åº”è¯¥è¾“å‡º unix:///tmp/.../docker.sock

# é‡æ–°è®¾ç½®
export DOCKER_HOST=unix:///tmp/$(id -u)/docker/run/docker.sock
```

**é—®é¢˜ 2: æœåŠ¡æœªè¿è¡Œ**

```bash
# ç—‡çŠ¶
Cannot connect to the Docker daemon

# è§£å†³ï¼šå¯åŠ¨æœåŠ¡
PATH=/usr/bin:/sbin:/usr/sbin:$PATH dockerd-rootless.sh &

# æˆ–ä½¿ç”¨ systemd
systemctl --user start docker
```

**é—®é¢˜ 3: GPU ä¸å¯ç”¨**

```bash
# ç—‡çŠ¶
docker: Error response from daemon: could not select device driver "" with capabilities: [[gpu]]

# è§£å†³ï¼šä½¿ç”¨æ­£ç¡®çš„ GPU å‚æ•°
docker run --gpus 0 ...        # ä½¿ç”¨ GPU 0
docker run --gpus all ...      # ä½¿ç”¨æ‰€æœ‰ GPU
docker run --gpus '"device=0,1"' ...  # ä½¿ç”¨ GPU 0 å’Œ 1
```

---

## ğŸ“Š Enroot vs Docker å¯¹æ¯”

| ç‰¹æ€§ | Enroot | Docker |
|------|--------|--------|
| **æ€§èƒ½** | â­â­â­â­â­ æ›´å¿« | â­â­â­â­â˜† ç•¥æ…¢ |
| **SLURM é›†æˆ** | â­â­â­â­â­ åŸç”Ÿæ”¯æŒ | â­â­â­â˜†â˜† éœ€æ‰‹åŠ¨ |
| **æ˜“ç”¨æ€§** | â­â­â­â­â˜† ç®€å• | â­â­â­â­â­ æ›´æˆç†Ÿ |
| **é•œåƒæ„å»º** | â­â­â­â˜†â˜† éœ€ Docker | â­â­â­â­â­ åŸç”Ÿ |
| **é•œåƒåˆ†äº«** | â­â­â­â˜†â˜† éœ€å¯¼å‡º | â­â­â­â­â­ Docker Hub |
| **HPC ä¼˜åŒ–** | â­â­â­â­â­ ä¸“ä¸º HPC è®¾è®¡ | â­â­â­â˜†â˜† é€šç”¨ |
| **æ¨èåœºæ™¯** | HPC è®­ç»ƒã€æ‰¹é‡ä½œä¸š | é•œåƒå¼€å‘ã€å¯¼å‡ºåˆ†äº« |

**æ¨èç­–ç•¥**ï¼š
- âœ… **Enroot**: æ—¥å¸¸è®­ç»ƒã€SLURM ä½œä¸šã€å›¢é˜Ÿå…±äº«é•œåƒ
- âœ… **Docker**: é•œåƒå¼€å‘ã€è°ƒè¯•ã€æ¨é€åˆ° Docker Hub

**æ··åˆä½¿ç”¨**ï¼š
```bash
# 1. ç”¨ Docker æ„å»ºé•œåƒ
docker build -t kava:latest .

# 2. å¯¼å‡ºå¹¶è½¬æ¢ä¸º Enroot
docker save kava:latest | enroot import docker://kava:latest -
enroot create --name kava kava+latest.sqsh

# 3. åœ¨ SLURM ä¸­ä½¿ç”¨ Enroot
sbatch --container-image kava submit_multi_seed.slurm
```

---

## ï¿½ğŸ”— ç›¸å…³æ–‡æ¡£

- **å®Œæ•´æŒ‡å—**: `REPRODUCTION_GUIDE.md`
- **å¿«é€Ÿå‚è€ƒ**: `QUICK_REFERENCE.md`
- **é¡¹ç›®æ¸…å•**: `PROJECT_INVENTORY.md`
- **äº¤äº’å¼ä½¿ç”¨**: `SLURM_INTERACTIVE_GUIDE.md`
- **Enroot å®˜æ–¹æ–‡æ¡£**: https://github.com/NVIDIA/enroot
- **Enroot + SLURM**: https://github.com/NVIDIA/pyxis

---

## ğŸ’¡ æœ€ä½³å®è·µ

1. **ä½¿ç”¨æ•°ç»„ä½œä¸š**: æ‰¹é‡è¿è¡Œå¤šä¸ªç§å­ï¼Œè‡ªåŠ¨å¹¶è¡Œ
2. **è®¾ç½®ä¾èµ–å…³ç³»**: è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ï¼Œæ— éœ€æ‰‹åŠ¨ç­‰å¾…
3. **å®šæœŸæ£€æŸ¥æ—¥å¿—**: åŠæ—©å‘ç°é—®é¢˜
4. **åˆç†ä¼°ç®—æ—¶é—´**: é¿å…ä»»åŠ¡è¢«è¿‡æ—©ç»ˆæ­¢
5. **å¤‡ä»½é‡è¦æ•°æ®**: å®šæœŸä¸‹è½½æ£€æŸ¥ç‚¹å’Œç»“æœ
6. **ä½¿ç”¨ WandB**: è¿œç¨‹ç›‘æ§è®­ç»ƒè¿›åº¦
7. **å‹ç¼©å­˜å‚¨**: èŠ‚çœç£ç›˜é…é¢
8. **æ¸…ç†ä¸´æ—¶æ–‡ä»¶**: å®šæœŸåˆ é™¤ä¸éœ€è¦çš„æ£€æŸ¥ç‚¹

---

**å¿«é€Ÿè”ç³»æ–¹å¼**
- æŠ€æœ¯é—®é¢˜ï¼šæŸ¥çœ‹ Issues
- HPC æ”¯æŒï¼šè”ç³»é›†ç¾¤ç®¡ç†å‘˜
