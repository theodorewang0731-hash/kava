# å®¹å™¨åŒ–éƒ¨ç½²å¿«é€ŸæŒ‡å—

**åœ¨ HPC ä¸Šä½¿ç”¨ Enroot/Docker å®¹å™¨è¿è¡Œ KAVA**

---

## ğŸ¯ ä¸ºä»€ä¹ˆä½¿ç”¨å®¹å™¨ï¼Ÿ

| ä¼˜åŠ¿ | è¯´æ˜ |
|------|------|
| âœ… **ç¯å¢ƒä¸€è‡´æ€§** | é¿å…"åœ¨æˆ‘æœºå™¨ä¸Šèƒ½è·‘"é—®é¢˜ |
| âœ… **ä¾èµ–éš”ç¦»** | ä¸åŒé¡¹ç›®ä½¿ç”¨ä¸åŒ Python/CUDA ç‰ˆæœ¬ |
| âœ… **å¿«é€Ÿéƒ¨ç½²** | é¢„è£…æ‰€æœ‰ä¾èµ–ï¼Œç§’çº§å¯åŠ¨ |
| âœ… **æ˜“äºåˆ†äº«** | å¯¼å‡ºé•œåƒç»™å›¢é˜Ÿï¼Œä¸€æ¬¡æ„å»ºå¤„å¤„è¿è¡Œ |
| âœ… **GPU æ”¯æŒ** | å®¹å™¨å†…ç›´æ¥è®¿é—® GPUï¼Œæ€§èƒ½æ— æŸ |

---

## ğŸš€ Enroot å¿«é€Ÿå¼€å§‹ï¼ˆæ¨èï¼‰

### 1 åˆ†é’Ÿå¿«é€Ÿéƒ¨ç½²

```bash
# 1. å¯¼å…¥ PyTorch é•œåƒï¼ˆä»…é¦–æ¬¡ï¼Œçº¦ 2-5 åˆ†é’Ÿï¼‰
enroot import docker://pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# 2. åˆ›å»ºå®¹å™¨ï¼ˆä»…é¦–æ¬¡ï¼Œçº¦ 10 ç§’ï¼‰
enroot create --name kava-torch pytorch+pytorch+2.5.1-cuda12.1-cudnn9-runtime.sqsh

# 3. è¿è¡Œè®­ç»ƒï¼ˆç«‹å³å¼€å§‹ï¼‰
enroot start \
    --mount $PWD:/workspace \
    --mount /home/share/models:/models:ro \
    kava-torch python /workspace/train.py --config /workspace/configs/llama1b_aug.yaml
```

### SLURM æ‰¹é‡ä½œä¸šï¼ˆ3 ä¸ªç§å­ï¼‰

```bash
# 1. å‡†å¤‡å®¹å™¨ï¼ˆä»…é¦–æ¬¡ï¼‰
enroot import docker://pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime
enroot create --name kava-torch pytorch+pytorch+2.5.1-cuda12.1-cudnn9-runtime.sqsh

# 2. æäº¤ä½œä¸š
sbatch --export=CONFIG=llama1b_aug submit_enroot.slurm

# 3. ç›‘æ§
squeue --me
tail -f logs/kava_enroot_*.out
```

---

## ğŸ“¦ æ¨èé•œåƒ

| é•œåƒ | é€‚ç”¨åœºæ™¯ | å¯¼å…¥å‘½ä»¤ |
|------|---------|---------|
| **PyTorch 2.5.1 + CUDA 12.1** | KAVA è®­ç»ƒï¼ˆæ¨èï¼‰ | `enroot import docker://pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime` |
| **PyTorch 2.1.0 + CUDA 12.1** | å…¼å®¹æ€§æ›´å¥½ | `enroot import docker://pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel` |
| **TensorFlow 2.14 + GPU** | TensorFlow é¡¹ç›® | `enroot import docker://tensorflow/tensorflow:2.14.0-gpu` |
| **NVIDIA CUDA 12.1** | è‡ªå®šä¹‰ç¯å¢ƒ | `enroot import docker://nvidia/cuda:12.1.0-runtime-ubuntu22.04` |

---

## ğŸ› ï¸ å®Œæ•´å·¥ä½œæµç¨‹

### Step 1: å¯¼å…¥é•œåƒ

```bash
# æ–¹æ³• 1: ä» Docker Hubï¼ˆå›½å†…å¯èƒ½è¾ƒæ…¢ï¼‰
enroot import docker://pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# æ–¹æ³• 2: ä½¿ç”¨å›½å†…é•œåƒï¼ˆæ¨èï¼‰
enroot import docker://dockerpull.org/pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# æ–¹æ³• 3: ä½¿ç”¨ä»£ç†åŠ é€Ÿ
export all_proxy=http://localhost:55555
enroot import docker://pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# å¯¼å…¥åä¼šç”Ÿæˆ .sqsh æ–‡ä»¶
ls -lh pytorch+pytorch+*.sqsh
```

### Step 2: åˆ›å»ºå®¹å™¨

```bash
# ä» .sqsh åˆ›å»ºå‘½åå®¹å™¨
enroot create --name kava-torch pytorch+pytorch+2.5.1-cuda12.1-cudnn9-runtime.sqsh

# éªŒè¯å®¹å™¨
enroot list
# è¾“å‡ºï¼škava-torch
```

### Step 3: æµ‹è¯•å®¹å™¨

```bash
# æµ‹è¯• GPU
enroot start kava-torch nvidia-smi

# æµ‹è¯• PyTorch
enroot start kava-torch python -c "import torch; print(torch.cuda.is_available())"

# äº¤äº’å¼ Shell
enroot start kava-torch bash
```

### Step 4: å®‰è£…ä¾èµ–ï¼ˆé¦–æ¬¡ï¼‰

```bash
# è¿›å…¥å®¹å™¨å¹¶æŒ‚è½½é¡¹ç›®
enroot start --mount $PWD:/workspace kava-torch bash

# åœ¨å®¹å™¨å†…
cd /workspace
pip install -r requirements.txt
pip install peft wandb bitsandbytes

# é€€å‡º
exit
```

### Step 5: è¿è¡Œè®­ç»ƒ

#### æ–¹æ³• A: å‘½ä»¤è¡Œç›´æ¥è¿è¡Œ

```bash
enroot start \
    --mount $PWD:/workspace \
    --mount /home/share/models:/models:ro \
    kava-torch python /workspace/train.py \
        --config /workspace/configs/llama1b_aug.yaml \
        --output_dir /workspace/outputs/llama1b_aug_seed_42 \
        --seed 42
```

#### æ–¹æ³• B: SLURM æäº¤ï¼ˆæ¨èï¼‰

**ç¼–è¾‘ submit_enroot.slurm**:
```bash
#!/usr/bin/bash
#SBATCH --job-name=kava-enroot
#SBATCH --partition=compute
#SBATCH --gres=gpu:a100-sxm4-80gb:1
#SBATCH --cpus-per-task=8
#SBATCH --mem=64G
#SBATCH --time=48:00:00
#SBATCH --output=logs/kava_%j.out
#SBATCH --error=logs/kava_%j.err

# Enroot é…ç½®
#SBATCH --container-writable
#SBATCH --container-mount-home
#SBATCH --container-mounts /home/share/models:/models:ro
#SBATCH --container-image kava-torch  # æˆ– .sqsh æ–‡ä»¶è·¯å¾„

# é…ç½® HuggingFace
export HF_HOME=/models
export TRANSFORMERS_CACHE=/models
export HF_DATASETS_CACHE=/models

# è¿è¡Œè®­ç»ƒ
cd $HOME/kava
python train.py --config configs/llama1b_aug.yaml --use_wandb
```

**æäº¤ä½œä¸š**:
```bash
sbatch submit_enroot.slurm
```

---

## ğŸ‹ Docker ä½¿ç”¨ï¼ˆå¯é€‰ï¼‰

### åˆæ¬¡é…ç½®

```bash
# 1. ç™»å½•åˆ°è®¡ç®—èŠ‚ç‚¹
srun -w gpu10 --pty bash

# 2. é…ç½® rootless Docker
dockerd-rootless-setuptool.sh install

# 3. é…ç½®æ•°æ®ç›®å½•
mkdir -p ~/.config/docker
cat > ~/.config/docker/daemon.json << EOF
{
  "data-root": "/tmp/$(id -u)/docker-data"
}
EOF

# 4. å¯åŠ¨æœåŠ¡
systemctl --user start docker

# 5. éªŒè¯
docker run hello-world
docker run --rm --gpus 0 pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel nvidia-smi
```

### ä½¿ç”¨ Docker è®­ç»ƒ

```bash
# æ‹‰å–é•œåƒ
docker pull pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# è¿è¡Œè®­ç»ƒ
docker run --rm --gpus all \
    -v $PWD:/workspace \
    -v /home/share/models:/models:ro \
    -e HF_HOME=/models \
    -e TRANSFORMERS_CACHE=/models \
    pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime \
    python /workspace/train.py --config /workspace/configs/llama1b_aug.yaml
```

---

## ğŸ“Š Enroot vs Docker

| ç‰¹æ€§ | Enroot | Docker |
|------|--------|--------|
| **æ¨èåº¦** | â­â­â­â­â­ HPC é¦–é€‰ | â­â­â­â­â˜† é•œåƒå¼€å‘ |
| **SLURM é›†æˆ** | âœ… åŸç”Ÿæ”¯æŒ | âŒ éœ€æ‰‹åŠ¨ |
| **æ€§èƒ½** | âœ… æ›´å¿« | âœ… è‰¯å¥½ |
| **é•œåƒæ„å»º** | âŒ éœ€ Docker | âœ… åŸç”Ÿ |
| **é€‚ç”¨åœºæ™¯** | æ—¥å¸¸è®­ç»ƒ | é•œåƒå¼€å‘ |

**æ¨èç­–ç•¥**: ç”¨ Docker æ„å»ºé•œåƒ â†’ è½¬ä¸º Enroot åœ¨ HPC ä½¿ç”¨

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: å®¹å™¨å†…æ‰¾ä¸åˆ°æ¨¡å‹

```bash
# ç—‡çŠ¶
FileNotFoundError: Model 'meta-llama/Llama-3.2-1B-Instruct' not found

# è§£å†³ï¼šæŒ‚è½½å…¬å…±æ¨¡å‹åº“å¹¶è®¾ç½®ç¯å¢ƒå˜é‡
enroot start \
    --mount /home/share/models:/models:ro \
    kava-torch bash

# åœ¨å®¹å™¨å†…
export HF_HOME=/models
export TRANSFORMERS_CACHE=/models
```

### Q2: ä¾èµ–æœªå®‰è£…

```bash
# ç—‡çŠ¶
ModuleNotFoundError: No module named 'peft'

# è§£å†³ï¼šåœ¨å®¹å™¨å†…å®‰è£…
enroot start --mount $PWD:/workspace kava-torch bash
cd /workspace
pip install -r requirements.txt
pip install peft wandb bitsandbytes
```

### Q3: å®¹å™¨å†…æ— æ³•å†™å…¥

```bash
# ç—‡çŠ¶
PermissionError: [Errno 13] Permission denied

# è§£å†³ï¼šä½¿ç”¨ --writable æˆ– SLURM çš„ --container-writable
enroot start --writable --mount $PWD:/workspace kava-torch bash

# æˆ–åœ¨ SLURM è„šæœ¬ä¸­
#SBATCH --container-writable
```

### Q4: GPU ä¸å¯ç”¨

```bash
# ç—‡çŠ¶
torch.cuda.is_available() = False

# æ£€æŸ¥ï¼š
# 1. èŠ‚ç‚¹æ˜¯å¦æœ‰ GPU
nvidia-smi

# 2. SLURM æ˜¯å¦åˆ†é… GPU
echo $CUDA_VISIBLE_DEVICES

# 3. å®¹å™¨å†…æ˜¯å¦è¯†åˆ«
enroot start kava-torch nvidia-smi
```

### Q5: é•œåƒå¤ªå¤§ä¸‹è½½æ…¢

```bash
# è§£å†³ï¼šä½¿ç”¨ä»£ç†
export all_proxy=http://localhost:55555
enroot import docker://pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# æˆ–ä½¿ç”¨å›½å†…é•œåƒ
enroot import docker://dockerpull.org/pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# æˆ–é€‰æ‹©æ›´å°çš„ runtime é•œåƒï¼ˆä¸å«ç¼–è¯‘å·¥å…·ï¼‰
enroot import docker://pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime  # çº¦ 5GB
# è€Œä¸æ˜¯ devel é•œåƒï¼ˆçº¦ 10GBï¼‰
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„é•œåƒæ ‡ç­¾

```bash
# âœ… æ¨èï¼šruntimeï¼ˆæ›´å°ï¼Œé€‚åˆè®­ç»ƒï¼‰
pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# âš ï¸ å¯é€‰ï¼šdevelï¼ˆæ›´å¤§ï¼ŒåŒ…å«ç¼–è¯‘å·¥å…·ï¼‰
pytorch/pytorch:2.5.1-cuda12.1-cudnn9-devel

# é€‰æ‹©åŸåˆ™ï¼š
# - ä»…è®­ç»ƒ/æ¨ç† â†’ runtime
# - éœ€è¦ç¼–è¯‘æ‰©å±• â†’ devel
```

### 2. æŒ‚è½½ç›®å½•è§„åˆ’

```bash
# æ¨èçš„æŒ‚è½½ç­–ç•¥
enroot start \
    --mount $HOME/kava:/workspace \              # é¡¹ç›®ä»£ç ï¼ˆè¯»å†™ï¼‰
    --mount /home/share/models:/models:ro \      # å…¬å…±æ¨¡å‹ï¼ˆåªè¯»ï¼‰
    --mount /home/username/data:/data:ro \       # æ•°æ®é›†ï¼ˆåªè¯»ï¼‰
    --mount /home/username/outputs:/outputs \    # è¾“å‡ºç›®å½•ï¼ˆè¯»å†™ï¼‰
    kava-torch bash

# :ro = åªè¯»ï¼Œ:rw = è¯»å†™ï¼ˆé»˜è®¤ï¼‰
```

### 3. ä½¿ç”¨ SLURM å®¹å™¨å‚æ•°

```bash
# âœ… æ¨èï¼šä½¿ç”¨ SLURM çš„å®¹å™¨å‚æ•°ï¼ˆæ›´ç®€æ´ï¼‰
#SBATCH --container-image kava-torch
#SBATCH --container-mount-home
#SBATCH --container-mounts /home/share/models:/models:ro
#SBATCH --container-writable

# âŒ é¿å…ï¼šåœ¨è„šæœ¬ä¸­æ‰‹åŠ¨è°ƒç”¨ enroot startï¼ˆå¤æ‚ï¼‰
```

### 4. é¢„å®‰è£…ä¾èµ–

```bash
# æ–¹æ³• 1: æ„å»ºè‡ªå®šä¹‰é•œåƒï¼ˆæ¨èï¼‰
# ç¼–å†™ Dockerfileï¼Œé¢„è£…æ‰€æœ‰ä¾èµ–
docker build -t kava:latest .
docker save kava:latest | enroot import docker://kava:latest -

# æ–¹æ³• 2: ä¿®æ”¹ç°æœ‰å®¹å™¨å¹¶å¯¼å‡º
enroot start --writable --mount $PWD:/workspace kava-torch bash
# åœ¨å®¹å™¨å†…å®‰è£…ä¾èµ–
pip install -r /workspace/requirements.txt
exit
enroot export --output kava-ready.sqsh kava-torch
enroot create --name kava-ready kava-ready.sqsh
```

### 5. æ¸…ç†æœªä½¿ç”¨çš„å®¹å™¨

```bash
# åˆ—å‡ºæ‰€æœ‰å®¹å™¨
enroot list

# åˆ é™¤ä¸éœ€è¦çš„å®¹å™¨
enroot remove old-container

# æ¸…ç†ç£ç›˜ç©ºé—´
rm -f *.sqsh  # åˆ é™¤ .sqsh æ–‡ä»¶
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **HPC å®Œæ•´å‚è€ƒ**: [HPC_REFERENCE.md](HPC_REFERENCE.md) - å®¹å™¨åŒ–éƒ¨ç½²è¯¦ç»†ç« èŠ‚
- **å¤ç°æŒ‡å—**: [REPRODUCTION_GUIDE.md](REPRODUCTION_GUIDE.md)
- **SLURM äº¤äº’å¼**: [SLURM_INTERACTIVE_GUIDE.md](SLURM_INTERACTIVE_GUIDE.md)

**å®˜æ–¹æ–‡æ¡£**:
- [Enroot åŸºæœ¬ç”¨æ³•](https://github.com/NVIDIA/enroot)
- [Enroot + SLURM (Pyxis)](https://github.com/NVIDIA/pyxis)
- [Docker Rootless](https://docs.docker.com/engine/security/rootless/)

---

## ğŸ“ å¿«é€Ÿå‘½ä»¤å¤‡å¿˜

```bash
# === Enroot ===
# å¯¼å…¥é•œåƒ
enroot import docker://pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# åˆ›å»ºå®¹å™¨
enroot create --name kava-torch pytorch+*.sqsh

# è¿è¡Œå‘½ä»¤
enroot start kava-torch nvidia-smi

# æŒ‚è½½ç›®å½•è¿è¡Œ
enroot start --mount $PWD:/workspace kava-torch python /workspace/train.py

# åˆ—å‡ºå®¹å™¨
enroot list

# åˆ é™¤å®¹å™¨
enroot remove kava-torch

# === SLURM + Enroot ===
# æäº¤ä½œä¸š
sbatch --container-image kava-torch submit_enroot.slurm

# æŸ¥çœ‹ä½œä¸š
squeue --me

# å–æ¶ˆä½œä¸š
scancel <JOB_ID>

# === Docker ===
# å¯åŠ¨æœåŠ¡
systemctl --user start docker

# æ‹‰å–é•œåƒ
docker pull pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# è¿è¡Œå®¹å™¨
docker run --rm --gpus all -v $PWD:/workspace pytorch/pytorch:2.5.1 python train.py

# åˆ—å‡ºé•œåƒ
docker images

# åˆ—å‡ºå®¹å™¨
docker ps -a
```

---

**æç¤º**: Enroot æ˜¯ HPC çš„æœ€ä½³é€‰æ‹©ï¼Œä¸ SLURM æ— ç¼é›†æˆï¼Œæ€§èƒ½ä¼˜å¼‚ï¼
