# ğŸš€ å¿«é€ŸéªŒè¯æŒ‡å—

åœ¨è¿è¡Œå®Œæ•´å®éªŒå‰ï¼Œå…ˆè¿›è¡Œå¿«é€ŸéªŒè¯ç¡®ä¿æ‰€æœ‰ç»„ä»¶æ­£å¸¸å·¥ä½œã€‚

---

## Step 1: çƒŸé›¾æµ‹è¯• (2 åˆ†é’Ÿ)

éªŒè¯æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ï¼š

```bash
python smoke_test.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
âœ… All smoke tests passed!

You can now proceed with:
  1. Quick training test
  2. Full experiment
  3. Complete replication
```

å¦‚æœå¤±è´¥ï¼Œæ£€æŸ¥ï¼š
- ä¾èµ–å®‰è£…ï¼š`pip install -r requirements.txt`
- é…ç½®æ–‡ä»¶å­˜åœ¨ï¼š`ls configs/`
- Python ç‰ˆæœ¬ï¼šâ‰¥ 3.8

---

## Step 2: å¿«é€Ÿè®­ç»ƒæµ‹è¯• (5-10 åˆ†é’Ÿ)

åœ¨å°‘é‡æ•°æ®ä¸Šå¿«é€Ÿæµ‹è¯•è®­ç»ƒæµç¨‹ï¼š

```bash
python train.py \
    --config configs/llama1b_aug.yaml \
    --seed 42 \
    --max_train_samples 100 \
    --max_eval_samples 20 \
    --num_epochs 1 \
    --output_dir experiments/quick_test
```

**é¢„æœŸè¾“å‡º**ï¼š
- è®­ç»ƒè¿›åº¦æ¡è¿è¡Œæ— é”™è¯¯
- æŸå¤±å€¼ä¸‹é™ï¼ˆloss < 5.0ï¼‰
- ä¿å­˜ checkpoint åˆ° `experiments/quick_test/`

**å¸¸è§é—®é¢˜**ï¼š
- **OOM**ï¼šå‡å°‘ batch size åˆ° 1
- **æ•°æ®é›†åŠ è½½å¤±è´¥**ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥
- **æ¨¡å‹ä¸‹è½½æ…¢**ï¼šè®¾ç½® HF_ENDPOINT

---

## Step 3: å¿«é€Ÿè¯„ä¼°æµ‹è¯• (2-3 åˆ†é’Ÿ)

æµ‹è¯•è¯„ä¼°æµç¨‹ï¼ˆéœ€è¦ checkpointï¼‰ï¼š

```bash
python evaluate.py \
    --checkpoint experiments/quick_test/checkpoint-epoch1 \
    --config configs/llama1b_aug.yaml \
    --datasets gsm8k \
    --max_samples 20 \
    --output experiments/quick_test/eval_results.yaml
```

**é¢„æœŸè¾“å‡º**ï¼š
```
=== Evaluating on gsm8k ===
Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:45<00:00]
Accuracy: 25.00% (5/20)
Avg Forward Passes: 48.2

Results saved to:
  - experiments/quick_test/eval_results.yaml
  - experiments/quick_test/eval_results.json
```

**æ³¨æ„**ï¼šå¿«é€Ÿæµ‹è¯•çš„å‡†ç¡®ç‡è¾ƒä½æ˜¯æ­£å¸¸çš„ï¼ˆåªè®­ç»ƒäº† 100 æ ·æœ¬ï¼‰ã€‚

---

## Step 4: æ¨ç†æµ‹è¯• (1 åˆ†é’Ÿ)

æµ‹è¯•äº¤äº’å¼æ¨ç†ï¼š

```bash
python inference.py \
    --checkpoint experiments/quick_test/checkpoint-epoch1 \
    --config configs/llama1b_aug.yaml \
    --mode interactive
```

**äº¤äº’ç¤ºä¾‹**ï¼š
```
Question: What is 5 + 3?

Generating answer...

Answer: 5 + 3 = 8
Forward passes: 18

Question: /quit
```

---

## Step 5: å•ç§å­å®Œæ•´å®éªŒ (2-3 å°æ—¶)

éªŒè¯å®Œæ•´è®­ç»ƒæµç¨‹ï¼š

```bash
python train.py \
    --config configs/llama1b_aug.yaml \
    --seed 42 \
    --wandb
```

**ç›‘æ§**ï¼š
- è®­ç»ƒæ—¶é—´ï¼š~2-3 å°æ—¶ (LLaMA 1B on A100)
- æœ€ç»ˆæŸå¤±ï¼š< 1.0
- ä¿å­˜ checkpoint

ç„¶åè¯„ä¼°ï¼š

```bash
python evaluate.py \
    --checkpoint experiments/llama1b_gsm8k-aug/seed_42/checkpoint-epoch10 \
    --config configs/llama1b_aug.yaml \
    --datasets gsm8k gsm8k-hard svamp \
    --output experiments/llama1b_gsm8k-aug/seed_42/results.yaml
```

**é¢„æœŸç»“æœ**ï¼š
- GSM8k å‡†ç¡®ç‡ï¼š~80-84%
- å‰å‘ä¼ æ’­æ•°ï¼š~45-50

---

## Step 6: å¤šç§å­å®éªŒ (6-9 å°æ—¶)

è¿è¡Œ 3 ä¸ªç§å­è·å¾—ç»Ÿè®¡æ˜¾è‘—æ€§ï¼š

```powershell
.\run_multi_seed_enhanced.ps1 -Config llama1b_aug -Seeds 42,43,44 -UseWandB
```

**æˆ–ä½¿ç”¨ Python ç‰ˆæœ¬**ï¼š
```bash
python run_multi_seed.py \
    --config configs/llama1b_aug.yaml \
    --seeds 42 43 44 \
    --output_dir experiments
```

**é¢„æœŸè¾“å‡º**ï¼š
```
=== FINAL RESULTS ===
Dataset         Accuracy (%)              Forward Passes
gsm8k           82.45 Â± 0.73              48.2 Â± 1.1
gsm8k-hard      68.91 Â± 1.24              52.7 Â± 1.9
svamp           75.33 Â± 0.89              45.1 Â± 1.3

Results based on 3 seeds
```

---

## Step 7: å®Œæ•´å¤ç° (24-48 å°æ—¶)

è¿è¡Œæ‰€æœ‰ 4 ä¸ªé…ç½®ï¼š

```powershell
.\run_all_experiments.ps1
```

è¿™å°†è¿è¡Œï¼š
- LLaMA 3.2-1B + GSM8k-AUG (3 seeds)
- LLaMA 3.2-1B + GSM8k-AUG-NL (3 seeds)
- Qwen2.5-0.5B + GSM8k-AUG (3 seeds)
- LLaMA 3.2-3B + GSM8k-AUG (3 seeds)

èšåˆç»“æœï¼š

```bash
python aggregate_results.py \
    --experiments_dir experiments \
    --output table1_results.csv
```

---

## éªŒè¯æ¸…å•

å®Œæˆæ¯ä¸ªæ­¥éª¤åæ‰“å‹¾ï¼š

- [ ] **Step 1**: çƒŸé›¾æµ‹è¯•é€šè¿‡
- [ ] **Step 2**: å¿«é€Ÿè®­ç»ƒå®Œæˆï¼ˆæ— é”™è¯¯ï¼‰
- [ ] **Step 3**: å¿«é€Ÿè¯„ä¼°å®Œæˆï¼ˆæœ‰ç»“æœï¼‰
- [ ] **Step 4**: æ¨ç†æ­£å¸¸å·¥ä½œ
- [ ] **Step 5**: å•ç§å­å®Œæ•´å®éªŒè¾¾åˆ°é¢„æœŸå‡†ç¡®ç‡
- [ ] **Step 6**: å¤šç§å­ç»Ÿè®¡ç»“æœåˆç†ï¼ˆstd < 2%ï¼‰
- [ ] **Step 7**: å®Œæ•´å¤ç°ä¸è®ºæ–‡ Table 1 å¯¹é½ï¼ˆÂ±1-2%ï¼‰

---

## å¸¸è§é—®é¢˜

### è®­ç»ƒé€Ÿåº¦æ…¢

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š
```yaml
# åœ¨ config YAML ä¸­è°ƒæ•´
training:
  per_device_train_batch_size: 4  # å¢å¤§ batch size
  gradient_accumulation_steps: 4  # å‡å°‘ç´¯ç§¯æ­¥æ•°
  
system:
  mixed_precision: bf16  # ç¡®ä¿å¼€å¯æ··åˆç²¾åº¦
```

### OOM é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
training:
  per_device_train_batch_size: 1  # å‡å° batch size
  gradient_accumulation_steps: 16  # å¢å¤§ç´¯ç§¯æ­¥æ•°ä¿æŒæœ‰æ•ˆ batch size
```

### å‡†ç¡®ç‡ä½äºé¢„æœŸ

**æ£€æŸ¥é¡¹**ï¼š
1. è®­ç»ƒæ˜¯å¦æ”¶æ•›ï¼ˆloss < 1.0ï¼‰
2. å­¦ä¹ ç‡æ˜¯å¦åˆé€‚ï¼ˆæ£€æŸ¥ W&B æ›²çº¿ï¼‰
3. æ•°æ®é›†æ˜¯å¦æ­£ç¡®åŠ è½½
4. è¯„ä¼°æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆç­”æ¡ˆæå–ï¼‰

### æ•°æ®é›†åŠ è½½å¤±è´¥

**æ›¿ä»£æ–¹æ¡ˆ**ï¼š
```python
# ä½¿ç”¨æœ¬åœ°ç¼“å­˜
export HF_DATASETS_OFFLINE=1

# æˆ–æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†
wget https://huggingface.co/datasets/whynlp/gsm8k-aug
```

---

## ä¸‹ä¸€æ­¥

å®ŒæˆéªŒè¯åï¼Œæ‚¨å¯ä»¥ï¼š

1. **è°ƒæ•´è¶…å‚æ•°**ï¼šä¿®æ”¹ configs/ ä¸­çš„é…ç½®
2. **æ·»åŠ æ¶ˆèå®éªŒ**ï¼šæµ‹è¯•ä¸åŒ loss æƒé‡
3. **æ‰©å±•åˆ°å…¶ä»–æ¨¡å‹**ï¼šLLaMA 7B, Mistral ç­‰
4. **è¯„ä¼°å…¶ä»–æ•°æ®é›†**ï¼šMATH, AQuA-RAT ç­‰

å‚è€ƒæ–‡æ¡£ï¼š
- `docs/EXAMPLES.md` - æ›´å¤šä½¿ç”¨ç¤ºä¾‹
- `docs/MULTI_SEED.md` - å¤šç§å­å®éªŒè¯¦ç»†æŒ‡å—
- `docs/INFERENCE.md` - æ¨ç†ä½¿ç”¨æŒ‡å—
