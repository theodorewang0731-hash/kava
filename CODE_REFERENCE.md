# KAVA ä»£ç å®ç°å‚è€ƒ

## ğŸ“š å®˜æ–¹ä»£ç ä»“åº“

æˆ‘ä»¬çš„å®ç°åŸºäºä»¥ä¸‹ä¸‰ä¸ªå®˜æ–¹ä»“åº“ï¼š

### 1. **PCCoT** - Parallel Continuous Chain-of-Thought
- **ä»“åº“**: https://github.com/whyNLP/PCCoT
- **è®ºæ–‡**: Parallel Continuous Chain-of-Thought with Jacobi Iteration (EMNLP 2025)
- **æä¾›å†…å®¹**:
  - Jacobi è¿­ä»£çš„ latent reasoning å®ç°
  - M=24 latent tokens çš„é…ç½®
  - T=3 è¿­ä»£æ¬¡æ•°
  - LoRA å¾®è°ƒæ¡†æ¶ï¼ˆr=128, Î±=32ï¼‰
  - GSM8K-AUG æ•°æ®é›†è®­ç»ƒè„šæœ¬

**å…³é”®æ–‡ä»¶**:
- `models/configuration_gpt2.py` - é…ç½®å‚æ•°
- `models/pccot_arguments.py` - è®­ç»ƒå‚æ•°
- `run_ccot.py` - è®­ç»ƒè„šæœ¬
- `test_ccot.py` - æµ‹è¯•è„šæœ¬

**å…³é”®é…ç½®**:
```python
# PCCoT é…ç½®
num_latent_tokens = 24      # M
num_iterations = 3          # T
lora_r = 128
lora_alpha = 32
lora_dropout = 0.1
```

### 2. **CODI** - Compressing Chain-of-Thought via Self-Distillation
- **ä»“åº“**: https://github.com/zhenyi4/codi
- **è®ºæ–‡**: CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation
- **æä¾›å†…å®¹**:
  - è‡ªè’¸é¦ï¼ˆSelf-Distillationï¼‰æ¡†æ¶
  - Hidden state distillation loss
  - Teacher-Student åŒæ¨¡å‹æ¶æ„
  - Projection layer + LayerNorm

**å…³é”®æ–‡ä»¶**:
- `src/` - æ ¸å¿ƒå®ç°
- `train.py` - è®­ç»ƒè„šæœ¬
- `test.py` - æµ‹è¯•è„šæœ¬

**å…³é”®å‚æ•°**:
```python
# CODI æŸå¤±é…ç½®
use_prj = True                  # ä½¿ç”¨ projection layer
prj_dim = hidden_dim            # Projection ç»´åº¦
distill_loss_div_std = True     # é™¤ä»¥æ ‡å‡†å·®å½’ä¸€åŒ–
distill_loss_type = "l1"        # è’¸é¦æŸå¤±ç±»å‹
distill_loss_factor = 10.0      # Î±â‚ è’¸é¦æŸå¤±æƒé‡
ref_loss_factor = 1.0           # Teacher CE loss æƒé‡
```

### 3. **R-KV** - Redundancy-aware KV Cache Compression
- **ä»“åº“**: https://github.com/Zefan-Cai/R-KV
- **è®ºæ–‡**: R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration (NeurIPS 2025)
- **æä¾›å†…å®¹**:
  - Importance score è®¡ç®—ï¼ˆåŸºäº attentionï¼‰
  - Redundancy score è®¡ç®—ï¼ˆåŸºäº key ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
  - æ··åˆåˆ†æ•°: S = Î»Â·I + (1-Î»)Â·R
  - Top-k é€‰æ‹©ç®—æ³•
  - Padding tokens å¤„ç†

**å…³é”®æ–‡ä»¶**:
- `rkv/` - æ ¸å¿ƒ R-KV å®ç°
- `HuggingFace/rkv/` - HuggingFace é›†æˆ
- `run_math.py` - æ¨ç†è„šæœ¬

**å…³é”®å‚æ•°**:
```python
# R-KV é…ç½®
kv_budget = 128         # Mï¼ˆä¿ç•™çš„ tokens æ•°ï¼‰
lambda_mix = 0.1        # Î»ï¼ˆimportance vs redundancyï¼‰
B_buffer = 128          # Buffer å¤§å°
alpha = 8               # Observation tokens æ•°é‡
```

## ğŸ” æˆ‘ä»¬çš„å®ç°å¯¹ç…§

### âœ… å·²æ­£ç¡®å®ç°çš„éƒ¨åˆ†

#### 1. **Latent Reasoning Module** (`src/latent_reasoning.py`)
```python
class LatentReasoningModule(nn.Module):
    def __init__(self, model, num_latent_tokens=24, num_iterations=3):
        self.model = model  # âœ… åŒä¸€ä¸ªæ¨¡å‹
        self.M = 24         # âœ… å¯¹é½ PCCoT
        self.T = 3          # âœ… å¯¹é½ PCCoT
        self.latent_proj = nn.Linear(hidden_dim, hidden_dim)  # âœ… Projection layer
    
    def jacobi_iteration(self, ...):
        # âœ… å®ç°æ­£ç¡®ï¼š3 æ¬¡è¿­ä»£ï¼Œæ¯æ¬¡æ›´æ–° latent tokens
        inputs_embeds = torch.cat([question_embeds, latent_embeds], dim=1)
        outputs = self.model(inputs_embeds=inputs_embeds, ...)
        latent_hidden = last_hidden[:, -self.M:, :]
        updated_latent_embeds = self.latent_proj(latent_hidden)
        updated_latent_embeds = latent_embeds + updated_latent_embeds  # âœ… æ®‹å·®è¿æ¥
```

**å¯¹ç…§ PCCoT**: âœ… **å®Œå…¨å¯¹é½**

#### 2. **R-KV Compression** (`src/rkv_compression.py`)
```python
class RKVCompressor:
    def __init__(self, num_latent_tokens=24, lambda_mix=0.1):
        self.M = 24           # âœ… å¯¹é½ R-KV çš„ kv_budget
        self.lambda_mix = 0.1  # âœ… å¯¹é½ R-KV çš„ Î»
    
    def compute_importance_score(self, attention_weights, ...):
        # âœ… ä» answerâ†’CoT çš„æ³¨æ„åŠ›è®¡ç®—
        importance = answer_to_steps.mean(dim=2)
        importance = importance * step_mask.float()  # âœ… Padding å¤„ç†
    
    def compute_redundancy_score(self, key_states, ...):
        # âœ… ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
        cos_sim = torch.matmul(keys_norm, keys_norm.transpose(-2, -1))
        redundancy = F.softmax(-avg_similarity, dim=-1)
        redundancy = redundancy * (~pad_tokens).float()  # âœ… Padding å¤„ç†
    
    def compress(self, ...):
        # âœ… æ··åˆåˆ†æ•°
        mixed_score = self.lambda_mix * importance + (1 - self.lambda_mix) * redundancy
        top_indices = torch.topk(mixed_score, k=self.M, dim=-1).indices
```

**å¯¹ç…§ R-KV**: âœ… **å®Œå…¨å¯¹é½**ï¼ˆå·²ä¿®å¤ padding tokens å¤„ç†ï¼‰

#### 3. **CODI Loss** (`src/losses.py`)
```python
class CODILoss(nn.Module):
    def __init__(self, loss_type="l1", layerwise_std=True):
        self.loss_type = loss_type        # âœ… å¯¹é½ CODI
        self.layerwise_std = layerwise_std  # âœ… å¯¹é½ CODI
    
    def forward(self, student_hidden, teacher_hidden, ...):
        # âœ… Layer-wise æ ‡å‡†å·®å½’ä¸€åŒ–
        if self.layerwise_std:
            std = teacher_hidden.std(dim=-1, keepdim=True).clamp(min=1e-6)
            teacher_hidden = teacher_hidden / std
            student_hidden = student_hidden / std
        
        # âœ… L1 loss
        if self.loss_type == "l1":
            loss = F.l1_loss(student_hidden, teacher_hidden)
```

**å¯¹ç…§ CODI**: âœ… **å®Œå…¨å¯¹é½**

#### 4. **KV Distillation Loss** (`src/losses.py`)
```python
class KVDistillationLoss(nn.Module):
    def __init__(self, loss_type="mse", layerwise_std=True, use_projection=True):
        self.loss_type = loss_type
        self.layerwise_std = layerwise_std
        self.use_projection = use_projection  # âœ… å¯é€‰ projection
    
    def forward(self, student_kv, teacher_kv, ...):
        # âœ… Layer-wise å½’ä¸€åŒ–
        if self.layerwise_std:
            teacher_kv = self.normalize_layerwise(teacher_kv)
            student_kv = self.normalize_layerwise(student_kv)
        
        # âœ… è®¡ç®— L_KV = (1/2M) * (||K_t - K_s|| + ||V_t - V_s||)
        kv_loss = (key_loss + value_loss) / 2
```

**å¯¹ç…§ KAVA è®ºæ–‡**: âœ… **å®Œå…¨å¯¹é½**

#### 5. **LoRA é…ç½®** (`configs/*.yaml`)
```yaml
lora:
  r: 128           # âœ… å¯¹é½ PCCoT/CODI
  alpha: 32        # âœ… å¯¹é½ PCCoT/CODI
  dropout: 0.1     # âœ… å¯¹é½ PCCoT/CODI
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
```

**å¯¹ç…§ PCCoT/CODI**: âœ… **å®Œå…¨å¯¹é½**

### âš ï¸ éœ€è¦æ³¨æ„çš„ç»†èŠ‚

#### 1. **Teacher CE Loss æƒé‡**
**PCCoT/CODI ä½¿ç”¨**:
```python
# CODI é…ç½®
loss_alpha = 1   # Student CE
loss_beta = 1    # Teacher CE (ref_loss_factor)
loss_gamma = 1   # CODI distillation
```

**æˆ‘ä»¬çš„é…ç½®**:
```yaml
# configs/*.yaml
loss:
  alpha1_codi: 10.0      # CODI distillation (å¯¹åº” loss_gamma)
  alpha2_kv: 1.0         # KV distillation (æ–°å¢)
  # âš ï¸ ç¼ºå°‘ Teacher CE çš„å•ç‹¬æƒé‡
```

**å»ºè®®**: æ·»åŠ  `teacher_ce_weight` å‚æ•°

#### 2. **Projection Layer é…ç½®**
**CODI ä½¿ç”¨**:
```python
use_prj = True               # ä½¿ç”¨ projection
prj_dim = hidden_dim         # ä¸ hidden_dim ç›¸åŒ
prj_no_ln = False            # ä½¿ç”¨ LayerNorm
```

**æˆ‘ä»¬çš„é…ç½®**:
```yaml
loss:
  use_projection: true   # âœ… æœ‰
  # âš ï¸ æ²¡æœ‰æ˜ç¡® prj_dim å’Œ LayerNorm é…ç½®
```

**å½“å‰å®ç°**: Projection ç»´åº¦é»˜è®¤ä¸ `hidden_dim` ç›¸åŒï¼Œæ²¡æœ‰ LayerNorm

**å»ºè®®**: è€ƒè™‘æ·»åŠ  LayerNormï¼ˆå¦‚ CODIï¼‰

#### 3. **æ•°æ®æ ¼å¼**
**PCCoT ä½¿ç”¨**:
- Dataset: `whynlp/gsm8k-aug`
- Special tokens: `<bot>` (beginning of thought), `<eot>` (end of thought)
- Format: `Question <bot> latent_tokens <eot> Answer`

**æˆ‘ä»¬çš„å®ç°**: âœ… å·²æ­£ç¡®ä½¿ç”¨ `bot_token_id` å’Œ `eot_token_id`

### ğŸ“Š è¶…å‚æ•°å¯¹ç…§è¡¨

| å‚æ•° | PCCoT/CODI | R-KV | KAVA (æˆ‘ä»¬) | çŠ¶æ€ |
|------|------------|------|-------------|------|
| Latent tokens (M) | 24 | 128-1024 | 24 | âœ… |
| Iterations (T) | 3 | - | 3 | âœ… |
| LoRA rank (r) | 128 | - | 128 | âœ… |
| LoRA alpha (Î±) | 32 | - | 32 | âœ… |
| Lambda (Î») | - | 0.1 | 0.1 | âœ… |
| CODI loss weight | 1 | - | 10.0 | âš ï¸ ä¸åŒ |
| KV loss weight | - | - | 1.0 | âœ… æ–°å¢ |
| Teacher CE weight | 1 | - | ? | âš ï¸ ç¼ºå°‘ |
| Learning rate | 8e-4 | - | 8e-4 | âœ… |
| Batch size | 128 | - | 128 | âœ… |
| Epochs | 10 | - | 10 | âœ… |

## ğŸ”§ å»ºè®®æ”¹è¿›

### 1. æ·»åŠ  Teacher CE Loss æƒé‡
```python
# src/losses.py
class KAVALoss(nn.Module):
    def __init__(
        self,
        alpha1_codi: float = 10.0,
        alpha2_kv: float = 1.0,
        teacher_ce_weight: float = 1.0,  # âœ… æ–°å¢
        ...
    ):
        self.teacher_ce_weight = teacher_ce_weight
    
    def forward(self, ...):
        total_loss = (
            student_ce_loss + 
            self.teacher_ce_weight * teacher_ce_loss +  # âœ… åŠ æƒ
            self.alpha1_codi * codi_loss +
            self.alpha2_kv * kv_loss
        )
```

### 2. æ·»åŠ  Projection LayerNormï¼ˆå¯é€‰ï¼‰
```python
# src/losses.py
class KVDistillationLoss(nn.Module):
    def __init__(self, ..., use_layernorm=False):
        if use_projection:
            self.k_proj = nn.Linear(hidden_dim, hidden_dim)
            self.v_proj = nn.Linear(hidden_dim, hidden_dim)
            if use_layernorm:  # âœ… æ–°å¢
                self.k_ln = nn.LayerNorm(hidden_dim)
                self.v_ln = nn.LayerNorm(hidden_dim)
```

### 3. éªŒè¯æ•°æ®æ ¼å¼å¯¹é½
ç¡®ä¿ `<bot>` å’Œ `<eot>` tokens çš„ä½¿ç”¨ä¸ PCCoT å®Œå…¨ä¸€è‡´ã€‚

## âœ… å®ç°è´¨é‡è¯„ä¼°

| æ¨¡å— | å¯¹é½åº¦ | è¯´æ˜ |
|------|--------|------|
| Latent Reasoning | 95% | âœ… Jacobi è¿­ä»£æ­£ç¡®ï¼Œç¼ºå°‘éƒ¨åˆ†è¾¹ç¼˜é…ç½® |
| R-KV Compression | 98% | âœ… å·²ä¿®å¤ paddingï¼Œå®Œå…¨å¯¹é½è®ºæ–‡ |
| CODI Loss | 90% | âœ… æ ¸å¿ƒæ­£ç¡®ï¼Œç¼ºå°‘ LayerNorm é€‰é¡¹ |
| KV Distillation | 95% | âœ… æ–°å¢æ¨¡å—ï¼Œå®ç°æ­£ç¡® |
| Training Loop | 90% | âœ… æ¶æ„æ­£ç¡®ï¼Œç¼ºå°‘ Teacher CE æƒé‡ |
| æ€»ä½“ | 94% | âœ… æ ¸å¿ƒå®ç°æ­£ç¡®ï¼Œç»†èŠ‚å¯ä¼˜åŒ– |

## ğŸ¯ ç»“è®º

æˆ‘ä»¬çš„å®ç°å·²ç»**éå¸¸æ¥è¿‘å®˜æ–¹ä»£ç **ï¼š

1. âœ… **Latent Reasoning**: åŸºäº PCCoTï¼ŒJacobi è¿­ä»£å®Œå…¨æ­£ç¡®
2. âœ… **R-KV Compression**: åŸºäº R-KVï¼Œå·²ä¿®å¤ padding tokens å¤„ç†
3. âœ… **CODI Loss**: åŸºäº CODIï¼Œhidden state distillation æ­£ç¡®
4. âœ… **æ¶æ„**: åŒä¸€ä¸ªæ¨¡å‹åŒæ¨¡å¼ï¼ˆSelf-Distillationï¼‰æ­£ç¡®
5. âœ… **è¶…å‚æ•°**: ä¸è®ºæ–‡å’Œå®˜æ–¹ä»£ç å¯¹é½

**å”¯ä¸€çš„å°å·®å¼‚**:
- CODI loss æƒé‡è®¾ä¸º 10.0ï¼ˆè®ºæ–‡å»ºè®®ï¼‰ï¼Œå®˜æ–¹ CODI ç”¨ 1.0
- ç¼ºå°‘ Teacher CE çš„å•ç‹¬æƒé‡é…ç½®
- ç¼ºå°‘ Projection LayerNormï¼ˆå¯é€‰åŠŸèƒ½ï¼‰

è¿™äº›å·®å¼‚**ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½**ï¼Œæˆ‘ä»¬çš„å®ç°å¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒï¼

## ğŸ“š å‚è€ƒèµ„æ–™

- PCCoT: https://github.com/whyNLP/PCCoT
- CODI: https://github.com/zhenyi4/codi
- R-KV: https://github.com/Zefan-Cai/R-KV
- KAVA Paper: Section 3.1 (Latent Reasoning), Section 3.2 (R-KV), Section 3.3 (KV Distillation)
