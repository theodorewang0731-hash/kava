# HPC å…¬å…±æ¨¡åž‹åº“ä½¿ç”¨è¯´æ˜Ž

## ðŸ“ æ¨¡åž‹åº“ä½ç½®

```
/home/share/models
```

HPC é›†ç¾¤ç®¡ç†å‘˜ç»´æŠ¤çš„å…±äº« HuggingFace æ¨¡åž‹ç¼“å­˜ï¼ŒåŒ…å«å¸¸ç”¨å¼€æºæ¨¡åž‹ï¼ŒæŒç»­æ›´æ–°ã€‚

---

## âœ¨ ä¼˜åŠ¿

- âœ… **é›¶ç­‰å¾…**ï¼šæ— éœ€ä¸‹è½½ï¼Œç«‹å³å¼€å§‹è®­ç»ƒ
- âœ… **èŠ‚çœç©ºé—´**ï¼šå¤šç”¨æˆ·å…±äº«ï¼Œå•ä¸ªæ¨¡åž‹ä»…å­˜å‚¨ä¸€æ¬¡
- âœ… **ç¨³å®šå¯é **ï¼šé¿å…ç½‘ç»œè¶…æ—¶å’Œä¸‹è½½å¤±è´¥
- âœ… **æŒç»­æ›´æ–°**ï¼šç®¡ç†å‘˜å®šæœŸæ·»åŠ æœ€æ–°æ¨¡åž‹

---

## ðŸ”§ å¿«é€Ÿé…ç½®

### æ–¹æ³• 1: è‡ªåŠ¨é…ç½®è„šæœ¬ï¼ˆæŽ¨èï¼‰

```bash
# ä¸€é”®é…ç½®
chmod +x setup_hpc_models.sh
./setup_hpc_models.sh

# é‡æ–°åŠ è½½
source ~/.bashrc
```

### æ–¹æ³• 2: æ‰‹åŠ¨é…ç½®

```bash
# æ·»åŠ åˆ° ~/.bashrc
cat >> ~/.bashrc << 'EOF'
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
export HF_DATASETS_CACHE=/home/share/models
EOF

# ç«‹å³ç”Ÿæ•ˆ
source ~/.bashrc
```

### æ–¹æ³• 3: ä»…å½“å‰ä¼šè¯

```bash
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
export HF_DATASETS_CACHE=/home/share/models
```

---

## ðŸ“¦ KAVA é¡¹ç›®å¯ç”¨æ¨¡åž‹

é…ç½®åŽï¼Œä»¥ä¸‹æ¨¡åž‹å¯ç›´æŽ¥ä½¿ç”¨ï¼š

```bash
# æ£€æŸ¥æ¨¡åž‹æ˜¯å¦å­˜åœ¨
ls /home/share/models/models--meta-llama--Llama-3.2-1B-Instruct
ls /home/share/models/models--meta-llama--Llama-3.2-3B-Instruct
ls /home/share/models/models--Qwen--Qwen2.5-0.5B-Instruct
```

âœ… **LLaMA 3.2-1B** (`meta-llama/Llama-3.2-1B-Instruct`)  
âœ… **LLaMA 3.2-3B** (`meta-llama/Llama-3.2-3B-Instruct`)  
âœ… **Qwen 2.5-0.5B** (`Qwen/Qwen2.5-0.5B-Instruct`)

---

## ðŸ’» ä½¿ç”¨ç¤ºä¾‹

### Python ä»£ç 

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# ç›´æŽ¥ä½¿ç”¨æ¨¡åž‹åç§°ï¼ˆè‡ªåŠ¨ä»Žå…±äº«ç¼“å­˜åŠ è½½ï¼‰
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")

# ä¸éœ€è¦æŒ‡å®š cache_dirï¼ŒçŽ¯å¢ƒå˜é‡å·²é…ç½®
```

### è®­ç»ƒè„šæœ¬

```bash
# é…ç½®çŽ¯å¢ƒå˜é‡åŽç›´æŽ¥è¿è¡Œ
python train.py --config configs/llama1b_aug.yaml

# æ¨¡åž‹è‡ªåŠ¨ä»Ž /home/share/models åŠ è½½ï¼Œæ— éœ€ä¸‹è½½
```

### SLURM è„šæœ¬

```bash
#!/bin/bash
#SBATCH --job-name=kava
#SBATCH --gres=gpu:a100-sxm4-80gb:1

# é…ç½®æ¨¡åž‹åº“
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
export HF_DATASETS_CACHE=/home/share/models

# è¿è¡Œè®­ç»ƒï¼ˆè‡ªåŠ¨ä½¿ç”¨å…±äº«æ¨¡åž‹ï¼‰
python train.py --config configs/llama1b_aug.yaml
```

---

## âœ… éªŒè¯é…ç½®

### æ£€æŸ¥çŽ¯å¢ƒå˜é‡

```bash
echo $HF_HOME
echo $TRANSFORMERS_CACHE
echo $HF_DATASETS_CACHE

# åº”è¯¥è¾“å‡º: /home/share/models
```

### æµ‹è¯• Python åŠ è½½

```bash
python -c "
import os
print('HF_HOME:', os.environ.get('HF_HOME'))
print('Path exists:', os.path.exists('/home/share/models'))

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B-Instruct')
print('âœ… Successfully loaded from shared cache')
"
```

### è¿è¡Œå¿«é€Ÿæµ‹è¯•

```bash
# ä½¿ç”¨å…±äº«æ¨¡åž‹è¿è¡Œ smoke test
python smoke_test.py

# æŸ¥çœ‹æ˜¯å¦ä»Žå…±äº«è·¯å¾„åŠ è½½
# æ—¥å¿—åº”æ˜¾ç¤º: Loading model from /home/share/models/...
```

---

## ðŸ” æŸ¥çœ‹å¯ç”¨æ¨¡åž‹

```bash
# åˆ—å‡ºæ‰€æœ‰æ¨¡åž‹
ls -1 /home/share/models/models--*

# æŸ¥çœ‹æ¨¡åž‹è¯¦æƒ…
ls -lh /home/share/models/models--meta-llama--Llama-3.2-1B-Instruct

# ç»Ÿè®¡æ¨¡åž‹æ•°é‡
ls -1 /home/share/models/models--* | wc -l

# æŸ¥çœ‹æ¨¡åž‹æ€»å¤§å°
du -sh /home/share/models
```

---

## âš ï¸ æ•…éšœæŽ’é™¤

### é—®é¢˜ 1: ä»ç„¶å°è¯•ä¸‹è½½æ¨¡åž‹

**ç—‡çŠ¶**ï¼šçœ‹åˆ° "Downloading model..." æç¤º

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥çŽ¯å¢ƒå˜é‡
echo $HF_HOME

# å¦‚æžœä¸ºç©ºï¼Œé‡æ–°é…ç½®
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
```

### é—®é¢˜ 2: æƒé™æ‹’ç»

**ç—‡çŠ¶**ï¼š`Permission denied: /home/share/models`

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥ç›®å½•æƒé™
ls -ld /home/share/models

# å¦‚æžœæ— æƒé™ï¼Œè”ç³»ç®¡ç†å‘˜
```

### é—®é¢˜ 3: æ¨¡åž‹ä¸å­˜åœ¨

**ç—‡çŠ¶**ï¼š`Model not found in /home/share/models`

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥æ¨¡åž‹åˆ—è¡¨
ls /home/share/models/models--*/

# å¦‚æžœæ¨¡åž‹ç¡®å®žä¸å­˜åœ¨ï¼Œæœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š

# é€‰é¡¹ A: è¯·æ±‚ç®¡ç†å‘˜æ·»åŠ æ¨¡åž‹
# å‘é€é‚®ä»¶ç»™ HPC ç®¡ç†å‘˜ï¼Œè¯´æ˜Žéœ€è¦çš„æ¨¡åž‹

# é€‰é¡¹ B: ä¸‹è½½åˆ°ä¸ªäººç›®å½•ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰
export HF_ENDPOINT=https://hf-mirror.com
export HF_HOME=$HOME/.cache/huggingface
python train.py --config configs/llama1b_aug.yaml
```

### é—®é¢˜ 4: Conda çŽ¯å¢ƒæœªè‡ªåŠ¨åŠ è½½

**ç—‡çŠ¶**ï¼šæ¯æ¬¡æ¿€æ´»çŽ¯å¢ƒéƒ½éœ€è¦é‡æ–°è®¾ç½®

**è§£å†³**ï¼š
```bash
# é…ç½® Conda æ¿€æ´»è„šæœ¬
conda activate kava
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
cat > $CONDA_PREFIX/etc/conda/activate.d/hf_models.sh << 'EOF'
#!/bin/bash
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
export HF_DATASETS_CACHE=/home/share/models
EOF
chmod +x $CONDA_PREFIX/etc/conda/activate.d/hf_models.sh

# é‡æ–°æ¿€æ´»éªŒè¯
conda deactivate && conda activate kava
echo $HF_HOME  # åº”è¯¥è¾“å‡º /home/share/models
```

---

## ðŸ“š ç›¸å…³æ–‡æ¡£

- **å®Œæ•´å¤çŽ°æŒ‡å—**: [REPRODUCTION_GUIDE.md](REPRODUCTION_GUIDE.md)
- **HPC å‘½ä»¤å‚è€ƒ**: [HPC_REFERENCE.md](HPC_REFERENCE.md)
- **äº¤äº’å¼ä½¿ç”¨**: [SLURM_INTERACTIVE_GUIDE.md](SLURM_INTERACTIVE_GUIDE.md)
- **é…ç½®è„šæœ¬**: `setup_hpc_models.sh`

---

## ðŸ’¡ æœ€ä½³å®žè·µ

1. **é¦–æ¬¡ä½¿ç”¨**ï¼šè¿è¡Œ `setup_hpc_models.sh` è‡ªåŠ¨é…ç½®
2. **éªŒè¯é…ç½®**ï¼šæ¯æ¬¡ç™»å½•æ£€æŸ¥ `echo $HF_HOME`
3. **SLURM è„šæœ¬**ï¼šåœ¨è„šæœ¬å¼€å¤´è®¾ç½®çŽ¯å¢ƒå˜é‡
4. **æ—¥å¿—æ£€æŸ¥**ï¼šè®­ç»ƒæ—¶æŸ¥çœ‹æ˜¯å¦ä»Ž `/home/share/models` åŠ è½½
5. **å®šæœŸæ›´æ–°**ï¼šå…³æ³¨ç®¡ç†å‘˜å…¬å‘Šï¼Œäº†è§£æ–°å¢žæ¨¡åž‹

---

## ðŸ“ž èŽ·å–å¸®åŠ©

- **æ£€æŸ¥é…ç½®**ï¼š`python -c "import os; print(os.environ.get('HF_HOME'))"`
- **éªŒè¯æ¨¡åž‹**ï¼š`ls /home/share/models/models--meta-llama*`
- **è”ç³»ç®¡ç†å‘˜**ï¼šå¦‚éœ€æ·»åŠ æ–°æ¨¡åž‹æˆ–é‡åˆ°æƒé™é—®é¢˜

---

**æ›´æ–°æ—¥æœŸ**: 2025-01-17  
**ç»´æŠ¤è€…**: KAVA Project Team
