# KAVA æ¨¡å‹ä¸‹è½½æŒ‡å—

**HPC å…¬å…±æ¨¡å‹åº“ä¸åŒ…å« KAVA æ‰€éœ€æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆ**

---

## âš ï¸ é‡è¦å‘ç°

ç»è¿‡æ£€æŸ¥ HPC å…¬å…±æ¨¡å‹åº“ï¼ˆ`/home/share/models`ï¼‰ï¼Œå‘ç°**æ²¡æœ‰ KAVA é¡¹ç›®æ‰€éœ€çš„ç‰¹å®šæ¨¡å‹**ã€‚

### KAVA éœ€è¦çš„æ¨¡å‹

| æ¨¡å‹ | HuggingFace ID | åœ¨å…¬å…±åº“ï¼Ÿ | å¤§å° |
|------|---------------|-----------|------|
| LLaMA 3.2-1B | `meta-llama/Llama-3.2-1B-Instruct` | âŒ å¦ | ~5 GB |
| LLaMA 3.2-3B | `meta-llama/Llama-3.2-3B-Instruct` | âŒ å¦ | ~12 GB |
| Qwen 2.5-0.5B | `Qwen/Qwen2.5-0.5B-Instruct` | âŒ å¦ | ~2 GB |

**æ€»ä¸‹è½½å¤§å°**ï¼š~19 GB

### HPC å…¬å…±åº“å®é™…åŒ…å«çš„æ¨¡å‹

é€šè¿‡ `ls /home/share/models` æŸ¥çœ‹ï¼Œå…¬å…±åº“åŒ…å«ï¼š

**Llama ç³»åˆ—**ï¼ˆä½†ä¸æ˜¯ Llama-3.2ï¼‰ï¼š
- `llama-7b`
- `Llama-2-7b`, `Llama-2-13b`, `Llama-2-70b`
- `Llama-30b`, `llama-65b`

**Code ç³»åˆ—**ï¼š
- `CodeLlama-7b/13b/34b/70b-hf/Instruct/Python`

**Qwen ç³»åˆ—**ï¼ˆä½†ä¸æ˜¯ Qwen2.5-0.5Bï¼‰ï¼š
- `Qwen1.5-MoE-A2.7B`

**å…¶ä»–æ¨¡å‹**ï¼š
- `Mistral-7B`, `Mixtral-8x7B`
- `phi-1/2`, `gemma-2b/7b`
- `deepseek-coder`, `deepseek-llm`
- `WizardCoder`, `WizardLM`
- `vicuna`, `stable-code`

---

## ğŸš€ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ A: ä¸‹è½½åˆ°ä¸ªäººç›®å½•ï¼ˆæ¨èï¼‰

è¿™æ˜¯æœ€ç›´æ¥çš„æ–¹æ¡ˆï¼Œå°†æ¨¡å‹ä¸‹è½½åˆ°ä½ çš„ä¸ªäººç¼“å­˜ç›®å½•ã€‚

#### Step 1: é…ç½®ç¯å¢ƒå˜é‡

```bash
# é…ç½®ä¸ªäºº HuggingFace ç¼“å­˜
cat >> ~/.bashrc << 'EOF'
# HuggingFace ä¸ªäººç¼“å­˜ï¼ˆKAVA é¡¹ç›®ï¼‰
export HF_HOME=$HOME/.cache/huggingface
export TRANSFORMERS_CACHE=$HOME/.cache/huggingface
export HF_DATASETS_CACHE=$HOME/.cache/huggingface
EOF

# ç«‹å³ç”Ÿæ•ˆ
source ~/.bashrc

# éªŒè¯
echo $HF_HOME
# è¾“å‡ºï¼š/home/username/.cache/huggingface
```

#### Step 2: ä¸‹è½½æ¨¡å‹ï¼ˆ3 ç§æ–¹æ³•ï¼‰

**æ–¹æ³• 1: ç›´æ¥ä¸‹è½½ï¼ˆå¦‚æœç½‘ç»œå¥½ï¼‰**

```bash
# å®‰è£… huggingface-cli
pip install -U huggingface-hub

# ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 35-55 åˆ†é’Ÿï¼‰
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct
huggingface-cli download meta-llama/Llama-3.2-3B-Instruct
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct

# æ˜¾ç¤ºè¿›åº¦
# Fetching 15 files:   60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [05:23<03:35, 35.89s/it]
```

**æ–¹æ³• 2: ä½¿ç”¨ä»£ç†åŠ é€Ÿï¼ˆæ¨èï¼‰**

```bash
# === åœ¨æœ¬åœ°æœºå™¨ ===
# 1. å¯åŠ¨ Clash/Shadowrocketï¼Œå¯ç”¨ "Allow LAN"
# 2. å»ºç«‹åå‘éš§é“
ssh -N -R 55555:localhost:7890 username@hpc.example.edu &

# === åœ¨ HPC ç»ˆç«¯ ===
# 3. é…ç½®ä»£ç†
export http_proxy=http://localhost:55555
export https_proxy=http://localhost:55555
export all_proxy=http://localhost:55555

# 4. æµ‹è¯•è¿æ¥
curl -I https://huggingface.co
# HTTP/2 200

# 5. ä¸‹è½½æ¨¡å‹ï¼ˆé€šè¿‡ä»£ç†åŠ é€Ÿï¼‰
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct
huggingface-cli download meta-llama/Llama-3.2-3B-Instruct
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct
```

**æ–¹æ³• 3: ä½¿ç”¨ HuggingFace é•œåƒ**

```bash
# é…ç½®é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# ä¸‹è½½æ¨¡å‹
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct
huggingface-cli download meta-llama/Llama-3.2-3B-Instruct
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct
```

#### Step 3: éªŒè¯ä¸‹è½½

```bash
# æ£€æŸ¥ä¸‹è½½çš„æ¨¡å‹
ls -lh ~/.cache/huggingface/hub/

# åº”è¯¥çœ‹åˆ°ï¼š
# models--meta-llama--Llama-3.2-1B-Instruct/
# models--meta-llama--Llama-3.2-3B-Instruct/
# models--Qwen--Qwen2.5-0.5B-Instruct/

# æµ‹è¯•åŠ è½½
python << EOF
from transformers import AutoTokenizer

# æµ‹è¯• LLaMA 1B
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
print("âœ“ LLaMA 3.2-1B loaded successfully")

# æµ‹è¯• LLaMA 3B
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")
print("âœ“ LLaMA 3.2-3B loaded successfully")

# æµ‹è¯• Qwen
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
print("âœ“ Qwen 2.5-0.5B loaded successfully")
EOF
```

---

### æ–¹æ¡ˆ B: è‡ªåŠ¨ä¸‹è½½ï¼ˆè®­ç»ƒæ—¶ï¼‰

å¦‚æœä½ ä¸æƒ³æ‰‹åŠ¨ä¸‹è½½ï¼Œå¯ä»¥åœ¨é¦–æ¬¡è®­ç»ƒæ—¶è‡ªåŠ¨ä¸‹è½½ï¼š

```bash
# é…ç½®ç¯å¢ƒå˜é‡ï¼ˆåŒä¸Šï¼‰
export HF_HOME=$HOME/.cache/huggingface
export TRANSFORMERS_CACHE=$HOME/.cache/huggingface
export HF_DATASETS_CACHE=$HOME/.cache/huggingface

# é¦–æ¬¡è¿è¡Œè®­ç»ƒæ—¶ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹
python train.py --config configs/llama1b_aug.yaml

# è¾“å‡ºï¼š
# Downloading model meta-llama/Llama-3.2-1B-Instruct...
# Fetching 15 files:  100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [10:23<00:00]
# Model downloaded to ~/.cache/huggingface/hub/
# Starting training...
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ— éœ€æ‰‹åŠ¨æ“ä½œ
- âœ… æŒ‰éœ€ä¸‹è½½

**ç¼ºç‚¹**ï¼š
- âŒ å ç”¨è®­ç»ƒä»»åŠ¡æ—¶é—´
- âŒ å¯èƒ½å¯¼è‡´ä»»åŠ¡è¶…æ—¶ï¼ˆå¦‚æœä¸‹è½½å¤ªæ…¢ï¼‰
- âŒ å¤šä¸ªä»»åŠ¡ä¼šé‡å¤ä¸‹è½½ï¼ˆå¦‚æœåŒæ—¶å¯åŠ¨ï¼‰

---

### æ–¹æ¡ˆ C: è¯·æ±‚ç®¡ç†å‘˜æ·»åŠ ï¼ˆå¤šç”¨æˆ·ï¼‰

å¦‚æœä½ çš„å›¢é˜Ÿæœ‰å¤šäººéœ€è¦è¿™äº›æ¨¡å‹ï¼Œå¯ä»¥è¯·æ±‚ç®¡ç†å‘˜æ·»åŠ åˆ°å…¬å…±åº“ï¼š

```bash
# ç»™ç®¡ç†å‘˜çš„é‚®ä»¶æ¨¡æ¿
ä¸»é¢˜ï¼šè¯·æ±‚æ·»åŠ æ¨¡å‹åˆ° HPC å…¬å…±åº“

æ‚¨å¥½ï¼Œ

æˆ‘ä»¬çš„ç ”ç©¶é¡¹ç›®ï¼ˆKAVAï¼‰éœ€è¦ä½¿ç”¨ä»¥ä¸‹æ¨¡å‹ï¼š
1. meta-llama/Llama-3.2-1B-Instruct (~5 GB)
2. meta-llama/Llama-3.2-3B-Instruct (~12 GB)
3. Qwen/Qwen2.5-0.5B-Instruct (~2 GB)

è¿™äº›æ¨¡å‹ç›®å‰ä¸åœ¨ /home/share/models ä¸­ã€‚
å¦‚æœèƒ½æ·»åŠ åˆ°å…¬å…±åº“ï¼Œå°†èŠ‚çœæ‰€æœ‰ç”¨æˆ·çš„ä¸‹è½½æ—¶é—´å’Œå­˜å‚¨ç©ºé—´ã€‚

ä¸‹è½½å‘½ä»¤ï¼š
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct
huggingface-cli download meta-llama/Llama-3.2-3B-Instruct
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct

æ„Ÿè°¢ï¼
```

---

## ğŸ“Š ä¸‹è½½æ—¶é—´ä¼°ç®—

| ç½‘ç»œç¯å¢ƒ | LLaMA 1B (5GB) | LLaMA 3B (12GB) | Qwen 0.5B (2GB) | æ€»è®¡ (19GB) |
|---------|----------------|-----------------|-----------------|-------------|
| ç›´è¿ HuggingFace | 15-30 åˆ†é’Ÿ | 30-60 åˆ†é’Ÿ | 5-10 åˆ†é’Ÿ | 50-100 åˆ†é’Ÿ |
| ä½¿ç”¨ä»£ç† | 5-10 åˆ†é’Ÿ | 10-20 åˆ†é’Ÿ | 2-5 åˆ†é’Ÿ | 17-35 åˆ†é’Ÿ |
| HF é•œåƒ | 10-20 åˆ†é’Ÿ | 20-40 åˆ†é’Ÿ | 3-8 åˆ†é’Ÿ | 33-68 åˆ†é’Ÿ |

**æ¨è**ï¼šä½¿ç”¨ä»£ç†åŠ é€Ÿï¼ˆæ–¹æ¡ˆ A æ–¹æ³• 2ï¼‰ï¼Œæœ€å¿« **17-35 åˆ†é’Ÿ**å®Œæˆæ‰€æœ‰ä¸‹è½½ã€‚

---

## ğŸ” éªŒè¯ä¸‹è½½å®Œæ•´æ€§

```bash
# æ£€æŸ¥æ–‡ä»¶æ•°é‡
find ~/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct -type f | wc -l
# åº”è¯¥æœ‰ 15-20 ä¸ªæ–‡ä»¶

# æ£€æŸ¥æ€»å¤§å°
du -sh ~/.cache/huggingface/hub/models--*

# è¾“å‡ºç¤ºä¾‹ï¼š
# 5.2G    models--meta-llama--Llama-3.2-1B-Instruct
# 12.8G   models--meta-llama--Llama-3.2-3B-Instruct
# 2.1G    models--Qwen--Qwen2.5-0.5B-Instruct

# æµ‹è¯•åŠ è½½é€Ÿåº¦
python << EOF
import time
from transformers import AutoModelForCausalLM

start = time.time()
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-1B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)
elapsed = time.time() - start
print(f"âœ“ Model loaded in {elapsed:.2f}s")
# é¢„æœŸï¼š5-15 ç§’
EOF
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. ä½¿ç”¨äº¤äº’å¼ä¼šè¯ä¸‹è½½

```bash
# ç”³è¯· GPU èŠ‚ç‚¹ï¼ˆè™½ç„¶ä¸‹è½½ä¸éœ€è¦ GPUï¼Œä½†é¿å…å ç”¨ç™»å½•èŠ‚ç‚¹ï¼‰
srun --time=2:00:00 --mem=16G --pty bash -i

# é…ç½®ç¯å¢ƒ
conda activate kava
export HF_HOME=$HOME/.cache/huggingface

# ä¸‹è½½æ¨¡å‹
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct
huggingface-cli download meta-llama/Llama-3.2-3B-Instruct
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct

# å®Œæˆåé€€å‡º
exit
```

### 2. åå°ä¸‹è½½ï¼ˆé¿å… SSH æ–­å¼€ï¼‰

```bash
# ä½¿ç”¨ nohup åå°ä¸‹è½½
nohup huggingface-cli download meta-llama/Llama-3.2-1B-Instruct > download_llama1b.log 2>&1 &
nohup huggingface-cli download meta-llama/Llama-3.2-3B-Instruct > download_llama3b.log 2>&1 &
nohup huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct > download_qwen05b.log 2>&1 &

# æŸ¥çœ‹è¿›åº¦
tail -f download_llama1b.log

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep huggingface-cli
```

### 3. ç£ç›˜ç©ºé—´ç®¡ç†

```bash
# æ£€æŸ¥ç£ç›˜é…é¢
quota -s

# æ£€æŸ¥ HuggingFace ç¼“å­˜å¤§å°
du -sh ~/.cache/huggingface

# å¦‚æœç©ºé—´ä¸è¶³ï¼Œæ¸…ç†æ—§æ¨¡å‹
rm -rf ~/.cache/huggingface/hub/models--old-model-name

# æˆ–è½¯é“¾æ¥åˆ°å…¶ä»–ç›®å½•ï¼ˆå¦‚æœæœ‰å¤§å®¹é‡å­˜å‚¨ï¼‰
mkdir -p /scratch/username/huggingface
mv ~/.cache/huggingface /scratch/username/
ln -s /scratch/username/huggingface ~/.cache/huggingface
```

---

## ğŸš¨ æ•…éšœæ’é™¤

### é—®é¢˜ 1: ä¸‹è½½ä¸­æ–­

```bash
# ç—‡çŠ¶ï¼šä¸‹è½½åˆ°ä¸€åŠæ–­å¼€
# ConnectionError: HTTPSConnectionPool

# è§£å†³ï¼šé‡æ–°è¿è¡Œä¸‹è½½å‘½ä»¤ï¼Œä¼šè‡ªåŠ¨ç»­ä¼ 
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct

# æˆ–ä½¿ç”¨ --resume-download
huggingface-cli download --resume-download meta-llama/Llama-3.2-1B-Instruct
```

### é—®é¢˜ 2: ç£ç›˜é…é¢ä¸è¶³

```bash
# ç—‡çŠ¶ï¼šNo space left on device

# è§£å†³ï¼šä½¿ç”¨ scratch ç›®å½•
export HF_HOME=/scratch/$USER/huggingface
mkdir -p $HF_HOME
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct
```

### é—®é¢˜ 3: æƒé™é”™è¯¯

```bash
# ç—‡çŠ¶ï¼šPermission denied

# è§£å†³ï¼šæ£€æŸ¥ç›®å½•æƒé™
ls -ld ~/.cache/huggingface
chmod 755 ~/.cache/huggingface
```

### é—®é¢˜ 4: æ¨¡å‹åŠ è½½å¤±è´¥

```bash
# ç—‡çŠ¶ï¼šOSError: Model not found

# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $HF_HOME
echo $TRANSFORMERS_CACHE

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls ~/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/

# å¦‚æœæ–‡ä»¶ä¸å®Œæ•´ï¼Œåˆ é™¤å¹¶é‡æ–°ä¸‹è½½
rm -rf ~/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [GETTING_STARTED_HPC.md](GETTING_STARTED_HPC.md) - HPC å®Œæ•´ä¸Šæ‰‹æŒ‡å—
- [HPC_REFERENCE.md](HPC_REFERENCE.md) - HPC å‘½ä»¤å‚è€ƒ
- [SSH_PORT_FORWARDING.md](SSH_PORT_FORWARDING.md) - ä»£ç†é…ç½®è¯¦è§£
- [REPRODUCTION_GUIDE.md](REPRODUCTION_GUIDE.md) - å®Œæ•´å¤ç°æŒ‡å—

---

## ğŸ¯ å¿«é€Ÿå‘½ä»¤å¤‡å¿˜

```bash
# é…ç½®ç¯å¢ƒ
export HF_HOME=$HOME/.cache/huggingface
echo 'export HF_HOME=$HOME/.cache/huggingface' >> ~/.bashrc

# ä¸‹è½½æ¨¡å‹ï¼ˆç›´è¿ï¼‰
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct
huggingface-cli download meta-llama/Llama-3.2-3B-Instruct
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct

# ä¸‹è½½æ¨¡å‹ï¼ˆä½¿ç”¨ä»£ç†ï¼‰
export all_proxy=http://localhost:55555
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct

# éªŒè¯
ls ~/.cache/huggingface/hub/
python -c "from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B-Instruct')"
```

---

**é¢„è®¡ä¸‹è½½æ—¶é—´**ï¼š17-100 åˆ†é’Ÿï¼ˆå–å†³äºç½‘ç»œï¼‰  
**ç£ç›˜ç©ºé—´éœ€æ±‚**ï¼š~19 GB  
**ä¸€æ¬¡æ€§æ“ä½œ**ï¼šæ¨¡å‹ä¸‹è½½åæ°¸ä¹…ä¿å­˜ï¼Œåç»­è®­ç»ƒæ— éœ€é‡å¤ä¸‹è½½
