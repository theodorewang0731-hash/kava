# KAVA å¯†é›†ç›‘ç£ä¿®å¤æ–¹æ¡ˆ - ä»£ç ç¤ºä¾‹

## 1. ä¿®æ”¹ `src/latent_reasoning.py` - ä¿å­˜æ‰€æœ‰è¿­ä»£çš„ KV

### åŸå§‹ forward_student æ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰
```python
def forward_student(self, ...):
    # ...
    for t in range(self.T):
        latent_embeds, outputs = self.jacobi_iteration(...)
        if t == self.T - 1:
            final_outputs = outputs
    
    return {
        'logits': final_outputs['logits'],
        'latent_kv': extract_kv(...),  # ä»…æœ€åä¸€æ­¥
        'hidden_states': final_outputs['hidden_states'],
    }
```

### ä¿®å¤åçš„ç‰ˆæœ¬
```python
def forward_student(self, ...):
    # ...
    all_outputs = []  # âœ… æ–°å¢ï¼šä¿å­˜æ‰€æœ‰è¿­ä»£çš„è¾“å‡º
    
    for t in range(self.T):
        latent_embeds, outputs = self.jacobi_iteration(...)
        all_outputs.append(outputs)  # âœ… æ–°å¢ï¼šæ”¶é›†æ¯ä¸€æ­¥çš„è¾“å‡º
        
        if t == self.T - 1:
            final_outputs = outputs
    
    # âœ… æ–°å¢ï¼šæå–æ‰€æœ‰æ­¥éª¤çš„ KV ç¼“å­˜
    all_kv_steps = []
    for t, outputs in enumerate(all_outputs):
        kv_step = extract_kv_from_output(outputs)  # [batch, ..., M, head_dim]
        all_kv_steps.append(kv_step)
    
    return {
        'logits': final_outputs['logits'],
        'all_kv_steps': all_kv_steps,  # âœ… æ–°å¢ï¼š[(KV_z1), (KV_z2), (KV_z3), (KV_a)]
        'final_kv': all_kv_steps[-1],   # ä¿ç•™å‘åå…¼å®¹
        'hidden_states': final_outputs['hidden_states'],
    }
```

---

## 2. ä¿®æ”¹ `src/trainer.py` - å¯¹æ‰€æœ‰æ­¥éª¤è®¡ç®— KV è’¸é¦æŸå¤±

### åŸå§‹ train_step ä¸­å…³äºæŸå¤±è®¡ç®—çš„éƒ¨åˆ†
```python
def train_step(self, batch_data):
    # ... teacher forward ...
    # ... r-kv compression ...
    
    # âš ï¸ é—®é¢˜ï¼šä»…å–æœ€åä¸€æ­¥
    student_keys, student_values = student_outputs['latent_kv']
    
    loss, loss_dict = self.criterion(
        ...
        student_keys=student_keys.unsqueeze(0),
        student_values=student_values.unsqueeze(0),
        teacher_keys=teacher_keys_compressed.unsqueeze(0),
        teacher_values=teacher_values_compressed.unsqueeze(0),
        ...
    )
```

### ä¿®å¤åçš„ç‰ˆæœ¬
```python
def train_step(self, batch_data):
    # ... existing code for teacher forward ...
    
    # ========== R-KV COMPRESSION FOR ALL STEPS ==========
    # âœ… æ–°å¢ï¼šä¸ºæ¯ä¸€ä¸ª Jacobi è¿­ä»£æ­¥éª¤å‹ç¼© KV
    teacher_all_kv_steps = teacher_outputs['all_kv_steps']  # éœ€è¦ teacher ä¹Ÿè¿”å›
    teacher_kv_compressed_all = []
    
    for step_idx, (teacher_kv_step) in enumerate(teacher_all_kv_steps):
        # å¯¹æ¯ä¸€æ­¥è¿›è¡Œ R-KV å‹ç¼©
        kv_compressed = self.rkv_compressor.compress(
            key_cache=teacher_kv_step[0],  # Keys
            value_cache=teacher_kv_step[1],  # Values
            attention_weights=teacher_attention,
            # ... other parameters ...
        )
        teacher_kv_compressed_all.append(kv_compressed)
    
    # ========== STUDENT FORWARD ==========
    student_outputs = self.latent_module.forward_student(...)
    
    # âœ… ä¿®æ”¹ï¼šè·å–æ‰€æœ‰æ­¥éª¤çš„ KV
    student_kv_all_steps = student_outputs['all_kv_steps']  # [(KV_z1), (KV_z2), (KV_z3), (KV_a)]
    
    # ========== COMPUTE LOSSES ==========
    # ... prepare labels ...
    
    # âœ… ä¿®æ”¹ï¼šä¼ å…¥æ‰€æœ‰æ­¥éª¤çš„ KV
    loss, loss_dict = self.criterion(
        student_logits=student_outputs['logits'],
        student_labels=student_labels,
        teacher_logits=teacher_outputs['logits'],
        teacher_labels=teacher_labels,
        student_kv_all_steps=student_kv_all_steps,  # âœ… æ–°å¢
        teacher_kv_all_steps=teacher_kv_compressed_all,  # âœ… æ–°å¢
        student_hidden_states=student_outputs['hidden_states'],
        teacher_hidden_states=teacher_outputs['hidden_states'],
        distill_token_idx=-self.config['latent']['num_tokens']-1
    )
```

---

## 3. ä¿®æ”¹ `src/losses.py` - æ”¯æŒå¤šæ­¥éª¤ KV è’¸é¦

### åŸå§‹ KAVALoss.forward æ–¹æ³•
```python
def forward(self, ..., student_keys, student_values, teacher_keys, teacher_values, ...):
    # ... compute CE losses ...
    
    # âš ï¸ é—®é¢˜ï¼šä»…è®¡ç®—æœ€åä¸€æ­¥çš„ KV è’¸é¦
    loss_kv = self.kv_loss(
        teacher_keys,
        teacher_values,
        student_keys,
        student_values
    )
    
    total_loss = (
        loss_student_ce +
        loss_teacher_ce +
        self.alpha1 * loss_codi +
        self.alpha2 * loss_kv  # ä»…æœ€åä¸€æ­¥
    )
```

### ä¿®å¤åçš„ç‰ˆæœ¬
```python
def forward(
    self,
    student_logits, student_labels,
    teacher_logits, teacher_labels,
    student_kv_all_steps,  # âœ… æ–°å¢ï¼š[(KV_z1_s, KV_z1_s), ..., (KV_a_s, KV_a_s)]
    teacher_kv_all_steps,  # âœ… æ–°å¢ï¼š[(KV_z1_t, KV_z1_t), ..., (KV_a_t, KV_a_t)]
    student_hidden_states, teacher_hidden_states,
    distill_token_idx,
    ...
):
    """
    Compute full KAVA loss with dense supervision.
    
    New: Supervise all Jacobi iteration steps, not just the final answer.
    """
    
    # 1. Student CE loss
    loss_student_ce = self.compute_ce_loss(student_logits, student_labels)
    
    # 2. Teacher CE loss
    loss_teacher_ce = self.compute_ce_loss(teacher_logits, teacher_labels)
    
    # 3. CODI loss
    loss_codi = self.codi_loss(
        teacher_hidden_states,
        student_hidden_states,
        distill_token_idx
    )
    
    # 4. âœ… ä¿®æ”¹ï¼šå¤šæ­¥éª¤ KV è’¸é¦æŸå¤±ï¼ˆå¯†é›†ç›‘ç£ï¼‰
    kv_losses_per_step = []
    
    for step_idx, (student_kv_step, teacher_kv_step) in enumerate(
        zip(student_kv_all_steps, teacher_kv_all_steps)
    ):
        # å¯¹æ¯ä¸ª Jacobi è¿­ä»£æ­¥éª¤è®¡ç®— KV è’¸é¦æŸå¤±
        loss_kv_step = self.kv_loss(
            teacher_keys=teacher_kv_step[0],    # Teacher keys at step t
            teacher_values=teacher_kv_step[1],  # Teacher values at step t
            student_keys=student_kv_step[0],    # Student keys at step t
            student_values=student_kv_step[1]   # Student values at step t
        )
        kv_losses_per_step.append(loss_kv_step)
        
        # å¯é€‰ï¼šè®°å½•æ¯ä¸€æ­¥çš„æŸå¤±ç”¨äºè°ƒè¯•
        self.last_kv_losses = kv_losses_per_step
    
    # å¯¹æ‰€æœ‰æ­¥éª¤çš„ KV æŸå¤±å–å¹³å‡ï¼ˆå¯†é›†ç›‘ç£ï¼‰
    loss_kv_total = torch.stack(kv_losses_per_step).mean()
    
    # Total loss
    total_loss = (
        loss_student_ce +
        loss_teacher_ce +
        self.alpha1 * loss_codi +
        self.alpha2 * loss_kv_total  # âœ… ä¿®æ”¹ï¼šä½¿ç”¨æ‰€æœ‰æ­¥éª¤çš„å¹³å‡æŸå¤±
    )
    
    # Return with detailed loss breakdown
    return total_loss, {
        'loss_student_ce': loss_student_ce.item() if hasattr(loss_student_ce, 'item') else float(loss_student_ce),
        'loss_teacher_ce': loss_teacher_ce.item() if hasattr(loss_teacher_ce, 'item') else float(loss_teacher_ce),
        'loss_codi': loss_codi.item() if hasattr(loss_codi, 'item') else float(loss_codi),
        'loss_kv_total': loss_kv_total.item() if hasattr(loss_kv_total, 'item') else float(loss_kv_total),
        'kv_losses_per_step': [
            l.item() if hasattr(l, 'item') else float(l) 
            for l in kv_losses_per_step
        ],  # âœ… æ–°å¢ï¼šç”¨äºç›‘æ§æ¯ä¸€æ­¥çš„ç›‘ç£æ•ˆæœ
        'total_loss': total_loss.item() if hasattr(total_loss, 'item') else float(total_loss),
    }
```

---

## 4. ä¿®æ”¹ `forward_teacher()` - ä¹Ÿè¦ä¿å­˜ä¸­é—´æ­¥éª¤

### éœ€è¦ç¡®è®¤çš„ä¿®æ”¹
```python
def forward_teacher(self, input_ids, attention_mask, ...):
    """
    Teacher forward pass.
    
    âœ… æ–°å¢è¦æ±‚ï¼šä¹Ÿè¦è¿”å›æ‰€æœ‰ä¸­é—´æ­¥éª¤çš„ KV ç¼“å­˜ï¼ˆå¦‚æœæ˜¯å¤šæ­¥æ¨ç†ï¼‰
    """
    
    outputs = self.model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        output_hidden_states=True,
        use_cache=True,
        return_dict=True
    )
    
    # âœ… å¦‚æœæ•™å¸ˆä¹Ÿä½¿ç”¨ Jacobi è¿­ä»£ï¼Œéœ€è¦ä¿å­˜æ‰€æœ‰æ­¥éª¤
    # âœ… å¦‚æœæ•™å¸ˆåªæ˜¯ä¸€æ­¥å‰å‘ï¼Œåˆ™ä»…ä¿å­˜è¯¥æ­¥éª¤ï¼ˆä½†åŒ…è£…æˆåˆ—è¡¨ä¾¿äºä¸å­¦ç”Ÿå¯¹é½ï¼‰
    
    return {
        'logits': outputs.logits,
        'hidden_states': outputs.hidden_states,
        'past_key_values': outputs.past_key_values,
        'all_kv_steps': [extract_kv(outputs)],  # åŒ…è£…æˆåˆ—è¡¨ï¼Œä¾¿äºä¸å­¦ç”Ÿå¯¹é½
        'attentions': outputs.attentions,
    }
```

---

## 5. é…ç½®æ–‡ä»¶ - ç¡®è®¤è¶…å‚æ•°

### `configs/llama1b_aug.yaml`
```yaml
# âœ… ç¡®ä¿è¿™äº›å€¼æ­£ç¡®
loss:
  alpha1_codi: 10.0       # âœ“ è®ºæ–‡ Table 6
  alpha2_kv: 1.0          # âœ“ è®ºæ–‡ Table 6
  kv_loss_type: "smooth_l1"
  layerwise_std: true     # âœ“ LLaMA-1B éœ€è¦
```

### `configs/llama3b_aug.yaml`
```yaml
# âœ… æ³¨æ„ 3B æ¨¡å‹çš„ä¸åŒé…ç½®
loss:
  alpha1_codi: 20.0       # âœ“ è®ºæ–‡ Table 6ï¼šæ›´å¤§çš„æ¨¡å‹æƒé‡åŠ å€
  alpha2_kv: 2.0          # âœ“ è®ºæ–‡ Table 6ï¼šKV æƒé‡ä¹ŸåŠ å€
  kv_loss_type: "smooth_l1"
  layerwise_std: false    # âœ“ LLaMA-3B ä¸éœ€è¦å±‚çº§å½’ä¸€åŒ–
```

---

## æµ‹è¯•éªŒè¯

ä¿®å¤åï¼ŒéªŒè¯è¾“å‡ºæ—¥å¿—åº”è¯¥åŒ…å«ï¼š

```
[Step 100] Loss breakdown:
  - student_ce: 2.31
  - teacher_ce: 1.89
  - codi: 0.45
  - kv_step_1: 0.82        # âœ… æ–°å¢ï¼šæ¯ä¸€æ­¥çš„ KV æŸå¤±
  - kv_step_2: 0.78        # âœ… æ–°å¢
  - kv_step_3: 0.75        # âœ… æ–°å¢
  - kv_step_4: 0.71        # âœ… æ–°å¢ï¼šæœ€åç­”æ¡ˆæ­¥éª¤
  - kv_total: 0.77 (mean)  # âœ… æ–°å¢ï¼šå¹³å‡å€¼
  - total: 5.83
```

---

## æ€§èƒ½é¢„æœŸ

ä¿®å¤å‰åçš„å‡†ç¡®ç‡å¯¹æ¯”ï¼š

```
æœªä¿®å¤ï¼š
  GSM8K: ~81-82% (ç¼ºå°‘ä¸­é—´ç›‘ç£)
  GSM8K-Hard: ~68-69%
  SVAMP: ~75-76%

ä¿®å¤åï¼ˆé¢„æœŸï¼‰ï¼š
  GSM8K: ~83.7% (+1.7-2.7%) âœ“
  GSM8K-Hard: ~70.5%
  SVAMP: ~77.3%
```

---

## ä¼˜å…ˆçº§æ€»ç»“

```
ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆå…³é”®ï¼‰
  1. ä¿å­˜æ‰€æœ‰ Jacobi è¿­ä»£çš„ KV â† å¼€å§‹è¿™é‡Œ
  2. è®¡ç®—æ‰€æœ‰æ­¥éª¤çš„ KV è’¸é¦æŸå¤±

ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆé‡è¦ï¼‰
  3. æ•™å¸ˆç«¯ä¹Ÿè¿”å›ä¸­é—´æ­¥éª¤
  4. éªŒè¯è¶…å‚æ•°é…ç½®

ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰
  5. æ€§èƒ½ç›‘æ§æ—¥å¿—
  6. å¯è§†åŒ–è°ƒè¯•å·¥å…·
```

---

**ä¼°è®¡ä¿®å¤æ—¶é—´**ï¼š2-3 å°æ—¶
**å½±å“èŒƒå›´**ï¼šæ ¸å¿ƒè®­ç»ƒé€»è¾‘ï¼Œéœ€è¦ä»”ç»†æµ‹è¯•
**å›å½’é£é™©**ï¼šä¸­ç­‰ï¼ˆä¿®æ”¹äº†æ ¸å¿ƒæŸå¤±è®¡ç®—ï¼‰
