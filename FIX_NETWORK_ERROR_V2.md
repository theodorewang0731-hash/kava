# ğŸ”§ ä¿®å¤ "Network is unreachable" é—®é¢˜ - æ­£ç¡®ç‰ˆæœ¬

## ğŸ“‹ é—®é¢˜æ ¹æºåˆ†æ

### âŒ ä¹‹å‰çš„è¯¯è§£
æˆ‘ä¹‹å‰è®¤ä¸ºéœ€è¦ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ŒæŠŠ `meta-llama/Llama-3.2-1B-Instruct` æ”¹æˆæœ¬åœ°è·¯å¾„ `/home/share/models/Llama-3.2-1B-Instruct`ã€‚

### âœ… çœŸæ­£çš„é—®é¢˜
**HPC å…±äº«åº“é‡Œçš„æ¨¡å‹æ˜¯æŒ‰ç…§ HuggingFace æ ‡å‡†æ ¼å¼å­˜å‚¨çš„**ï¼š
```
/home/share/models/
â”œâ”€â”€ models--meta-llama--Llama-3.2-1B-Instruct/
â”‚   â””â”€â”€ snapshots/<hash>/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ model.safetensors
â”‚       â””â”€â”€ ...
â”œâ”€â”€ models--Qwen--Qwen2.5-0.5B-Instruct/
â”‚   â””â”€â”€ snapshots/<hash>/
â”‚       â””â”€â”€ ...
```

å½“è®¾ç½® `HF_HOME=/home/share/models` æ—¶ï¼Œtransformers **åº”è¯¥èƒ½æ‰¾åˆ°è¿™äº›æ¨¡å‹**ï¼Œä½†ä»£ç ä¸­çš„ `from_pretrained()` **é»˜è®¤ä¼šå…ˆå°è¯•è”ç½‘**æ£€æŸ¥æ›´æ–°ï¼Œå³ä½¿æœ¬åœ°å·²æœ‰å®Œæ•´æ¨¡å‹ã€‚

---

## âœ… æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼ˆä»£ç çº§ä¿®å¤ï¼‰

### ä¿®æ”¹ 1: `src/trainer.py` - æ·»åŠ  `local_files_only` å‚æ•°

**é—®é¢˜ä»£ç **ï¼š
```python
self.model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map='auto',
    trust_remote_code=True
)
```

**ä¿®å¤å**ï¼š
```python
# æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç¦»çº¿æ¨¡å¼
use_local = os.path.exists(model_name) or os.environ.get('HUGGINGFACE_HUB_OFFLINE') == '1'

self.model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map='auto',
    trust_remote_code=True,
    local_files_only=use_local  # âœ… å…³é”®å‚æ•°
)
```

### ä¿®æ”¹ 2: `evaluate.py` - åŒæ ·çš„ä¿®å¤

å·²åœ¨è¯„ä¼°è„šæœ¬ä¸­æ·»åŠ ç›¸åŒçš„ `local_files_only` å‚æ•°ã€‚

### ä¿®æ”¹ 3: `submit_multi_seed.slurm` - ç¯å¢ƒå˜é‡ä¿æŒä¸å˜

SLURM è„šæœ¬ä¸­çš„ç¯å¢ƒå˜é‡**å·²ç»æ­£ç¡®**ï¼š
```bash
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
export HUGGINGFACE_HUB_OFFLINE=1
export TRANSFORMERS_OFFLINE=1
```

---

## ğŸ” å·¥ä½œåŸç†

### HuggingFace æ¨¡å‹æŸ¥æ‰¾é€»è¾‘

å½“è°ƒç”¨ `from_pretrained("meta-llama/Llama-3.2-1B-Instruct")` æ—¶ï¼š

1. **é»˜è®¤è¡Œä¸º**ï¼ˆ`local_files_only=False`ï¼‰ï¼š
   ```
   â‘  å°è¯•è”ç½‘åˆ° huggingface.co æ£€æŸ¥æœ€æ–°ç‰ˆæœ¬
   â‘¡ å¦‚æœç½‘ç»œå¤±è´¥ â†’ æŠ›å‡º "Network is unreachable"
   â‘¢ å³ä½¿æœ¬åœ°ç¼“å­˜å­˜åœ¨ä¹Ÿä¸ä¼šä½¿ç”¨ï¼ˆå› ä¸ºç¬¬â‘ æ­¥å°±å¤±è´¥äº†ï¼‰
   ```

2. **ç¦»çº¿æ¨¡å¼**ï¼ˆ`local_files_only=True`ï¼‰ï¼š
   ```
   â‘  ç›´æ¥æŸ¥æ‰¾æœ¬åœ°ç¼“å­˜ï¼š$HF_HOME/models--meta-llama--Llama-3.2-1B-Instruct/
   â‘¡ å¦‚æœæ‰¾åˆ° â†’ ç›´æ¥åŠ è½½
   â‘¢ å¦‚æœæ²¡æ‰¾åˆ° â†’ ç«‹å³æŠ¥é”™ï¼ˆä¸å°è¯•è”ç½‘ï¼‰
   ```

3. **æ™ºèƒ½åˆ¤æ–­**ï¼ˆæˆ‘ä»¬çš„æ–¹æ¡ˆï¼‰ï¼š
   ```python
   # å¦‚æœè®¾ç½®äº† HUGGINGFACE_HUB_OFFLINE=1ï¼Œä½¿ç”¨ç¦»çº¿æ¨¡å¼
   use_local = os.environ.get('HUGGINGFACE_HUB_OFFLINE') == '1'
   
   # æˆ–è€…å¦‚æœ model_name æ˜¯æœ¬åœ°è·¯å¾„ï¼ˆå¦‚ /home/share/models/xxxï¼‰ï¼Œä¹Ÿç”¨ç¦»çº¿
   use_local = os.path.exists(model_name) or use_local
   ```

---

## ğŸ“Š é…ç½®æ–‡ä»¶æ— éœ€ä¿®æ”¹

**ä¿æŒåŸæ ·**ï¼ˆä½¿ç”¨æ ‡å‡† repo IDï¼‰ï¼š
```yaml
model:
  name: "meta-llama/Llama-3.2-1B-Instruct"  # âœ… æ ‡å‡†æ ¼å¼
```

**ä¸è¦æ”¹æˆ**ï¼ˆè¿™æ ·åè€Œä¸å¯¹ï¼‰ï¼š
```yaml
model:
  name: "/home/share/models/Llama-3.2-1B-Instruct"  # âŒ é”™è¯¯
```

**åŸå› **ï¼š
- HPC å…±äº«åº“ä½¿ç”¨ HuggingFace æ ‡å‡†ç¼“å­˜æ ¼å¼ï¼ˆ`models--<org>--<model>/snapshots/<hash>/`ï¼‰
- transformers é€šè¿‡ repo ID è‡ªåŠ¨è§£æè·¯å¾„
- ç›´æ¥æŒ‡å®šè·¯å¾„ä¼šè·³è¿‡ç¼“å­˜æœºåˆ¶ï¼Œå¯èƒ½æ‰¾ä¸åˆ°æ–‡ä»¶

---

## ğŸš€ æµ‹è¯•æ­¥éª¤

### æ­¥éª¤ 1: éªŒè¯ä»£ç ä¿®å¤

```bash
cd "/home/rpwang/kava review"
source venv/bin/activate

# è®¾ç½®ç¯å¢ƒå˜é‡
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
export HUGGINGFACE_HUB_OFFLINE=1
export TRANSFORMERS_OFFLINE=1

# å¿«é€Ÿæµ‹è¯•åŠ è½½
python -c "
import os
os.environ['HF_HOME'] = '/home/share/models'
os.environ['HUGGINGFACE_HUB_OFFLINE'] = '1'

from transformers import AutoConfig

# æµ‹è¯•åŠ è½½é…ç½®ï¼ˆä¸åŠ è½½æ•´ä¸ªæ¨¡å‹ï¼Œé€Ÿåº¦å¿«ï¼‰
config = AutoConfig.from_pretrained(
    'meta-llama/Llama-3.2-1B-Instruct',
    local_files_only=True
)
print(f'âœ“ æˆåŠŸåŠ è½½ Llama-1B é…ç½®')
print(f'  æ¨¡å‹ç±»å‹: {config.model_type}')
print(f'  éšè—å±‚: {config.hidden_size}')
"
```

**é¢„æœŸè¾“å‡º**ï¼š
```
âœ“ æˆåŠŸåŠ è½½ Llama-1B é…ç½®
  æ¨¡å‹ç±»å‹: llama
  éšè—å±‚: 2048
```

### æ­¥éª¤ 2: å•ä»»åŠ¡æµ‹è¯•

```bash
# æäº¤æœ€å°ä»»åŠ¡
sbatch --export=CONFIG=qwen05b_aug --array=0 submit_multi_seed.slurm

# ç­‰å¾… 2-3 åˆ†é’ŸæŸ¥çœ‹æ—¥å¿—
tail -n 100 outputs/logs/kava_qwen05b_aug_*.out
```

**æˆåŠŸæ ‡å¿—**ï¼š
```
Loading base model...
Model: Qwen/Qwen2.5-0.5B-Instruct
Loading mode: Local/Offline              â† âœ… åº”è¯¥æ˜¾ç¤ºè¿™ä¸ª
âœ“ Model loaded successfully
```

**ä¸åº”å‡ºç°**ï¼š
```
Network is unreachable
Cannot connect to huggingface.co
```

### æ­¥éª¤ 3: æäº¤æ‰€æœ‰ä»»åŠ¡

```bash
bash submit_all_jobs.sh
bash monitor_jobs.sh --auto
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆä¸ç›´æ¥æ”¹é…ç½®æ–‡ä»¶è·¯å¾„ï¼Ÿ

**A**: HPC çš„æ¨¡å‹å­˜å‚¨æ ¼å¼æ˜¯ï¼š
```
/home/share/models/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/<hash>/
```

å¦‚æœé…ç½®æ–‡ä»¶å†™ `/home/share/models/Llama-3.2-1B-Instruct`ï¼Œtransformers ä¼šå»æ‰¾ï¼š
```
/home/share/models/Llama-3.2-1B-Instruct/config.json  â† âŒ ä¸å­˜åœ¨
```

æ­£ç¡®çš„åšæ³•æ˜¯è®¾ç½® `HF_HOME` + ä½¿ç”¨æ ‡å‡† repo IDï¼Œtransformers ä¼šè‡ªåŠ¨è§£ææˆï¼š
```
/home/share/models/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/<hash>/config.json  â† âœ… å­˜åœ¨
```

### Q2: `local_files_only` å’Œ `HUGGINGFACE_HUB_OFFLINE` æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**: 
- `HUGGINGFACE_HUB_OFFLINE=1`ï¼šç¯å¢ƒå˜é‡ï¼Œå…¨å±€è®¾ç½®
- `local_files_only=True`ï¼šå‚æ•°ï¼Œå•æ¬¡è°ƒç”¨è®¾ç½®

æˆ‘ä»¬çš„ä»£ç ä¼šè¯»å–ç¯å¢ƒå˜é‡ï¼Œè‡ªåŠ¨å†³å®šæ˜¯å¦ä¼  `local_files_only=True`ã€‚

### Q3: å¦‚æœ HPC çš„æ¨¡å‹æ ¼å¼ä¸æ˜¯æ ‡å‡†ç¼“å­˜æ ¼å¼æ€ä¹ˆåŠï¼Ÿ

**A**: è¿è¡Œè¿™ä¸ªå‘½ä»¤æ£€æŸ¥ï¼š
```bash
ls -la /home/share/models/

# å¦‚æœçœ‹åˆ° models--meta-llama--Llama-3.2-1B-Instruct/ â†’ æ ‡å‡†æ ¼å¼ âœ…
# å¦‚æœçœ‹åˆ° Llama-3.2-1B-Instruct/config.json â†’ ç›´æ¥æ ¼å¼ï¼ˆéœ€è¦æ”¹é…ç½®ï¼‰
```

å¦‚æœæ˜¯ç›´æ¥æ ¼å¼ï¼Œé‚£ä¹ˆ**ä¹‹å‰çš„æ–¹æ¡ˆæ˜¯å¯¹çš„**ï¼ˆæ”¹é…ç½®æ–‡ä»¶ç”¨æœ¬åœ°è·¯å¾„ï¼‰ã€‚

---

## ğŸ“ éªŒè¯ HPC æ¨¡å‹æ ¼å¼

è¯· AI åŠ©æ‰‹è¿è¡Œè¿™ä¸ªå‘½ä»¤ç¡®è®¤ï¼š

```bash
# æ£€æŸ¥æ¨¡å‹å­˜å‚¨æ ¼å¼
echo "=== HPC æ¨¡å‹åº“æ ¼å¼æ£€æŸ¥ ==="
ls -la /home/share/models/ | grep -i llama | head -5
echo ""
ls -la /home/share/models/ | grep -i qwen | head -5

# å¦‚æœçœ‹åˆ° models--xxx æ ¼å¼
if ls /home/share/models/models--* 2>/dev/null | grep -q .; then
    echo "âœ“ ä½¿ç”¨ HuggingFace æ ‡å‡†ç¼“å­˜æ ¼å¼"
    echo "  â†’ é…ç½®æ–‡ä»¶åº”ä½¿ç”¨: meta-llama/Llama-3.2-1B-Instruct"
    echo "  â†’ ä»£ç å·²ä¿®å¤: local_files_only=True"
else
    echo "âœ— ä½¿ç”¨ç›´æ¥ç›®å½•æ ¼å¼"
    echo "  â†’ é…ç½®æ–‡ä»¶åº”ä½¿ç”¨: /home/share/models/Llama-3.2-1B-Instruct"
fi
```

---

## âœ¨ ä¿®å¤æ€»ç»“

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | åŸå›  |
|------|----------|------|
| `src/trainer.py` | æ·»åŠ  `local_files_only=use_local` | é¿å…è”ç½‘æ£€æŸ¥ |
| `evaluate.py` | æ·»åŠ  `local_files_only=use_local` | é¿å…è”ç½‘æ£€æŸ¥ |
| `configs/*.yaml` | **ä¿æŒåŸæ ·**ï¼ˆrepo IDï¼‰ | é…åˆ HF_HOME ä½¿ç”¨ |
| `submit_multi_seed.slurm` | **å·²æ­£ç¡®**ï¼ˆç¯å¢ƒå˜é‡ï¼‰ | æ— éœ€ä¿®æ”¹ |

**æ ¸å¿ƒåŸç†**ï¼š
```python
# å½“ HUGGINGFACE_HUB_OFFLINE=1 æ—¶
from_pretrained(
    "meta-llama/Llama-3.2-1B-Instruct",  # repo ID
    local_files_only=True                  # å¼ºåˆ¶ç¦»çº¿
)

# transformers è‡ªåŠ¨æŸ¥æ‰¾ï¼š
# $HF_HOME/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/<hash>/
```

è¿™æ ·æ—¢ä¿æŒäº†é…ç½®æ–‡ä»¶çš„æ ‡å‡†æ€§ï¼Œåˆè§£å†³äº†ç½‘ç»œè®¿é—®é—®é¢˜ï¼
