# KAVA: Latent Reasoning via Compressed KV-Cache Distillation

[![Implementation](https://img.shields.io/badge/Implementation-Complete-brightgreen)](STATUS.md)
[![Paper](https://img.shields.io/badge/Paper-arXiv%3A2510.02312-blue)](https://arxiv.org/abs/2510.02312)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](requirements.txt)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)](requirements.txt)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

**Strict Paper Implementation for Reproducibility**

This repository implements KAVA (Latent Reasoning via Compressed KV-Cache Distillation) following the exact specifications from the paper, including all hyperparameters from Table 6.

**ğŸ‰ Status**: âœ… Implementation complete with multi-seed automation, interactive inference, and comprehensive documentation.

## ğŸ“„ Paper Reference

**Title:** Latent Reasoning via Compressed KV-Cache Distillation  
**Authors:** Shen & Wu (2025)  
**ArXiv:** [2510.02312](https://arxiv.org/abs/2510.02312)

This implementation reproduces:
- **Table 1:** Accuracy on GSM8k, GSM8k-Hard, SVAMP
- **Table 2:** Average forward passes
- **Table 6:** All hyperparameter configurations

## ğŸ¯ Key Features

âœ… **Exact Paper Configuration**
- LLaMA 3.2-1B/3B-Instruct models
- Qwen2.5-0.5B-Instruct model
- GSM8k-AUG (equation-only CoT)
- GSM8k-AUG-NL (natural language CoT)

âœ… **Core Components Implemented**
- **R-KV Compression:** Importance + Redundancy scoring
- **KV Distillation Loss:** Smooth L1 / MSE with layer-wise normalization
- **CODI Loss:** Hidden state distillation
- **PCCoT Latent Reasoning:** 24 tokens, 3 Jacobi iterations
- **LoRA Fine-tuning:** r=128, Î±=32

âœ… **All Table 6 Hyperparameters**
- Loss weights (Î±â‚, Î±â‚‚)
- Learning rates (2e-4 to 8e-4)
- Loss types (Smooth L1 vs MSE)
- Layer-wise std normalization flags
- Projection layer flags

## ğŸš€ Quick Start

### âš¡ æœ€ç®€å•æ–¹å¼ï¼šLinux HPC ä¸€é”®å¯åŠ¨ï¼ˆæ¨èï¼‰

```bash
# 1. ä¸Šä¼ ä»£ç åˆ° HPC
scp -r kava/ user@hpc:/home/user/

# 2. SSH ç™»å½•å¹¶å¯åŠ¨
ssh user@hpc
cd ~/kava
bash start.sh  # è‡ªåŠ¨éªŒè¯+é…ç½®+ä¸‹è½½+è®­ç»ƒ
```

**å®Œæˆï¼** ä»…éœ€ 3 æ­¥ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰äº‹æƒ…ã€‚

**è¯¦ç»†é€‰é¡¹ï¼š**
```bash
bash start.sh --verify-only   # ä»…éªŒè¯ç¯å¢ƒ
bash start.sh --method mirror # ä½¿ç”¨ä¸­å›½é•œåƒä¸‹è½½
bash start.sh --skip-download # è·³è¿‡æ¨¡å‹ä¸‹è½½ï¼ˆå·²ç¼“å­˜ï¼‰
```

**ğŸ¤– ä½¿ç”¨ HPC AI åŠ©æ‰‹ï¼Ÿ** æŸ¥çœ‹è¿™äº›å¼•å¯¼æ–‡æ¡£ï¼š
- **[AI_ASSISTANT_PROMPT.md](AI_ASSISTANT_PROMPT.md)** - å®Œæ•´çš„ AI åŠ©æ‰‹æç¤ºè¯
- **[PROMPT_FOR_AI.txt](PROMPT_FOR_AI.txt)** - å¿«é€Ÿæç¤ºè¯ï¼ˆå¯ç›´æ¥å¤åˆ¶ï¼‰
- **[CONVERSATION_GUIDE.md](CONVERSATION_GUIDE.md)** - åˆ†æ­¥å¯¹è¯è„šæœ¬

---

### ğŸ¯ ä¸€é”®å¤ç°ï¼ˆHPC é›†ç¾¤æ¨èï¼‰

**æœ€å¿«æ–¹å¼ï¼š** åªéœ€ä¸€æ¡å‘½ä»¤å³å¯å®Œæˆæ‰€æœ‰é…ç½®å’Œè®­ç»ƒï¼

```bash
# 1. ä¸Šä¼ ä»£ç åˆ° HPC
scp -r kava/ user@hpc:/home/user/

# 2. ç™»å½•å¹¶è¿è¡Œè‡ªåŠ¨åŒ–è„šæœ¬
ssh user@hpc
cd ~/kava
bash run_reproduce.sh  # è‡ªåŠ¨å®Œæˆï¼šç¯å¢ƒ+æ¨¡å‹ä¸‹è½½+ä½œä¸šæäº¤
```

**è¯¦ç»†è¯´æ˜ï¼š** å‚è§ **[REPRODUCTION_CHECKLIST.md](REPRODUCTION_CHECKLIST.md)** è·å–å®Œæ•´æ¸…å•

---

### ğŸ“š HPC éƒ¨ç½²æ–‡æ¡£ï¼ˆæŒ‰éœ€é˜…è¯»ï¼‰

**å¦‚æœä½ æƒ³äº†è§£ç»†èŠ‚æˆ–é‡åˆ°é—®é¢˜**ï¼Œå‚è€ƒä»¥ä¸‹æ–‡æ¡£ï¼š

1. **[REPRODUCTION_CHECKLIST.md](REPRODUCTION_CHECKLIST.md)** - å¿«é€Ÿå¯åŠ¨æ¸…å•ï¼ˆ5åˆ†é’Ÿï¼‰
2. **[GETTING_STARTED_HPC.md](docs/GETTING_STARTED_HPC.md)** - å®Œæ•´ HPC æŒ‡å—ï¼ˆ30åˆ†é’Ÿï¼‰
3. **[KAVA_MODEL_DOWNLOAD.md](docs/KAVA_MODEL_DOWNLOAD.md)** - æ¨¡å‹ä¸‹è½½è¯¦è§£ï¼ˆ17-100åˆ†é’Ÿï¼‰

âš ï¸ **é‡è¦**ï¼šHPC å…¬å…±æ¨¡å‹åº“**æ²¡æœ‰ KAVA æ‰€éœ€æ¨¡å‹**ï¼ˆLlama-3.2, Qwen2.5ï¼‰  
âœ… **è§£å†³æ–¹æ¡ˆ**ï¼š`run_reproduce.sh` ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° `$HOME/.cache/huggingface`ï¼ˆ~19GBï¼‰

---

### ğŸ†• æ–°æ‰‹å¿…è¯»ï¼šHPC ä¸Šæ‰‹æŒ‡å—

**å¦‚æœä½ æ˜¯ç¬¬ä¸€æ¬¡åœ¨ HPC ä¸Šè¿è¡Œæœ¬é¡¹ç›®**ï¼Œè¯·ç›´æ¥é˜…è¯»ï¼š

ğŸ‘‰ **[GETTING_STARTED_HPC.md](docs/GETTING_STARTED_HPC.md)** ğŸ‘ˆ

è¿™ä¸ªæŒ‡å—å°†å¸¦ä½ å®Œæˆï¼š
1. âœ… ä¸Šä¼ é¡¹ç›®åˆ° HPCï¼ˆ5 åˆ†é’Ÿï¼‰
2. âœ… ä¸€é”®è‡ªåŠ¨é…ç½®ç¯å¢ƒï¼ˆ15 åˆ†é’Ÿï¼‰
3. âœ… **è‡ªåŠ¨ä¸‹è½½æ‰€éœ€æ¨¡å‹**ï¼ˆ17-100 åˆ†é’Ÿï¼‰
4. âœ… æäº¤è®­ç»ƒä»»åŠ¡ï¼ˆ5 åˆ†é’Ÿï¼‰
5. âœ… ç›‘æ§è¿›åº¦å¹¶ç”Ÿæˆè®ºæ–‡ç»“æœï¼ˆ48 å°æ—¶è‡ªåŠ¨è¿è¡Œï¼‰

**æ€»è®¡**ï¼š30 åˆ†é’Ÿé…ç½® + æ¨¡å‹ä¸‹è½½ + 48 å°æ—¶è®­ç»ƒ â†’ å¾—åˆ°è®ºæ–‡ Table 1 & 2 ç»“æœï¼

---

### HPC Cluster Setup (æ‰‹åŠ¨æ–¹å¼)

å¦‚æœéœ€è¦æ‰‹åŠ¨é…ç½®è€Œéä½¿ç”¨ `run_reproduce.sh`ï¼š

**å¿«é€Ÿé…ç½®ä¸ªäººç¯å¢ƒ**:

```bash
# é…ç½®ä¸ªäºº HuggingFace ç¼“å­˜
cat >> ~/.bashrc << 'EOF'
export HF_HOME=$HOME/.cache/huggingface
export TRANSFORMERS_CACHE=$HOME/.cache/huggingface
export HF_DATASETS_CACHE=$HOME/.cache/huggingface
EOF

source ~/.bashrc

# ä¸‹è½½æ¨¡å‹ï¼ˆ17-100 åˆ†é’Ÿï¼Œä½¿ç”¨ä»£ç†å¯åŠ é€Ÿï¼‰
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct
huggingface-cli download meta-llama/Llama-3.2-3B-Instruct
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct
```

**æäº¤è®­ç»ƒä»»åŠ¡**:

```bash
# æ–¹æ³• 1: ä¸€é”®æäº¤æ‰€æœ‰å®éªŒï¼ˆ4 é…ç½® Ã— 3 ç§å­ï¼‰
./hpc_run_all.sh

# æ–¹æ³• 2: å•ä¸ªé…ç½®æäº¤
sbatch --export=CONFIG=llama1b_aug submit_multi_seed.slurm
```

**è¯¦ç»†æ–‡æ¡£**ï¼š
- ğŸŒŸ æ–°æ‰‹æŒ‡å—: [GETTING_STARTED_HPC.md](GETTING_STARTED_HPC.md)
- ğŸ“¦ æ¨¡å‹ä¸‹è½½: [KAVA_MODEL_DOWNLOAD.md](KAVA_MODEL_DOWNLOAD.md) âš ï¸ **å¿…è¯»**
- ğŸ“– HPC å‚è€ƒ: [HPC_REFERENCE.md](HPC_REFERENCE.md)
- ğŸ”¬ å®Œæ•´å¤ç°: [REPRODUCTION_GUIDE.md](REPRODUCTION_GUIDE.md)

---

### Installation

```powershell
# Install dependencies
pip install -r requirements.txt

# Verify installation (2 minutes, no GPU needed)
python smoke_test.py
```

### Multi-Seed Experiment (Recommended)

The fastest way to get statistical results:

```powershell
# One-line multi-seed training + evaluation + aggregation
.\run_multi_seed.ps1 -Config llama1b_aug -Seeds 42,123,456
```

This will:
1. Train 3 models with different seeds (6-8 hours)
2. Evaluate on GSM8k, GSM8k-Hard, SVAMP (30 minutes)
3. Aggregate results with mean Â± std statistics (1 minute)

See [Multi-Seed Guide](docs/MULTI_SEED_GUIDE.md) for details.

### Single Experiment

Train LLaMA 3.2-1B on GSM8k-AUG:

```powershell
python train.py --config configs/llama1b_aug.yaml --use_wandb
```

Evaluate trained model:

```powershell
python evaluate.py `
    --checkpoint_dir outputs/best_checkpoint `
    --eval_dataset gsm8k `
    --output results/gsm8k.yaml
```

### Interactive Inference

Test your trained model:

```powershell
python inference.py `
    --checkpoint_dir outputs/best_checkpoint `
    --interactive
```

**Example**:
```
Question: What is 25% of 80?
Answer: 25% of 80 is 20.
Forward passes: 6.2
```

See [Scripts Overview](docs/SCRIPTS_OVERVIEW.md) for all available commands.

### Full Paper Replication

Run all experiments (3 seeds Ã— 4 configurations):

```powershell
# All Table 6 configurations
.\run_multi_seed.ps1 -Config llama1b_aug -Seeds 42,123,456
.\run_multi_seed.ps1 -Config llama1b_aug_nl -Seeds 42,123,456
.\run_multi_seed.ps1 -Config qwen05b_aug -Seeds 42,123,456
.\run_multi_seed.ps1 -Config llama3b_aug -Seeds 42,123,456
```

Expected time: **24-48 hours** on A100 40GB GPU.

This will:
1. Train LLaMA 3.2-1B on GSM8k-AUG (3 seeds)
2. Train LLaMA 3.2-1B on GSM8k-AUG-NL (3 seeds)
3. Train Qwen2.5-0.5B on GSM8k-AUG (3 seeds)
4. Train LLaMA 3.2-3B on GSM8k-AUG (3 seeds)
5. Evaluate all checkpoints
6. Aggregate results with mean Â± std

See [Multi-Seed Guide](docs/MULTI_SEED.md) for detailed instructions.

## ğŸ“Š Configuration Files

All configs strictly follow **Table 6** from the paper:

| Config | Model | Dataset | Î±â‚ | Î±â‚‚ | KV Loss | Layer-wise std | LR | Epochs |
|--------|-------|---------|----|----|---------|----------------|-----|--------|
| `llama1b_aug.yaml` | LLaMA-1B | AUG | 10 | 1 | Smooth L1 | âœ“ | 8e-4 | 10 |
| `llama1b_aug_nl.yaml` | LLaMA-1B | AUG-NL | 10 | 1 | MSE | âœ“ | 8e-4 | 10 |
| `qwen05b_aug.yaml` | Qwen-0.5B | AUG | 10 | 1 | MSE | âœ— | 5e-4 | 10 |
| `llama3b_aug.yaml` | LLaMA-3B | AUG | 20 | 2 | Smooth L1 | âœ— | 2e-4 | 5 |

## ğŸ—ï¸ Architecture

### Directory Structure

```
kava review/
â”œâ”€â”€ configs/              # Exact Table 6 configurations
â”‚   â”œâ”€â”€ llama1b_aug.yaml
â”‚   â”œâ”€â”€ llama1b_aug_nl.yaml
â”‚   â”œâ”€â”€ qwen05b_aug.yaml
â”‚   â””â”€â”€ llama3b_aug.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rkv_compression.py       # R-KV algorithm (Section 3.2)
â”‚   â”œâ”€â”€ losses.py                # KV + CODI losses (Section 3)
â”‚   â”œâ”€â”€ latent_reasoning.py      # PCCoT with Jacobi iterations
â”‚   â”œâ”€â”€ data_utils.py            # GSM8k-AUG dataset loading
â”‚   â”œâ”€â”€ evaluation_datasets.py   # GSM8k-Hard, SVAMP loaders
â”‚   â””â”€â”€ trainer.py               # Main training loop
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ MULTI_SEED.md        # Multi-seed experiments guide
â”‚   â”œâ”€â”€ INFERENCE.md         # Inference usage guide
â”‚   â”œâ”€â”€ QUICKSTART.md        # Quick start tutorial
â”‚   â””â”€â”€ PAPER_MAPPING.md     # Paper section â†’ code mapping
â”œâ”€â”€ train.py                 # Training entry point
â”œâ”€â”€ evaluate.py              # Evaluation script
â”œâ”€â”€ inference.py             # Interactive/batch inference
â”œâ”€â”€ run_multi_seed.py        # Multi-seed automation
â”œâ”€â”€ aggregate_results.py     # Results aggregation
â”œâ”€â”€ run_all_experiments.ps1  # Full replication script
â””â”€â”€ requirements.txt
```

### Scripts Overview

| Script | Purpose | When to Use |
|--------|---------|-------------|
| `train.py` | Single training run | One config, one seed |
| `evaluate.py` | Evaluate checkpoint on GSM8k/GSM8k-Hard/SVAMP | After training |
| `inference.py` | Interactive testing or batch inference | Test trained model |
| `run_multi_seed.py` | Automated multi-seed training & eval | Statistical significance |
| `aggregate_results.py` | Combine results into paper tables | After all experiments |
| `run_all_experiments.ps1` | Run all 4 configs Ã— 3 seeds | Full replication |

### Key Modules

**1. R-KV Compression (`rkv_compression.py`)**
```python
# Importance score: I = (1/N_A) Î£_j A_{j,i}
importance = compute_importance_score(attention, answer_idx, steps_idx)

# Redundancy score: R = softmax(-avg_cosine_similarity)
redundancy = compute_redundancy_score(keys)

# Mixed score: S = Î»*I + (1-Î»)*R
score = lambda * importance + (1 - lambda) * redundancy

# Select top-M tokens
compressed_kv = select_top_tokens(score, M=24)
```

**2. KV Distillation Loss (`losses.py`)**
```python
# L_KV = (1/2M) * (||K_t - K_s||_p + ||V_t - V_s||_p)
loss_kv = kv_distillation(
    teacher_kv_compressed,
    student_kv_latent,
    loss_type="smooth_l1",  # or "mse"
    layerwise_std=True
)
```

**3. KAVA Total Loss**
```python
# L_KAVA = -log p(A|Z,Q) - log p(A,C|Q) + Î±â‚*L_CODI + Î±â‚‚*L_KV
loss = (
    student_ce_loss +
    teacher_ce_loss +
    alpha1 * codi_loss +
    alpha2 * kv_loss
)
```

## ğŸ“ˆ Evaluation Metrics

### Accuracy
- **Exact Match (EM):** Extract numerical answer, compare with ground truth
- Report: mean Â± std over 3 seeds

### Efficiency
- **Forward Passes:** Count forward passes during inference
  - Latent reasoning: T iterations (3) + answer tokens
  - vs Full CoT: Much higher due to long reasoning chains

## ğŸ”¬ Datasets

Following **Appendix B** of the paper:

| Dataset | HuggingFace Path | Train | Val | Test | CoT Type |
|---------|------------------|-------|-----|------|----------|
| GSM8k-AUG | `whynlp/gsm8k-aug` | 385,620 | 500 | 1,319 | Equation-only |
| GSM8k-AUG-NL | `whynlp/gsm8k-aug-nl` | 385,620 | 500 | 1,319 | Natural language |

Evaluation datasets:
- GSM8k test (original)
- GSM8k-Hard (Gao et al.)
- SVAMP (Patel et al.)

## âš™ï¸ Hyperparameters (Table 6)

### LoRA Configuration (All Models)
```yaml
r: 128
alpha: 32
dropout: 0.1
target_modules: [q_proj, k_proj, v_proj, o_proj]
```

### Latent Reasoning (All Models)
```yaml
num_tokens: 24  # M
num_iterations: 3  # T (Jacobi)
```

### Training Configuration

**LLaMA 3.2-1B + AUG:**
- Learning rate: 8e-4
- Batch size: 128
- Weight decay: 0.1
- KV loss: Smooth L1
- Layer-wise std: True
- Î±â‚=10, Î±â‚‚=1, Î»=0.1
- Epochs: 10

**LLaMA 3.2-3B + AUG:**
- Learning rate: 2e-4 (lower for larger model)
- Î±â‚=20, Î±â‚‚=2 (stronger regularization)
- Epochs: 5 (fewer needed)

*(See configs/ for complete settings)*

## ğŸ” Implementation Notes

### What's Strictly Following the Paper

âœ… All hyperparameters from Table 6  
âœ… R-KV compression algorithm (Section 3.2)  
âœ… Loss formulations (Section 3)  
âœ… Dataset sizes and sources (Appendix B)  
âœ… Evaluation protocol (Section 4)  

### Engineering Choices (Not Specified in Paper)

âš ï¸ Exact HuggingFace checkpoint names (paper only mentions model families)  
âš ï¸ Prompt templates (paper says "follow CODI/PCCoT" but no exact strings)  
âš ï¸ Batch processing details  

These are common to all reproductions and don't affect core methodology.

## ğŸ“ Citation

If you use this implementation, please cite the original paper:

```bibtex
@article{shen2025kava,
  title={Latent Reasoning via Compressed KV-Cache Distillation},
  author={Shen and Wu},
  journal={arXiv preprint arXiv:2510.02312},
  year={2025}
}
```

## ğŸ“š Documentation

### ğŸŒŸ æ–°æ‰‹æŒ‡å—ï¼ˆå¿…è¯»ï¼‰

| æ–‡æ¡£ | è¯´æ˜ | é¢„è®¡æ—¶é—´ | æ¨èåº¦ |
|------|------|---------|--------|
| **[GETTING_STARTED_HPC.md](GETTING_STARTED_HPC.md)** | **HPC å®Œæ•´ä¸Šæ‰‹æŒ‡å—** | 30 åˆ†é’Ÿ | â­â­â­â­â­ |
| **[KAVA_MODEL_DOWNLOAD.md](KAVA_MODEL_DOWNLOAD.md)** | **æ¨¡å‹ä¸‹è½½æŒ‡å—** âš ï¸ å¿…è¯» | 17-100 åˆ†é’Ÿ | â­â­â­â­â­ |
| [HPC_REFERENCE.md](HPC_REFERENCE.md) | HPC å‘½ä»¤é€ŸæŸ¥ | æŒ‰éœ€æŸ¥é˜… | â­â­â­â­â­ |
| [REPRODUCTION_GUIDE.md](REPRODUCTION_GUIDE.md) | å®Œæ•´å¤ç°æµç¨‹ | 1 å°æ—¶ | â­â­â­â­â˜† |

âš ï¸ **é‡è¦è¯´æ˜**ï¼šHPC å…¬å…±æ¨¡å‹åº“ï¼ˆ`/home/share/models`ï¼‰ä¸­æ²¡æœ‰ KAVA æ‰€éœ€çš„ LLaMA 3.2 å’Œ Qwen 2.5 æ¨¡å‹ï¼Œéœ€è¦ä¸‹è½½åˆ°ä¸ªäººç›®å½•ã€‚è¯¦è§ [KAVA_MODEL_DOWNLOAD.md](KAVA_MODEL_DOWNLOAD.md)ã€‚

### ğŸ“– HPC éƒ¨ç½²æ–‡æ¡£

| æ–‡æ¡£ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| [HPC_MODELS_QUICKSTART.md](HPC_MODELS_QUICKSTART.md) | å…¬å…±æ¨¡å‹åº“é…ç½® | åŠ é€Ÿæ¨¡å‹ä¸‹è½½ |
| [SLURM_INTERACTIVE_GUIDE.md](SLURM_INTERACTIVE_GUIDE.md) | äº¤äº’å¼è°ƒè¯• | ä»£ç è°ƒè¯•ã€å¿«é€Ÿæµ‹è¯• |
| [SSH_PORT_FORWARDING.md](SSH_PORT_FORWARDING.md) | ç«¯å£æ˜ å°„ | TensorBoardã€Jupyter |
| [CONTAINER_QUICKSTART.md](CONTAINER_QUICKSTART.md) | å®¹å™¨åŒ–éƒ¨ç½² | Enroot/Docker |
| [CONDA_CUDA_GUIDE.md](CONDA_CUDA_GUIDE.md) | CUDA ç¯å¢ƒé…ç½® | ä¾èµ–é—®é¢˜æ’æŸ¥ |

### ğŸ“ æŠ€æœ¯æ–‡æ¡£

#### Getting Started
- **[Scripts Overview](docs/SCRIPTS_OVERVIEW.md)**: All commands and workflows â­ **Start here**
- **[Quick Validation Guide](docs/QUICK_VALIDATION.md)**: 7-step validation (2 min to 48 hrs)
- **[Multi-Seed Guide](docs/MULTI_SEED_GUIDE.md)**: Statistical experiments
- **[Inference Guide](docs/INFERENCE.md)**: Interactive and batch inference

#### Technical Details
- **[Training Guide](docs/TRAINING_GUIDE.md)**: Hyperparameters and optimization
- **[Evaluation Guide](docs/EVALUATION_GUIDE.md)**: Metrics and datasets
- **[Implementation Notes](docs/IMPLEMENTATION_NOTES.md)**: Design decisions
- **[Paper Mapping](docs/PAPER_MAPPING.md)**: Paper sections â†’ code

### ğŸ—ºï¸ æ–‡æ¡£è·¯çº¿å›¾

```
ä½ æ˜¯è°ï¼Ÿ
â”œâ”€ ğŸ†• ç¬¬ä¸€æ¬¡ä½¿ç”¨ HPC
â”‚   â””â”€> é˜…è¯» GETTING_STARTED_HPC.md
â”‚       â””â”€> 30 åˆ†é’Ÿé…ç½® â†’ 48 å°æ—¶å¾—åˆ°è®ºæ–‡ç»“æœ âœ“
â”‚
â”œâ”€ ğŸ”§ é‡åˆ°ç¯å¢ƒé—®é¢˜
â”‚   â”œâ”€> HPC_REFERENCE.mdï¼ˆå‘½ä»¤é€ŸæŸ¥ï¼‰
â”‚   â”œâ”€> CONDA_CUDA_GUIDE.mdï¼ˆCUDA é—®é¢˜ï¼‰
â”‚   â””â”€> HPC_MODELS_QUICKSTART.mdï¼ˆæ¨¡å‹ä¸‹è½½ï¼‰
â”‚
â”œâ”€ ğŸ› éœ€è¦è°ƒè¯•ä»£ç 
â”‚   â”œâ”€> SLURM_INTERACTIVE_GUIDE.mdï¼ˆäº¤äº’å¼ä¼šè¯ï¼‰
â”‚   â””â”€> SSH_PORT_FORWARDING.mdï¼ˆè¿œç¨‹ç›‘æ§ï¼‰
â”‚
â”œâ”€ ğŸ“Š æƒ³è¦æ·±å…¥äº†è§£
â”‚   â”œâ”€> REPRODUCTION_GUIDE.mdï¼ˆå®Œæ•´æµç¨‹ï¼‰
â”‚   â”œâ”€> docs/TRAINING_GUIDE.mdï¼ˆè®­ç»ƒç»†èŠ‚ï¼‰
â”‚   â””â”€> docs/PAPER_MAPPING.mdï¼ˆä»£ç å¯¹åº”ï¼‰
â”‚
â””â”€ ğŸš¢ å®¹å™¨åŒ–éƒ¨ç½²
    â””â”€> CONTAINER_QUICKSTART.mdï¼ˆEnroot/Dockerï¼‰
```

### ğŸ“‹ å¿«é€Ÿå‘½ä»¤å¤‡å¿˜

```bash
# === æ–°æ‰‹ä¸€é”®å¯åŠ¨ ===
./setup_hpc_models.sh        # é…ç½®ç¯å¢ƒ
./hpc_run_all.sh              # æäº¤æ‰€æœ‰å®éªŒ

# === ç›‘æ§ ===
squeue --me                   # æŸ¥çœ‹ä»»åŠ¡
tail -f logs/kava_*.out       # å®æ—¶æ—¥å¿—

# === ç»“æœ ===
python format_results.py      # ç”Ÿæˆ LaTeX è¡¨æ ¼
cat results/table1.tex        # æŸ¥çœ‹ç»“æœ
```

---

## ğŸ“š Documentation (English)### Reference
- **[Status Report](STATUS.md)**: Implementation completeness
- **[Checklist](docs/CHECKLIST.md)**: Feature tracking

## ğŸ¤ Contributing

This is a paper replication project. Contributions should:
- Maintain strict adherence to paper specifications
- Add missing evaluation datasets (GSM8k-Hard, SVAMP)
- Improve engineering efficiency without changing methodology
- Fix bugs in implementation

## ğŸ“§ Issues

If you find discrepancies between this implementation and the paper:
1. Check if it's in Table 6 or explicitly stated in the paper
2. Open an issue with paper section reference
3. Engineering choices (not in paper) are open for optimization

## ğŸ™ Acknowledgments

- Original KAVA paper authors (Shen & Wu)
- PCCoT paper (Wu et al., 2025)
- CODI paper (for hidden state distillation baseline)
- R-KV compression algorithm

---

**Status:** âœ… **Implementation Complete** with production-ready tools

**Latest Features:**
- âœ… Multi-seed automation with PowerShell (`run_multi_seed.ps1`)
- âœ… Statistical aggregation with LaTeX output (`aggregate_multi_seed.py`)
- âœ… Smoke test suite for quick validation (`smoke_test.py`)
- âœ… Enhanced answer extraction with 4-strategy matching
- âœ… Dual-format output (JSON + YAML) for all results
- âœ… Interactive and batch inference modes
- âœ… GSM8k-Hard and SVAMP dataset support
- âœ… Comprehensive documentation (15+ guides, ~80 pages)

**Quick Links:**
- ğŸ“– [Scripts Overview](docs/SCRIPTS_OVERVIEW.md) - All commands in one place
- âš¡ [Quick Validation](docs/QUICK_VALIDATION.md) - 2 min smoke test â†’ 48 hr full replication
- ğŸ² [Multi-Seed Guide](docs/MULTI_SEED_GUIDE.md) - Statistical experiments

**License:** MIT (implementation only; paper content Â© authors)
