# ç»™ HPC AI åŠ©æ‰‹çš„ç®€æ´æŒ‡ä»¤

## ğŸ¯ å¿«é€Ÿæ‰§è¡Œï¼ˆæ¨èæ–¹æ¡ˆï¼‰

### æ–¹æ¡ˆä¸€ï¼šå…ˆéªŒè¯åæäº¤ï¼ˆæœ€ç¨³å¦¥ï¼‰

```bash
# æ­¥éª¤ 1: éªŒè¯ä¿®å¤
cd "/home/rpwang/kava review"
bash INSTRUCTIONS_FOR_AI.sh
# é€‰æ‹© 'y' è¿è¡ŒéªŒè¯æµ‹è¯•

# æ­¥éª¤ 2: å•ä»»åŠ¡æµ‹è¯•
# é€‰æ‹© 'y' æäº¤å•ä»»åŠ¡æµ‹è¯•
# ç­‰å¾… 2-3 åˆ†é’ŸæŸ¥çœ‹æ—¥å¿—

# æ­¥éª¤ 3: ç¡®è®¤æˆåŠŸåæäº¤å…¨éƒ¨
bash submit_all_jobs.sh
```

### æ–¹æ¡ˆäºŒï¼šä¸€æ­¥åˆ°ä½ï¼ˆå¿«é€Ÿä½†éœ€è¦ç¡®è®¤ï¼‰

```bash
cd "/home/rpwang/kava review"
bash submit_all_jobs.sh
```

---

## ğŸ“‹ å·²å®Œæˆçš„ä¿®å¤

### âœ… ä¿®æ”¹ 1: é…ç½®æ–‡ä»¶ä½¿ç”¨æœ¬åœ°è·¯å¾„

æ‰€æœ‰é…ç½®æ–‡ä»¶å·²æ›´æ–°ï¼š
```yaml
# æ”¹å‰: "meta-llama/Llama-3.2-1B-Instruct"
# æ”¹å: "/home/share/models/Llama-3.2-1B-Instruct"
```

**æ–‡ä»¶åˆ—è¡¨**ï¼š
- `configs/llama1b_aug.yaml`
- `configs/llama1b_aug_nl.yaml`
- `configs/llama3b_aug.yaml`
- `configs/qwen05b_aug.yaml`

### âœ… ä¿®æ”¹ 2: SLURM è„šæœ¬å¼ºåˆ¶ç¦»çº¿

`submit_multi_seed.slurm` å·²æ·»åŠ ï¼š
```bash
export HUGGINGFACE_HUB_OFFLINE=1
export TRANSFORMERS_OFFLINE=1
export HF_HUB_OFFLINE=1
```

### âœ… æ–°å¢å·¥å…·

1. **quick_model_test.py** - è¯Šæ–­è„šæœ¬
   - éªŒè¯æ¨¡å‹æ˜¯å¦èƒ½ä»æœ¬åœ°åŠ è½½
   - æµ‹è¯• 3 ç§åŠ è½½æ–¹å¼
   - ç»™å‡ºæ˜ç¡®å»ºè®®

2. **FIX_NETWORK_ERROR.md** - å®Œæ•´æ–‡æ¡£
   - é—®é¢˜åˆ†æ
   - ä¿®å¤æ­¥éª¤
   - å¸¸è§é—®é¢˜ FAQ
   - é¢„æœŸç»“æœ

3. **INSTRUCTIONS_FOR_AI.sh** - äº¤äº’å¼æ‰§è¡Œè„šæœ¬
   - å¼•å¯¼å¼æ“ä½œ
   - éªŒè¯ â†’ æµ‹è¯• â†’ æäº¤
   - è‡ªåŠ¨æ£€æŸ¥æ—¥å¿—

---

## ğŸ” éªŒè¯å‘½ä»¤ï¼ˆæ¨èå…ˆè¿è¡Œï¼‰

```bash
cd "/home/rpwang/kava review"
source venv/bin/activate

# è®¾ç½®ç¯å¢ƒï¼ˆä¸ SLURM ä¸€è‡´ï¼‰
export HF_HOME=/home/share/models
export TRANSFORMERS_CACHE=/home/share/models
export HUGGINGFACE_HUB_OFFLINE=1
export TRANSFORMERS_OFFLINE=1

# è¿è¡Œè¯Šæ–­
python quick_model_test.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
âœ… æ¨èæ–¹æ¡ˆ: åœ¨é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨æœ¬åœ°è·¯å¾„
  âœ“ é¿å…ç½‘ç»œè®¿é—®
  âœ“ åŠ è½½é€Ÿåº¦æ›´å¿«
  âœ“ ä¸ä¾èµ–ç¼“å­˜å¸ƒå±€
```

---

## ğŸš€ æäº¤ä»»åŠ¡

### é€‰æ‹© A: å•ä»»åŠ¡æµ‹è¯•ï¼ˆæ¨èï¼‰

```bash
# æµ‹è¯• Qwen 0.5Bï¼ˆæœ€å°æœ€å¿«ï¼‰
sbatch --export=CONFIG=qwen05b_aug --array=0 submit_multi_seed.slurm

# ç­‰å¾… 2-3 åˆ†é’Ÿ
squeue --me

# æŸ¥çœ‹æ—¥å¿—ï¼ˆåº”è¯¥çœ‹åˆ°æ¨¡å‹åŠ è½½æˆåŠŸï¼‰
tail -n 100 outputs/logs/kava_qwen05b_aug_*.out
```

### é€‰æ‹© B: æäº¤å…¨éƒ¨ï¼ˆéªŒè¯é€šè¿‡åï¼‰

```bash
bash submit_all_jobs.sh
bash monitor_jobs.sh --auto
```

---

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

```bash
# è‡ªåŠ¨åˆ·æ–°ç›‘æ§ï¼ˆæ¯ 30 ç§’ï¼‰
bash monitor_jobs.sh --auto

# æŸ¥çœ‹é˜Ÿåˆ—
squeue --me

# æŸ¥çœ‹ä»»åŠ¡å†å²
sacct -u $USER -S today

# æŸ¥çœ‹æœ€æ–°æ—¥å¿—
ls -lht outputs/logs/ | head -20
tail -f outputs/logs/kava_*.out
```

---

## âœ… æˆåŠŸæ ‡å¿—

æ—¥å¿—åº”è¯¥æ˜¾ç¤ºï¼š
```
âœ“ Loading model from /home/share/models/Llama-3.2-1B-Instruct
âœ“ Model loaded successfully
âœ“ Training started
âœ“ Epoch 0 | Step 0 | Loss: 2.xxx
```

**ä¸åº”è¯¥çœ‹åˆ°**ï¼š
```
âœ— Network is unreachable
âœ— Cannot connect to huggingface.co
âœ— [Errno 101]
```

---

## ğŸ› å¦‚æœä»å¤±è´¥

æ£€æŸ¥æ¸…å•ï¼š
```bash
# 1. é…ç½®æ–‡ä»¶æ˜¯å¦ä½¿ç”¨æœ¬åœ°è·¯å¾„
grep "name:" configs/*.yaml

# åº”è¯¥çœ‹åˆ°ï¼š/home/share/models/...

# 2. SLURM è„šæœ¬æ˜¯å¦è®¾ç½®ç¦»çº¿
grep "OFFLINE" submit_multi_seed.slurm

# åº”è¯¥çœ‹åˆ°ï¼šHUGGINGFACE_HUB_OFFLINE=1

# 3. å…±äº«æ¨¡å‹æ˜¯å¦å®Œæ•´
ls -lh /home/share/models/Llama-3.2-1B-Instruct/
ls -lh /home/share/models/Qwen2.5-0.5B-Instruct/

# åº”è¯¥æœ‰ï¼šconfig.json, tokenizer.json, *.safetensors
```

---

## ğŸ“ æŠ¥å‘Šæ ¼å¼ï¼ˆå¦‚éœ€å¸®åŠ©ï¼‰

å¦‚æœé—®é¢˜æœªè§£å†³ï¼Œè¯·æä¾›ï¼š

1. **éªŒè¯è„šæœ¬è¾“å‡º**ï¼š
```bash
python quick_model_test.py > test_output.txt 2>&1
cat test_output.txt
```

2. **ä»»åŠ¡çŠ¶æ€**ï¼š
```bash
sacct -j <job_id> --format=JobID,JobName,State,ExitCode,Elapsed
```

3. **æœ€æ–°æ—¥å¿—**ï¼ˆå‰å 100 è¡Œï¼‰ï¼š
```bash
tail -n 100 outputs/logs/kava_*.out
tail -n 100 outputs/logs/kava_*.err
```

---

## æ ¸å¿ƒæ”¹å˜æ€»ç»“

| é—®é¢˜ | åŸå›  | ä¿®å¤ |
|------|------|------|
| Network unreachable | ä½¿ç”¨ HF repo ID | æ”¹ç”¨æœ¬åœ°è·¯å¾„ |
| å°è¯•è”ç½‘ | transformers é»˜è®¤è¡Œä¸º | å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ |
| å¤±è´¥è¿‡å¿« | ç½‘ç»œè¶…æ—¶é‡è¯• | ç«‹å³ä»æœ¬åœ°åŠ è½½ |

**å…³é”®ä¿®æ”¹**ï¼š
```diff
# configs/*.yaml
- name: "meta-llama/Llama-3.2-1B-Instruct"
+ name: "/home/share/models/Llama-3.2-1B-Instruct"

# submit_multi_seed.slurm
+ export HUGGINGFACE_HUB_OFFLINE=1
+ export TRANSFORMERS_OFFLINE=1
```

è¿™ç¡®ä¿ transformers åªä»æœ¬åœ°åŠ è½½ï¼Œä¸å°è¯•ä»»ä½•ç½‘ç»œè®¿é—®ï¼
