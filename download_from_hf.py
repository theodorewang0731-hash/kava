"""
åœ¨ HPC ç™»å½•èŠ‚ç‚¹ä¸‹è½½æ¨¡å‹å’Œæ•°æ®é›†
âš ï¸ æ³¨æ„: å¦‚æœ HPC å·²æœ‰å…±äº«æ¨¡å‹åº“ï¼Œæ— éœ€è¿è¡Œæ­¤è„šæœ¬ï¼
å…ˆè¿è¡Œ: bash check_hpc_models_availability.sh æ£€æŸ¥å…±äº«æ¨¡å‹

ç™»å½•èŠ‚ç‚¹æœ‰ç½‘ç»œè®¿é—®ï¼Œä¸‹è½½åˆ°ç”¨æˆ·ç›®å½•åè®¡ç®—èŠ‚ç‚¹å¯ä»¥ä½¿ç”¨ç¼“å­˜
"""

import os
import sys
from pathlib import Path
from huggingface_hub import snapshot_download

# ä½¿ç”¨ HF-Mirror é•œåƒåŠ é€Ÿä¸‹è½½ï¼ˆå¯é€‰ï¼‰
# å¦‚æœç›´è¿ HuggingFace é€Ÿåº¦æ…¢ï¼Œå–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Š
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

def check_hpc_shared_models():
    """æ£€æŸ¥ HPC å…±äº«æ¨¡å‹åº“"""
    hpc_models = Path("/home/share/models")
    
    if not hpc_models.exists():
        return False
    
    print("=" * 80)
    print("ğŸ” æ£€æµ‹åˆ° HPC å…±äº«æ¨¡å‹åº“: /home/share/models")
    print("=" * 80)
    
    # æ£€æŸ¥æ‰€éœ€æ¨¡å‹
    required_models = [
        "meta-llama/Llama-3.2-1B-Instruct",
        "meta-llama/Llama-3.2-3B-Instruct",
        "Qwen/Qwen2.5-0.5B-Instruct"
    ]
    
    all_found = True
    for model in required_models:
        model_path = hpc_models / f"models--{model.replace('/', '--')}"
        if model_path.exists():
            print(f"  âœ“ {model}")
        else:
            print(f"  âœ— {model} (æœªæ‰¾åˆ°)")
            all_found = False
    
    print()
    
    if all_found:
        print("âœ… æ‰€æœ‰æ¨¡å‹éƒ½åœ¨å…±äº«åº“ä¸­ï¼Œæ— éœ€ä¸‹è½½ï¼")
        print()
        print("è¯·ä½¿ç”¨ä»¥ä¸‹ç¯å¢ƒå˜é‡:")
        print("  export HF_HOME=/home/share/models")
        print("  export TRANSFORMERS_CACHE=/home/share/models")
        print("  export HUGGINGFACE_HUB_OFFLINE=1")
        print()
        print("æˆ–è¿è¡Œé…ç½®è„šæœ¬:")
        print("  bash simple_setup.sh")
        print()
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æ¨¡å‹ç¼ºå¤±ï¼Œå°†ä¸‹è½½åˆ°ä¸ªäººç¼“å­˜")
        print()
        return False

def download_models():
    """ä¸‹è½½æ‰€éœ€çš„æ¨¡å‹"""
    print("=" * 80)
    print("å¼€å§‹ä¸‹è½½æ¨¡å‹...")
    print("=" * 80)
    print()
    print("âš ï¸  æ³¨æ„: Llama æ¨¡å‹éœ€è¦ HuggingFace æˆæƒè®¿é—®")
    print("è¯·è®¿é—®ä»¥ä¸‹é“¾æ¥ç”³è¯·è®¿é—®æƒé™:")
    print("  https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct")
    print("  https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct")
    print()
    print("æˆæƒåï¼Œéœ€è¦è®¾ç½® HuggingFace token:")
    print("  export HF_TOKEN=your_token_here")
    print("=" * 80)
    print()
    
    models = [
        {
            "repo_id": "meta-llama/Llama-3.2-1B-Instruct",
            "local_dir": "./models/Llama-3.2-1B-Instruct"
        },
        {
            "repo_id": "meta-llama/Llama-3.2-3B-Instruct",
            "local_dir": "./models/Llama-3.2-3B-Instruct"
        },
        {
            "repo_id": "Qwen/Qwen2.5-0.5B-Instruct",
            "local_dir": "./models/Qwen2.5-0.5B-Instruct"
        }
    ]
    
    for model_info in models:
        print(f"\nä¸‹è½½æ¨¡å‹: {model_info['repo_id']}")
        print(f"ä¿å­˜è·¯å¾„: {model_info['local_dir']}")
        
        try:
            snapshot_download(
                repo_id=model_info['repo_id'],
                local_dir=model_info['local_dir'],
                local_dir_use_symlinks=False,  # ä¸ä½¿ç”¨ç¬¦å·é“¾æ¥
                resume_download=True  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            )
            print(f"âœ“ {model_info['repo_id']} ä¸‹è½½å®Œæˆ")
        except Exception as e:
            print(f"âœ— {model_info['repo_id']} ä¸‹è½½å¤±è´¥: {e}")
            print("  æç¤º: å¦‚æœæ˜¯ LLaMA æ¨¡å‹ï¼Œè¯·ç¡®ä¿å·²é€šè¿‡ HuggingFace æˆæƒ")


def download_datasets():
    """ä¸‹è½½æ‰€éœ€çš„æ•°æ®é›†"""
    print("\n" + "=" * 60)
    print("å¼€å§‹ä¸‹è½½æ•°æ®é›†...")
    print("=" * 60)
    
    datasets = [
        {
            "repo_id": "whynlp/gsm8k-aug",
            "local_dir": "./datasets/gsm8k-aug"
        },
        {
            "repo_id": "whynlp/gsm8k-aug-nl",
            "local_dir": "./datasets/gsm8k-aug-nl"
        },
        {
            "repo_id": "gsm8k",
            "local_dir": "./datasets/gsm8k"
        }
    ]
    
    for dataset_info in datasets:
        print(f"\nä¸‹è½½æ•°æ®é›†: {dataset_info['repo_id']}")
        print(f"ä¿å­˜è·¯å¾„: {dataset_info['local_dir']}")
        
        try:
            snapshot_download(
                repo_id=dataset_info['repo_id'],
                repo_type="dataset",
                local_dir=dataset_info['local_dir'],
                local_dir_use_symlinks=False,
                resume_download=True
            )
            print(f"âœ“ {dataset_info['repo_id']} ä¸‹è½½å®Œæˆ")
        except Exception as e:
            print(f"âœ— {dataset_info['repo_id']} ä¸‹è½½å¤±è´¥: {e}")


def check_hf_cache():
    """æ£€æŸ¥ HuggingFace ç¼“å­˜ç›®å½•"""
    cache_dir = os.path.expanduser("~/.cache/huggingface")
    print("\n" + "=" * 60)
    print(f"HuggingFace ç¼“å­˜ç›®å½•: {cache_dir}")
    
    if os.path.exists(cache_dir):
        try:
            import subprocess
            result = subprocess.run(
                ["du", "-sh", cache_dir],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0:
                print(f"ç¼“å­˜å¤§å°: {result.stdout.strip()}")
        except:
            print("æ— æ³•è·å–ç¼“å­˜å¤§å°")
    else:
        print("ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
    print("=" * 60)


if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           KAVA é¡¹ç›® HuggingFace èµ„æºä¸‹è½½å·¥å…·               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš ï¸  é‡è¦æç¤ºï¼š
  1. ä¼˜å…ˆæ£€æŸ¥ HPC å…±äº«æ¨¡å‹åº“ï¼ˆè¿è¡Œä¸‹é¢çš„æ£€æŸ¥ï¼‰
  2. å¦‚æœå…±äº«åº“æœ‰æ¨¡å‹ï¼Œåˆ™æ— éœ€ä¸‹è½½ï¼Œç›´æ¥ä½¿ç”¨ï¼
  3. Llama æ¨¡å‹éœ€è¦ HuggingFace æˆæƒè®¿é—®

ä½¿ç”¨æ–¹æ³•:
  1. ç›´è¿ä¸‹è½½:
     python download_from_hf.py
  
  2. ä½¿ç”¨é•œåƒä¸‹è½½ (æ¨èï¼Œå›½å†…æ›´å¿«):
     HF_ENDPOINT=https://hf-mirror.com python download_from_hf.py
  
  3. åªä¸‹è½½æ¨¡å‹:
     python download_from_hf.py --models-only
  
  4. åªä¸‹è½½æ•°æ®é›†:
     python download_from_hf.py --datasets-only

æ³¨æ„äº‹é¡¹:
  - è¯·åœ¨ HPC ç™»å½•èŠ‚ç‚¹è¿è¡Œï¼ˆæœ‰ç½‘ç»œè®¿é—®ï¼‰
  - LLaMA æ¨¡å‹éœ€è¦å…ˆåœ¨ HuggingFace ç”³è¯·æˆæƒ
  - éœ€è¦ HuggingFace token: huggingface-cli login
  - ä¸‹è½½å®Œæˆåï¼Œè®¡ç®—èŠ‚ç‚¹å¯é€šè¿‡ç¼“å­˜è®¿é—®
""")
    
    # =========================================================================
    # é¦–å…ˆæ£€æŸ¥ HPC å…±äº«æ¨¡å‹åº“
    # =========================================================================
    if check_hpc_shared_models():
        print("ğŸ‰ å»ºè®®ï¼šç›´æ¥ä½¿ç”¨å…±äº«æ¨¡å‹ï¼Œæ— éœ€ä¸‹è½½ï¼")
        print()
        response = input("æ˜¯å¦ä»è¦ä¸‹è½½åˆ°ä¸ªäººç¼“å­˜? (y/N): ").strip().lower()
        if response != 'y':
            print("\nâœ“ å·²å–æ¶ˆä¸‹è½½ã€‚è¯·ä½¿ç”¨å…±äº«æ¨¡å‹åº“è¿è¡Œè®­ç»ƒã€‚")
            sys.exit(0)
        print("\nâš ï¸  å°†ä¸‹è½½åˆ°ä¸ªäººç¼“å­˜...")
    
    # =========================================================================
    # ç»§ç»­ä¸‹è½½æµç¨‹
    # =========================================================================
    
    download_models_flag = True
    download_datasets_flag = True
    
    if "--models-only" in sys.argv:
        download_datasets_flag = False
    elif "--datasets-only" in sys.argv:
        download_models_flag = False
    
    # æ£€æŸ¥æ˜¯å¦è®¾ç½®äº†é•œåƒ
    if os.environ.get('HF_ENDPOINT'):
        print(f"âœ“ ä½¿ç”¨é•œåƒ: {os.environ['HF_ENDPOINT']}\n")
    else:
        print("âœ“ ä½¿ç”¨å®˜æ–¹æº: https://huggingface.co\n")
    
    # ä¸‹è½½èµ„æº
    if download_models_flag:
        download_models()
    
    if download_datasets_flag:
        download_datasets()
    
    # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
    check_hf_cache()
    
    print("\n" + "=" * 60)
    print("ä¸‹è½½ä»»åŠ¡å®Œæˆ!")
    print("=" * 60)
    print("""
åç»­æ­¥éª¤:
  1. æ£€æŸ¥ä¸‹è½½çš„æ–‡ä»¶æ˜¯å¦å®Œæ•´
  2. æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹å’Œæ•°æ®é›†è·¯å¾„:
     - configs/llama1b_aug.yaml
     - configs/llama1b_aug_nl.yaml
     - configs/llama3b_aug.yaml
     - configs/qwen05b_aug.yaml
  
  3. æäº¤è®­ç»ƒä»»åŠ¡:
     bash submit_all_jobs.sh
""")
