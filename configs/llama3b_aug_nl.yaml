# LLaMA3.2-3B-Instruct + GSM8k-AUG-NL (Natural Language CoT)
# Strictly following Table 6 in the paper

model:
  name: "/home/share/models/Llama-3.2-3B-Instruct"  # HPC 共享库本地路径
  type: "llama"
  
lora:
  r: 128  # Paper Table 6: LoRA rank = 128
  alpha: 32  # Paper Table 6: LoRA α = 32
  dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

latent:
  num_tokens: 24  # M = 24
  num_iterations: 3  # T = 3 (Jacobi iterations)

dataset:
  name: "whynlp/gsm8k-aug-nl"  # 优先使用本地 ./datasets/gsm8k-aug-nl
  train_size: 385620
  val_size: 500
  test_size: 1319
  cot_type: "natural_language"  # Natural language CoT

loss:
  alpha1_codi: 20.0  # Paper Table 6: α₁ = 20 for LLaMA-3B
  alpha2_kv: 2.0     # Paper Table 6: α₂ = 2 for LLaMA-3B
  kv_loss_type: "smooth_l1"  # Paper Table 6: Smooth L1 for LLaMA-3B on AUG-NL
  layerwise_std: false  # Paper Table 6: False for LLaMA-3B on AUG-NL
  use_projection: false  # Paper Table 6: False for LLaMA-3B on AUG-NL

rkv:
  lambda: 0.0  # Paper Table 6: λ = 0.0 for LLaMA-3B on AUG-NL

training:
  learning_rate: 2.0e-4  # Paper Table 6: LLaMA-3B = 2e-4
  lr_scheduler: "cosine"
  optimizer: "adamw"
  batch_size: 128
  weight_decay: 0.1
  gradient_clipping: 2.0
  epochs: 5  # Paper Table 6: LLaMA-3B epochs = 5
  warmup_ratio: 0.05
  save_steps: 1000
  eval_steps: 500
  logging_steps: 50

evaluation:
  datasets: ["gsm8k", "gsm8k-hard", "svamp"]
  temperature: 0.0
  top_p: 1.0
  max_new_tokens: 256

system:
  mixed_precision: "bf16"
  gradient_accumulation_steps: 2  # May need more for 3B
  num_workers: 4
  seed: 42  # Will run with 3 different seeds

