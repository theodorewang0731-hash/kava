# LLaMA3.2-1B-Instruct + GSM8k-AUG (Equation-only CoT)
# Strictly following Table 6 in the paper

model:
  name: "/home/share/models/Llama-3.2-1B-Instruct"  # HPC 共享库本地路径
  type: "llama"
  
lora:
  r: 128  # Paper Table 6: LoRA rank = 128
  alpha: 32  # Paper Table 6: LoRA α = 32
  dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

latent:
  num_tokens: 24  # M = 24
  num_iterations: 3  # T = 3 (Jacobi iterations)

dataset:
  name: "whynlp/gsm8k-aug"
  train_size: 385620
  val_size: 500
  test_size: 1319
  cot_type: "equation"  # Equation-only CoT

loss:
  alpha1_codi: 10.0  # α₁
  alpha2_kv: 1.0     # α₂
  kv_loss_type: "smooth_l1"  # Smooth L1 for LLaMA-1B on AUG
  layerwise_std: true
  use_projection: true

rkv:
  lambda: 0.1  # λ for importance vs redundancy

training:
  learning_rate: 8.0e-4  # Paper Table 6: LLaMA-1B + AUG = 8e-4
  lr_scheduler: "cosine"
  optimizer: "adamw"
  batch_size: 128
  weight_decay: 0.1
  gradient_clipping: 2.0
  epochs: 10  # Paper Table 6: LLaMA-1B epochs = 10
  warmup_ratio: 0.05
  save_steps: 1000
  eval_steps: 500
  logging_steps: 50

evaluation:
  datasets: ["gsm8k", "gsm8k-hard", "svamp"]
  temperature: 0.0
  top_p: 1.0
  max_new_tokens: 256

system:
  mixed_precision: "bf16"
  gradient_accumulation_steps: 1
  num_workers: 4
  seed: 42  # Will run with 3 different seeds
