# LLaMA3.2-1B-Instruct + GSM8k-AUG-NL (Natural Language CoT)
# Strictly following Table 6 in the paper

model:
  name: "/home/share/models/Llama-3.2-1B-Instruct"  # HPC 共享库本地路径
  type: "llama"
  
lora:
  r: 128  # Paper Table 6: LoRA rank = 128
  alpha: 32  # Paper Table 6: LoRA α = 32
  dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

latent:
  num_tokens: 24
  num_iterations: 3

dataset:
  name: "/home/rpwang/kava review/datasets/gsm8k-aug-nl"  # HPC 本地数据集路径
  train_size: 385620
  val_size: 500
  test_size: 1319
  cot_type: "natural_language"  # Natural language CoT

loss:
  alpha1_codi: 10.0
  alpha2_kv: 1.0
  kv_loss_type: "mse"  # MSE for LLaMA-1B on AUG-NL
  layerwise_std: true
  use_projection: true

rkv:
  lambda: 0.1

training:
  learning_rate: 8.0e-4  # Paper Table 6: LLaMA-1B + AUG-NL = 8e-4
  lr_scheduler: "cosine"
  optimizer: "adamw"
  batch_size: 128
  weight_decay: 0.1
  gradient_clipping: 2.0
  epochs: 10  # Paper Table 6: LLaMA-1B epochs = 10
  warmup_ratio: 0.05
  save_steps: 1000
  eval_steps: 500
  logging_steps: 50

evaluation:
  datasets: ["gsm8k", "gsm8k-hard", "svamp"]
  temperature: 0.0
  top_p: 1.0
  max_new_tokens: 256

system:
  mixed_precision: "bf16"
  gradient_accumulation_steps: 1
  num_workers: 4
  seed: 42
