# HPC æ–°æ‰‹ä¸Šæ‰‹æŒ‡å—

**ä»Žä¸Šä¼ é¡¹ç›®åˆ°ç”Ÿæˆè®ºæ–‡ç»“æžœçš„å®Œæ•´æµç¨‹**

---

## ðŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ åœ¨ **30 åˆ†é’Ÿå†…**å®Œæˆä»Žé›¶å¼€å§‹åˆ°æäº¤è®­ç»ƒä»»åŠ¡ï¼Œ**48 å°æ—¶åŽ**èŽ·å¾—è®ºæ–‡ Table 1 å’Œ Table 2 çš„å®Œæ•´ç»“æžœã€‚

**ç›®æ ‡**ï¼šä¸¥æ ¼å¤çŽ° KAVA è®ºæ–‡ç»“æžœï¼ˆGSM8kã€GSM8k-Hardã€SVAMP ä¸‰ä¸ªæ•°æ®é›†çš„å‡†ç¡®çŽ‡ï¼‰

---

## ðŸŽ¯ å¿«é€Ÿå¯¼èˆª

| é˜¶æ®µ | é¢„è®¡æ—¶é—´ | å…³é”®æ­¥éª¤ |
|------|---------|---------|
| **é˜¶æ®µ 1: ä¸Šä¼ é¡¹ç›®** | 5 åˆ†é’Ÿ | ä½¿ç”¨ scp/Git ä¸Šä¼ ä»£ç  |
| **é˜¶æ®µ 2: çŽ¯å¢ƒé…ç½®** | 15 åˆ†é’Ÿ | è¿è¡Œè‡ªåŠ¨é…ç½®è„šæœ¬ |
| **é˜¶æ®µ 3: æäº¤è®­ç»ƒ** | 5 åˆ†é’Ÿ | ä¸€é”®æäº¤æ‰€æœ‰å®žéªŒ |
| **é˜¶æ®µ 4: ç›‘æŽ§è¿›åº¦** | 48 å°æ—¶ | å®šæœŸæ£€æŸ¥æ—¥å¿— |
| **é˜¶æ®µ 5: ç”Ÿæˆç»“æžœ** | 5 åˆ†é’Ÿ | æ ¼å¼åŒ–ä¸º LaTeX è¡¨æ ¼ |

**æ€»è®¡**ï¼š30 åˆ†é’Ÿé…ç½® + 48 å°æ—¶è‡ªåŠ¨è¿è¡Œ

---

## ðŸš€ é˜¶æ®µ 1: ä¸Šä¼ é¡¹ç›®åˆ° HPCï¼ˆ5 åˆ†é’Ÿï¼‰

### æ–¹æ³• A: ä½¿ç”¨ scp ä¸Šä¼ ï¼ˆæŽ¨èï¼‰

```bash
# åœ¨æœ¬åœ°ç»ˆç«¯è¿è¡Œï¼ˆå‡è®¾é¡¹ç›®åœ¨ D:\kavaï¼‰
# Windows PowerShell
scp -r "D:\kava" username@hpc.example.edu:~/

# Linux/macOS
scp -r /path/to/kava username@hpc.example.edu:~/

# éªŒè¯ä¸Šä¼ 
ssh username@hpc.example.edu
ls -lh ~/kava
```

### æ–¹æ³• B: ä½¿ç”¨ Git å…‹éš†

```bash
# ç™»å½•åˆ° HPC
ssh username@hpc.example.edu

# å…‹éš†é¡¹ç›®
cd ~
git clone https://github.com/yourusername/kava.git
cd kava

# å¦‚æžœ GitHub è®¿é—®æ…¢ï¼Œä½¿ç”¨ä»£ç†ï¼ˆå‚è§åŽç»­ç« èŠ‚ï¼‰
```

### æ–¹æ³• C: VSCode Remote SSHï¼ˆæœ€æ–¹ä¾¿ï¼‰

1. å®‰è£… VSCode çš„ **Remote - SSH** æ‰©å±•
2. æŒ‰ `F1` â†’ è¾“å…¥ "Remote-SSH: Connect to Host"
3. è¾“å…¥ `username@hpc.example.edu`
4. æ‰“å¼€è¿œç¨‹ç›®å½• `/home/username/kava`
5. åœ¨ VSCode ä¸­ç›´æŽ¥ç¼–è¾‘å’ŒåŒæ­¥æ–‡ä»¶

**ç»“æžœæ£€æŸ¥**ï¼š
```bash
# ç™»å½• HPCï¼Œç¡®è®¤é¡¹ç›®ç»“æž„
ssh username@hpc.example.edu
cd ~/kava
ls -lh

# åº”è¯¥çœ‹åˆ°ï¼š
# configs/          - é…ç½®æ–‡ä»¶
# submit_multi_seed.slurm  - SLURM è„šæœ¬
# train.py          - è®­ç»ƒè„šæœ¬
# setup_hpc_models.sh  - è‡ªåŠ¨é…ç½®è„šæœ¬
# GETTING_STARTED_HPC.md  - æœ¬æŒ‡å—
```

---

## âš™ï¸ é˜¶æ®µ 2: çŽ¯å¢ƒé…ç½®ï¼ˆ15 åˆ†é’Ÿï¼‰

### Step 1: ä¸€é”®è‡ªåŠ¨é…ç½®ï¼ˆæŽ¨èï¼‰

âš ï¸ **æ³¨æ„**ï¼šç”±äºŽ HPC å…¬å…±æ¨¡åž‹åº“æ²¡æœ‰ KAVA æ‰€éœ€æ¨¡åž‹ï¼Œè·³è¿‡ `setup_hpc_models.sh`ï¼Œç›´æŽ¥é…ç½®ä¸ªäººçŽ¯å¢ƒã€‚

```bash
# ç™»å½•åˆ° HPC
ssh username@hpc.example.edu
cd ~/kava

# é…ç½®ä¸ªäºº HuggingFace ç¼“å­˜ç›®å½•
cat >> ~/.bashrc << 'EOF'
# HuggingFace ä¸ªäººç¼“å­˜ï¼ˆKAVA é¡¹ç›®ï¼‰
export HF_HOME=$HOME/.cache/huggingface
export TRANSFORMERS_CACHE=$HOME/.cache/huggingface
export HF_DATASETS_CACHE=$HOME/.cache/huggingface
EOF

# é‡æ–°åŠ è½½é…ç½®
source ~/.bashrc

# éªŒè¯çŽ¯å¢ƒå˜é‡
echo $HF_HOME
# åº”è¯¥è¾“å‡ºï¼š/home/username/.cache/huggingface
```

### Step 2: åˆ›å»º Python çŽ¯å¢ƒ

```bash
# åŠ è½½ Anaconda
module load anaconda3  # æˆ– miniconda3

# åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ
conda create -n kava python=3.10 -y
conda activate kava

# å®‰è£… PyTorchï¼ˆCUDA 11.8ï¼‰
pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…é¡¹ç›®ä¾èµ–
pip install -r requirements.txt

# å®‰è£…é¢å¤–ä¾èµ–
pip install peft wandb bitsandbytes

# éªŒè¯å®‰è£…
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
python -c "import transformers, peft; print('âœ“ All dependencies installed')"
```

### Step 3: ä¸‹è½½é¡¹ç›®æ‰€éœ€æ¨¡åž‹

âš ï¸ **é‡è¦**ï¼šHPC å…¬å…±æ¨¡åž‹åº“ï¼ˆ`/home/share/models`ï¼‰ä¸­**æ²¡æœ‰ KAVA é¡¹ç›®æ‰€éœ€çš„æ¨¡åž‹**ã€‚

**é¡¹ç›®éœ€è¦**ï¼š
- `meta-llama/Llama-3.2-1B-Instruct` âŒ ä¸åœ¨å…¬å…±åº“
- `meta-llama/Llama-3.2-3B-Instruct` âŒ ä¸åœ¨å…¬å…±åº“
- `Qwen/Qwen2.5-0.5B-Instruct` âŒ ä¸åœ¨å…¬å…±åº“

**å…¬å…±åº“æœ‰çš„**ï¼šLlama-2 ç³»åˆ—ã€Llama-30b/65bã€Qwen1.5 ç­‰ï¼ˆå¯ç”¨ `ls /home/share/models` æŸ¥çœ‹ï¼‰

#### æ–¹æ¡ˆ A: ä¸‹è½½åˆ°ä¸ªäººç›®å½•ï¼ˆæŽ¨èï¼‰

```bash
# é…ç½®ä¸ªäººç¼“å­˜ç›®å½•
export HF_HOME=$HOME/.cache/huggingface
export TRANSFORMERS_CACHE=$HOME/.cache/huggingface
export HF_DATASETS_CACHE=$HOME/.cache/huggingface

# å†™å…¥ ~/.bashrc
cat >> ~/.bashrc << 'EOF'
# HuggingFace ä¸ªäººç¼“å­˜
export HF_HOME=$HOME/.cache/huggingface
export TRANSFORMERS_CACHE=$HOME/.cache/huggingface
export HF_DATASETS_CACHE=$HOME/.cache/huggingface
EOF

source ~/.bashrc

# ä¸‹è½½æ¨¡åž‹ï¼ˆéœ€è¦ 10-30 åˆ†é’Ÿï¼Œå–å†³äºŽç½‘ç»œï¼‰
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct
huggingface-cli download meta-llama/Llama-3.2-3B-Instruct
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct

# æˆ–åœ¨è®­ç»ƒæ—¶è‡ªåŠ¨ä¸‹è½½ï¼ˆé¦–æ¬¡è¿è¡Œä¼šä¸‹è½½ï¼‰
python train.py --config configs/llama1b_aug.yaml
```

#### æ–¹æ¡ˆ B: ä½¿ç”¨ä»£ç†åŠ é€Ÿä¸‹è½½

å¦‚æžœ HuggingFace è®¿é—®è¾ƒæ…¢ï¼Œä½¿ç”¨æœ¬åœ°ä»£ç†ï¼š

```bash
# åœ¨æœ¬åœ°æœºå™¨å¯åŠ¨ä»£ç†ï¼ˆClash/Shadowrocketï¼‰
# ç„¶åŽåœ¨æœ¬åœ°ç»ˆç«¯å»ºç«‹åå‘éš§é“
ssh -N -R 55555:localhost:7890 username@hpc.example.edu &

# åœ¨ HPC ç»ˆç«¯é…ç½®ä»£ç†
export http_proxy=http://localhost:55555
export https_proxy=http://localhost:55555
export all_proxy=http://localhost:55555

# æµ‹è¯•è¿žæŽ¥
curl -I https://huggingface.co

# ä¸‹è½½æ¨¡åž‹ï¼ˆé€šè¿‡ä»£ç†åŠ é€Ÿï¼‰
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct
huggingface-cli download meta-llama/Llama-3.2-3B-Instruct
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct
```

#### æ–¹æ¡ˆ C: ä½¿ç”¨ HuggingFace é•œåƒ

```bash
# ä½¿ç”¨å›½å†…é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# ä¸‹è½½æ¨¡åž‹
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct
```

#### éªŒè¯æ¨¡åž‹ä¸‹è½½

```bash
# æ£€æŸ¥æ¨¡åž‹æ˜¯å¦ä¸‹è½½æˆåŠŸ
ls -lh ~/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct
ls -lh ~/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct
ls -lh ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct

# æµ‹è¯•åŠ è½½
python << EOF
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
print("âœ“ Successfully loaded from personal cache")
EOF
```

**é¢„è®¡ä¸‹è½½æ—¶é—´**ï¼š
- LLaMA 3.2-1B: ~5 GB â†’ çº¦ 10-15 åˆ†é’Ÿ
- LLaMA 3.2-3B: ~12 GB â†’ çº¦ 20-30 åˆ†é’Ÿ
- Qwen 2.5-0.5B: ~2 GB â†’ çº¦ 5-10 åˆ†é’Ÿ
- æ€»è®¡ï¼š~19 GB â†’ çº¦ 35-55 åˆ†é’Ÿï¼ˆé¦–æ¬¡ï¼‰

**ðŸ’¡ æç¤º**ï¼š
- æ¨¡åž‹ä¸‹è½½åŽä¼šæ°¸ä¹…ä¿å­˜åœ¨ `~/.cache/huggingface/`
- åŽç»­è®­ç»ƒæ— éœ€é‡å¤ä¸‹è½½
- å¦‚æžœé›†ç¾¤æœ‰å¤šä¸ªç”¨æˆ·éœ€è¦ï¼Œå¯ä»¥è¯·æ±‚ç®¡ç†å‘˜æ·»åŠ åˆ°å…¬å…±åº“

### Step 4: é…ç½® WandBï¼ˆå¯é€‰ï¼Œä½†æŽ¨èï¼‰

```bash
# å®‰è£…å¹¶ç™»å½• WandBï¼ˆç”¨äºŽè¿œç¨‹ç›‘æŽ§è®­ç»ƒï¼‰
wandb login

# è¾“å…¥ä½ çš„ API keyï¼ˆä»Ž https://wandb.ai/settings èŽ·å–ï¼‰
# ç²˜è´´åŽæŒ‰ Enter

# éªŒè¯
wandb status
```

**å¦‚æžœé‡åˆ°é—®é¢˜**ï¼š
- å‚è€ƒ [`HPC_REFERENCE.md`](HPC_REFERENCE.md) çš„"çŽ¯å¢ƒè®¾ç½®"ç« èŠ‚
- å‚è€ƒ [`REPRODUCTION_GUIDE.md`](REPRODUCTION_GUIDE.md) çš„"çŽ¯å¢ƒå‡†å¤‡"ç« èŠ‚
- å‚è€ƒ [`CONDA_CUDA_GUIDE.md`](CONDA_CUDA_GUIDE.md) çš„è¯¦ç»† CUDA é…ç½®

---

## ðŸŽ¬ é˜¶æ®µ 3: æäº¤è®­ç»ƒä»»åŠ¡ï¼ˆ5 åˆ†é’Ÿï¼‰

### å¿«é€Ÿæµ‹è¯•ï¼ˆå¯é€‰ï¼Œ2 åˆ†é’Ÿï¼‰

âš ï¸ **å…ˆç¡®è®¤æ¨¡åž‹å·²ä¸‹è½½**ï¼Œå¦åˆ™æµ‹è¯•ä¼šå°è¯•è‡ªåŠ¨ä¸‹è½½ã€‚

```bash
# æ£€æŸ¥æ¨¡åž‹æ˜¯å¦å­˜åœ¨
ls ~/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct

# å¦‚æžœæ¨¡åž‹ä¸å­˜åœ¨ï¼Œå…ˆä¸‹è½½ï¼ˆå‚è§é˜¶æ®µ 2 çš„ Step 3ï¼‰
# å¦‚æžœå·²ä¸‹è½½ï¼Œè¿è¡Œå¿«é€Ÿæµ‹è¯•
python smoke_test.py

# åº”è¯¥è¾“å‡ºï¼š
# âœ“ PyTorch loaded
# âœ“ Transformers loaded
# âœ“ CUDA available
# âœ“ All checks passed
```

### æ–¹æ¡ˆ A: ä¸€é”®æäº¤æ‰€æœ‰å®žéªŒï¼ˆæŽ¨èï¼‰

```bash
# ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬æäº¤ 4 ä¸ªé…ç½® Ã— 3 ä¸ªç§å­ = 12 ä¸ªä»»åŠ¡
chmod +x hpc_run_all.sh
./hpc_run_all.sh

# è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
# âœ… æäº¤ llama1b_augï¼ˆLLaMA 1B + GSM8k-AUGï¼‰
# âœ… æäº¤ llama1b_aug_nlï¼ˆLLaMA 1B + GSM8k-AUG-NLï¼‰
# âœ… æäº¤ qwen05b_augï¼ˆQwen 0.5B + GSM8k-AUGï¼‰
# âœ… æäº¤ llama3b_augï¼ˆLLaMA 3B + GSM8k-AUGï¼‰
# âœ… æ¯ä¸ªé…ç½® 3 ä¸ªç§å­ï¼ˆ42, 123, 456ï¼‰
# âœ… è‡ªåŠ¨èšåˆç»“æžœ

# é¢„è®¡å®Œæˆæ—¶é—´ï¼šçº¦ 36-48 å°æ—¶ï¼ˆå¹¶è¡Œè¿è¡Œï¼‰
```

### æ–¹æ¡ˆ B: å•ä¸ªé…ç½®æäº¤ï¼ˆå¿«é€ŸéªŒè¯ï¼‰

```bash
# ä»…æäº¤ LLaMA 1B é…ç½®ï¼ˆç”¨äºŽå¿«é€ŸéªŒè¯ï¼‰
sbatch --export=CONFIG=llama1b_aug submit_multi_seed.slurm

# æŸ¥çœ‹æäº¤çš„ä»»åŠ¡
squeue --me

# åº”è¯¥çœ‹åˆ° 3 ä¸ªä»»åŠ¡ï¼ˆ3 ä¸ªç§å­ï¼‰ï¼š
# JOBID  PARTITION  NAME           USER  ST  TIME  NODES
# 12345  compute    kava-multi-se  user  PD  0:00  1
# 12346  compute    kava-multi-se  user  PD  0:00  1
# 12347  compute    kava-multi-se  user  PD  0:00  1
```

### æ–¹æ¡ˆ C: äº¤äº’å¼æµ‹è¯•ï¼ˆè°ƒè¯•ç”¨ï¼‰

```bash
# ç”³è¯·å•å¡ GPU è¿›è¡Œäº¤äº’å¼æµ‹è¯•
srun --gres=gpu:a100-sxm4-80gb:1 --time=1:00:00 --pty bash -i

# æ¿€æ´»çŽ¯å¢ƒ
conda activate kava

# å¿«é€Ÿæµ‹è¯•è®­ç»ƒï¼ˆ1 ä¸ª epochï¼‰
python train.py \
    --config configs/llama1b_aug.yaml \
    --output_dir outputs/test \
    --epochs 1 \
    --seed 42

# å®ŒæˆåŽé€€å‡º
exit
```

**å¦‚æžœé‡åˆ°é—®é¢˜**ï¼š
- æ£€æŸ¥ SLURM è„šæœ¬æ˜¯å¦æ­£ç¡®ï¼š`cat submit_multi_seed.slurm`
- æ£€æŸ¥æ—¥å¿—ç›®å½•ï¼š`mkdir -p logs`
- å‚è€ƒ [`SLURM_INTERACTIVE_GUIDE.md`](SLURM_INTERACTIVE_GUIDE.md)

---

## ðŸ“Š é˜¶æ®µ 4: ç›‘æŽ§è®­ç»ƒè¿›åº¦ï¼ˆ48 å°æ—¶ï¼‰

### å®žæ—¶ç›‘æŽ§å‘½ä»¤

```bash
# 1. æŸ¥çœ‹ä»»åŠ¡é˜Ÿåˆ—
squeue --me

# è¾“å‡ºç¤ºä¾‹ï¼š
# JOBID  PARTITION  NAME           ST  TIME      NODES
# 12345  compute    kava-multi-se  R   2:30:15   1     ï¼ˆè¿è¡Œä¸­ï¼‰
# 12346  compute    kava-multi-se  PD  0:00      1     ï¼ˆæŽ’é˜Ÿä¸­ï¼‰

# 2. å®žæ—¶æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f logs/kava_12345_0.out

# åº”è¯¥çœ‹åˆ°è®­ç»ƒè¿›åº¦ï¼š
# Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [10:30<00:00]
# Loss: 2.345, Acc: 45.6%
# Saving checkpoint to outputs/llama1b_aug_seed_42/checkpoint_epoch_1

# 3. æŸ¥çœ‹é”™è¯¯æ—¥å¿—ï¼ˆå¦‚æžœæœ‰é—®é¢˜ï¼‰
tail -f logs/kava_12345_0.err

# 4. æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡çš„ç®€è¦çŠ¶æ€
watch -n 30 'squeue --me'  # æ¯ 30 ç§’åˆ·æ–°

# 5. ä½¿ç”¨ WandB è¿œç¨‹ç›‘æŽ§ï¼ˆæŽ¨èï¼‰
# åœ¨æµè§ˆå™¨æ‰“å¼€ https://wandb.ai/your-username/kava
# å®žæ—¶æŸ¥çœ‹ï¼šLoss æ›²çº¿ã€å‡†ç¡®çŽ‡ã€GPU ä½¿ç”¨çŽ‡
```

### å¸¸ç”¨ç›‘æŽ§å‘½ä»¤

```bash
# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µï¼ˆéœ€è¦åœ¨è®¡ç®—èŠ‚ç‚¹ä¸Šï¼‰
scir-watch -s                    # æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹ GPU çŠ¶æ€
scir-watch gpu06 gpustat         # æŸ¥çœ‹ç‰¹å®šèŠ‚ç‚¹

# æŸ¥çœ‹ä»»åŠ¡è¯¦ç»†ä¿¡æ¯
scontrol show job 12345

# æŸ¥çœ‹ä»»åŠ¡èµ„æºä½¿ç”¨
sacct -j 12345 --format=JobID,JobName,Elapsed,MaxRSS,MaxVMSize

# å–æ¶ˆä»»åŠ¡ï¼ˆå¦‚æžœéœ€è¦ï¼‰
scancel 12345                    # å–æ¶ˆå•ä¸ªä»»åŠ¡
scancel -u $USER                 # å–æ¶ˆæ‰€æœ‰ä»»åŠ¡

# æ£€æŸ¥è¾“å‡ºç›®å½•
ls -lh outputs/llama1b_aug_multi_seed/
# åº”è¯¥çœ‹åˆ°ï¼š
# seed_42/
# seed_123/
# seed_456/
```

### ä»»åŠ¡çŠ¶æ€è¯´æ˜Ž

| çŠ¶æ€ | å«ä¹‰ | æ“ä½œ |
|------|------|------|
| `PD` (Pending) | æŽ’é˜Ÿç­‰å¾…èµ„æº | ç­‰å¾…å³å¯ |
| `R` (Running) | æ­£åœ¨è¿è¡Œ | æŸ¥çœ‹æ—¥å¿—ç›‘æŽ§ |
| `CG` (Completing) | å³å°†å®Œæˆ | ç­‰å¾…å®Œæˆ |
| `CD` (Completed) | å·²å®Œæˆ | æ£€æŸ¥ç»“æžœ |
| `F` (Failed) | å¤±è´¥ | æŸ¥çœ‹é”™è¯¯æ—¥å¿— |
| `CA` (Cancelled) | å·²å–æ¶ˆ | é‡æ–°æäº¤ |

### é¢„è®¡æ—¶é—´çº¿

```
æäº¤åŽï¼š
â”œâ”€ 0-5 åˆ†é’Ÿï¼šæŽ’é˜Ÿç­‰å¾… GPU èµ„æºï¼ˆå–å†³äºŽé›†ç¾¤è´Ÿè½½ï¼‰
â”œâ”€ 5-10 åˆ†é’Ÿï¼šä»»åŠ¡å¼€å§‹è¿è¡Œï¼Œæ¨¡åž‹åˆå§‹åŒ–
â”œâ”€ 10 åˆ†é’Ÿ-12 å°æ—¶ï¼šè®­ç»ƒè¿›è¡Œä¸­ï¼ˆç¬¬ä¸€ä¸ª seedï¼‰
â”œâ”€ 12-24 å°æ—¶ï¼šç¬¬ä¸€ä¸ª seed å®Œæˆï¼Œå¼€å§‹ç¬¬äºŒä¸ª
â”œâ”€ 24-36 å°æ—¶ï¼šç¬¬äºŒä¸ª seed å®Œæˆï¼Œå¼€å§‹ç¬¬ä¸‰ä¸ª
â””â”€ 36-48 å°æ—¶ï¼šæ‰€æœ‰ç§å­å®Œæˆï¼Œè‡ªåŠ¨èšåˆç»“æžœ âœ“
```

**å¦‚æžœé‡åˆ°é—®é¢˜**ï¼š
- ä»»åŠ¡å¤±è´¥ â†’ æŸ¥çœ‹ `logs/kava_*_*.err`
- å‚è€ƒ [`HPC_REFERENCE.md`](HPC_REFERENCE.md) çš„"ç›‘æŽ§å’Œè°ƒè¯•"ç« èŠ‚
- å‚è€ƒ [`REPRODUCTION_GUIDE.md`](REPRODUCTION_GUIDE.md) çš„"æ•…éšœæŽ’é™¤"ç« èŠ‚

---

## ðŸ“ˆ é˜¶æ®µ 5: ç”Ÿæˆè®ºæ–‡ç»“æžœï¼ˆ5 åˆ†é’Ÿï¼‰

### Step 1: æ£€æŸ¥è¾“å‡ºæ–‡ä»¶

```bash
# è®­ç»ƒå®ŒæˆåŽï¼Œæ£€æŸ¥è¾“å‡ºç›®å½•
cd ~/kava
tree outputs/

# åº”è¯¥çœ‹åˆ°ï¼š
# outputs/
# â”œâ”€â”€ llama1b_aug_multi_seed/
# â”‚   â”œâ”€â”€ seed_42/
# â”‚   â”‚   â”œâ”€â”€ best_checkpoint/
# â”‚   â”‚   â”œâ”€â”€ results_gsm8k.yaml
# â”‚   â”‚   â”œâ”€â”€ results_gsm8k-hard.yaml
# â”‚   â”‚   â””â”€â”€ results_svamp.yaml
# â”‚   â”œâ”€â”€ seed_123/
# â”‚   â”‚   â””â”€â”€ ...
# â”‚   â”œâ”€â”€ seed_456/
# â”‚   â”‚   â””â”€â”€ ...
# â”‚   â””â”€â”€ aggregated_results.json  â† èšåˆç»“æžœ
# â”œâ”€â”€ llama1b_aug_nl_multi_seed/
# â”‚   â””â”€â”€ ...
# â”œâ”€â”€ qwen05b_aug_multi_seed/
# â”‚   â””â”€â”€ ...
# â””â”€â”€ llama3b_aug_multi_seed/
#     â””â”€â”€ ...
```

### Step 2: ç”Ÿæˆ LaTeX è¡¨æ ¼

```bash
# è¿è¡Œæ ¼å¼åŒ–è„šæœ¬
python format_results.py \
    --input outputs/*/aggregated_results.json \
    --output results/

# ç”Ÿæˆçš„æ–‡ä»¶ï¼š
# results/
# â”œâ”€â”€ table1.tex          â† è®ºæ–‡ Table 1ï¼ˆGSM8k-AUG ç»“æžœï¼‰
# â”œâ”€â”€ table2.tex          â† è®ºæ–‡ Table 2ï¼ˆGSM8k-AUG-NL ç»“æžœï¼‰
# â”œâ”€â”€ all_results.csv     â† CSV æ ¼å¼ï¼ˆä¾¿äºŽåˆ†æžï¼‰
# â””â”€â”€ summary.txt         â† ç»“æžœæ‘˜è¦
```

### Step 3: æŸ¥çœ‹ç»“æžœ

```bash
# æŸ¥çœ‹ LaTeX è¡¨æ ¼
cat results/table1.tex

# è¾“å‡ºç¤ºä¾‹ï¼š
# \begin{table}[t]
# \caption{Test accuracy (\%) on GSM8k, GSM8k-Hard, and SVAMP...}
# \begin{tabular}{llccc}
# \toprule
# Model & Dataset & GSM8k & GSM8k-Hard & SVAMP \\
# \midrule
# LLaMA-3.2-1B & GSM8k-AUG & 56.5 (0.4) & 34.2 (0.6) & 48.3 (0.5) \\
# LLaMA-3.2-1B & GSM8k-AUG-NL & 55.8 (0.5) & 33.7 (0.7) & 47.9 (0.6) \\
# Qwen-2.5-0.5B & GSM8k-AUG & 42.3 (0.8) & 28.1 (0.9) & 35.7 (1.1) \\
# LLaMA-3.2-3B & GSM8k-AUG & 67.2 (0.3) & 45.8 (0.5) & 58.9 (0.4) \\
# \bottomrule
# \end{tabular}
# \end{table}

# æŸ¥çœ‹ CSV æ ¼å¼
cat results/all_results.csv

# æŸ¥çœ‹ç»“æžœæ‘˜è¦
cat results/summary.txt
```

### Step 4: ä¸‹è½½ç»“æžœåˆ°æœ¬åœ°

```bash
# åœ¨æœ¬åœ°ç»ˆç«¯è¿è¡Œï¼ˆWindows PowerShellï¼‰
scp -r username@hpc.example.edu:~/kava/results/ D:\kava\results\

# Linux/macOS
scp -r username@hpc.example.edu:~/kava/results/ /path/to/local/kava/results/

# æˆ–ä½¿ç”¨ VSCode Remote SSH ç›´æŽ¥ä¸‹è½½
# å³é”® results/ â†’ Download
```

---

## âœ… å®Œæˆæ£€æŸ¥æ¸…å•

åœ¨è®ºæ–‡ä¸­ä½¿ç”¨ç»“æžœå‰ï¼Œè¯·ç¡®è®¤ï¼š

- [ ] æ‰€æœ‰ 4 ä¸ªé…ç½®éƒ½æˆåŠŸå®Œæˆï¼ˆllama1b_aug, llama1b_aug_nl, qwen05b_aug, llama3b_augï¼‰
- [ ] æ¯ä¸ªé…ç½®éƒ½æœ‰ 3 ä¸ªç§å­çš„ç»“æžœï¼ˆseed_42, seed_123, seed_456ï¼‰
- [ ] æ¯ä¸ªç§å­éƒ½åœ¨ 3 ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ˆGSM8k, GSM8k-Hard, SVAMPï¼‰
- [ ] `aggregated_results.json` åŒ…å«å‡å€¼å’Œæ ‡å‡†å·®
- [ ] `table1.tex` å’Œ `table2.tex` æ ¼å¼æ­£ç¡®
- [ ] ç»“æžœä¸Žè®ºæ–‡ä¸­çš„æ•°å€¼èŒƒå›´ä¸€è‡´ï¼ˆÂ±5% è¯¯å·®æ­£å¸¸ï¼‰

**ç»“æžœéªŒè¯**ï¼š
```bash
# æ£€æŸ¥æ‰€æœ‰ä»»åŠ¡å®Œæˆ
squeue --me  # åº”è¯¥ä¸ºç©ºï¼ˆæ‰€æœ‰ä»»åŠ¡å®Œæˆï¼‰

# æ£€æŸ¥ç»“æžœæ–‡ä»¶æ•°é‡
find outputs/ -name "results_*.yaml" | wc -l
# åº”è¯¥è¾“å‡ºï¼š36ï¼ˆ4 é…ç½® Ã— 3 ç§å­ Ã— 3 æ•°æ®é›†ï¼‰

# æ£€æŸ¥èšåˆç»“æžœ
ls -lh outputs/*/aggregated_results.json
# åº”è¯¥æœ‰ 4 ä¸ªæ–‡ä»¶

# æ£€æŸ¥ LaTeX è¡¨æ ¼
ls -lh results/*.tex
# åº”è¯¥æœ‰ table1.tex å’Œ table2.tex
```

---

## ðŸ”§ æ•…éšœæŽ’é™¤

### å¸¸è§é—®é¢˜é€ŸæŸ¥

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ | å‚è€ƒæ–‡æ¡£ |
|------|---------|---------|
| ä¸Šä¼ é¡¹ç›®å¤±è´¥ | æ£€æŸ¥ SSH é…ç½®ï¼Œä½¿ç”¨ VSCode Remote SSH | æœ¬æ–‡æ¡£"é˜¶æ®µ 1" |
| çŽ¯å¢ƒé…ç½®å¤±è´¥ | è¿è¡Œ `setup_hpc_models.sh`ï¼Œæ£€æŸ¥æ¨¡å—åŠ è½½ | [`HPC_REFERENCE.md`](HPC_REFERENCE.md) |
| æ¨¡åž‹ä¸‹è½½è¶…æ—¶ | ä½¿ç”¨å…¬å…±æ¨¡åž‹åº“ `/home/share/models` | [`HPC_MODELS_QUICKSTART.md`](HPC_MODELS_QUICKSTART.md) |
| ä»»åŠ¡æŽ’é˜Ÿå¤ªä¹… | æ£€æŸ¥é›†ç¾¤è´Ÿè½½ `sinfo -p compute` | [`SLURM_INTERACTIVE_GUIDE.md`](SLURM_INTERACTIVE_GUIDE.md) |
| ä»»åŠ¡å¤±è´¥ | æŸ¥çœ‹ `logs/kava_*.err`ï¼Œæ£€æŸ¥ GPU å†…å­˜ | [`REPRODUCTION_GUIDE.md`](REPRODUCTION_GUIDE.md) |
| GPU å†…å­˜ä¸è¶³ | å‡å° batch size æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ | [`REPRODUCTION_GUIDE.md`](REPRODUCTION_GUIDE.md) |
| ä»£ç†è®¾ç½® | ä½¿ç”¨æœ¬åœ°ä»£ç†åŠ é€Ÿä¸‹è½½ | [`SSH_PORT_FORWARDING.md`](SSH_PORT_FORWARDING.md) |

### è¯¦ç»†æ•…éšœæŽ’é™¤

#### é—®é¢˜ 1: ä»»åŠ¡ä¸€ç›´å¤„äºŽ PDï¼ˆæŽ’é˜Ÿï¼‰çŠ¶æ€

```bash
# æ£€æŸ¥åŽŸå› 
squeue --me --start

# å¦‚æžœæ˜¾ç¤ºèµ„æºä¸è¶³ï¼Œå¯ä»¥ï¼š
# 1. ç­‰å¾…ï¼ˆæŽ¨èï¼‰
# 2. å‡å°‘èµ„æºéœ€æ±‚ï¼ˆä¿®æ”¹ submit_multi_seed.slurmï¼‰
#    #SBATCH --mem=32G  ï¼ˆæ”¹ä¸º 32Gï¼‰
#    #SBATCH --time=24:00:00  ï¼ˆæ”¹ä¸º 24 å°æ—¶ï¼‰
```

#### é—®é¢˜ 2: è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

```bash
# æ£€æŸ¥ GPU ä½¿ç”¨çŽ‡
scir-watch gpu06 gpustat

# å¦‚æžœ GPU åˆ©ç”¨çŽ‡ä½Žï¼ˆ<50%ï¼‰ï¼Œå¯èƒ½æ˜¯ï¼š
# - Batch size å¤ªå° â†’ å¢žå¤§ batch_size
# - æ•°æ®åŠ è½½æ…¢ â†’ å¢žåŠ  num_workers
# - æ¨¡åž‹åœ¨ CPU â†’ æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨
```

#### é—®é¢˜ 3: ç»“æžœæ–‡ä»¶ç¼ºå¤±

```bash
# æ£€æŸ¥ä»»åŠ¡æ˜¯å¦çœŸçš„å®Œæˆ
sacct -j 12345 --format=JobID,State,ExitCode

# å¦‚æžœ ExitCode ä¸æ˜¯ 0:0ï¼Œè¯´æ˜Žæœ‰é”™è¯¯
tail -100 logs/kava_12345_0.err

# é‡æ–°è¿è¡Œå¤±è´¥çš„ä»»åŠ¡
sbatch --export=CONFIG=llama1b_aug submit_multi_seed.slurm
```

---

## ðŸ“š æŽ¨èé˜…è¯»é¡ºåº

### ç¬¬ä¸€æ¬¡ä½¿ç”¨ HPCï¼ˆå¿…è¯»ï¼‰

1. **æœ¬æ–‡æ¡£** (`GETTING_STARTED_HPC.md`) - è·Ÿéšæœ¬æŒ‡å—å®Œæˆæ‰€æœ‰æ­¥éª¤ â­â­â­â­â­
2. [`HPC_REFERENCE.md`](HPC_REFERENCE.md) - æµè§ˆ"å¿«é€Ÿå¼€å§‹"å’Œ"SLURM å‘½ä»¤"ç« èŠ‚ â­â­â­â­â˜†
3. [`REPRODUCTION_GUIDE.md`](REPRODUCTION_GUIDE.md) - äº†è§£è¯¦ç»†é…ç½®å’Œå‚æ•° â­â­â­â­â˜†

### é‡åˆ°é—®é¢˜æ—¶ï¼ˆæŒ‰éœ€é˜…è¯»ï¼‰

4. [`HPC_MODELS_QUICKSTART.md`](HPC_MODELS_QUICKSTART.md) - å…¬å…±æ¨¡åž‹åº“é…ç½® â­â­â­â˜†â˜†
5. [`SLURM_INTERACTIVE_GUIDE.md`](SLURM_INTERACTIVE_GUIDE.md) - äº¤äº’å¼è°ƒè¯• â­â­â­â˜†â˜†
6. [`SSH_PORT_FORWARDING.md`](SSH_PORT_FORWARDING.md) - è¿œç¨‹ç›‘æŽ§ï¼ˆTensorBoard/Jupyterï¼‰ â­â­â­â˜†â˜†
7. [`CONDA_CUDA_GUIDE.md`](CONDA_CUDA_GUIDE.md) - CUDA çŽ¯å¢ƒé—®é¢˜ â­â­â˜†â˜†â˜†

### é«˜çº§åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰

8. [`CONTAINER_QUICKSTART.md`](CONTAINER_QUICKSTART.md) - å®¹å™¨åŒ–éƒ¨ç½² â­â­â˜†â˜†â˜†
9. [`MULTI_SEED_GUIDE.md`](docs/MULTI_SEED_GUIDE.md) - å¤šç§å­å®žéªŒç»†èŠ‚ â­â­â˜†â˜†â˜†

---

## ðŸ’¡ æœ€ä½³å®žè·µ

1. **ä½¿ç”¨ VSCode Remote SSH**ï¼šæœ€æ–¹ä¾¿çš„æ–‡ä»¶åŒæ­¥å’Œç¼–è¾‘æ–¹å¼
2. **ä¼˜å…ˆä½¿ç”¨å…¬å…±æ¨¡åž‹åº“**ï¼š`/home/share/models` é¿å…é‡å¤ä¸‹è½½
3. **ä½¿ç”¨ WandB ç›‘æŽ§**ï¼šè¿œç¨‹æŸ¥çœ‹è®­ç»ƒè¿›åº¦ï¼Œæ— éœ€ç™»å½• HPC
4. **å®šæœŸæ£€æŸ¥æ—¥å¿—**ï¼š`tail -f logs/kava_*.out` åŠæ—¶å‘çŽ°é—®é¢˜
5. **å¤‡ä»½é‡è¦ç»“æžœ**ï¼šå®šæœŸä¸‹è½½ `outputs/` åˆ°æœ¬åœ°
6. **ä½¿ç”¨ tmux**ï¼šé•¿æ—¶é—´ä»»åŠ¡åœ¨åŽå°è¿è¡Œï¼Œé˜²æ­¢ SSH æ–­å¼€
7. **æ‰¹é‡æäº¤ä»»åŠ¡**ï¼šä½¿ç”¨ `hpc_run_all.sh` ä¸€æ¬¡æäº¤æ‰€æœ‰å®žéªŒ

---

## ðŸ“ž èŽ·å–å¸®åŠ©

### æ–‡æ¡£ç´¢å¼•

- **å¿«é€Ÿå¼€å§‹**: æœ¬æ–‡æ¡£
- **HPC å‘½ä»¤**: [`HPC_REFERENCE.md`](HPC_REFERENCE.md)
- **å®Œæ•´å¤çŽ°**: [`REPRODUCTION_GUIDE.md`](REPRODUCTION_GUIDE.md)
- **äº¤äº’è°ƒè¯•**: [`SLURM_INTERACTIVE_GUIDE.md`](SLURM_INTERACTIVE_GUIDE.md)
- **å…¬å…±æ¨¡åž‹**: [`HPC_MODELS_QUICKSTART.md`](HPC_MODELS_QUICKSTART.md)
- **ç«¯å£æ˜ å°„**: [`SSH_PORT_FORWARDING.md`](SSH_PORT_FORWARDING.md)
- **å®¹å™¨éƒ¨ç½²**: [`CONTAINER_QUICKSTART.md`](CONTAINER_QUICKSTART.md)

### å‘½ä»¤é€ŸæŸ¥

```bash
# === çŽ¯å¢ƒ ===
conda activate kava                    # æ¿€æ´»çŽ¯å¢ƒ
source ~/.bashrc                       # é‡æ–°åŠ è½½é…ç½®

# === æäº¤ä»»åŠ¡ ===
./hpc_run_all.sh                       # ä¸€é”®æäº¤æ‰€æœ‰å®žéªŒ
sbatch --export=CONFIG=llama1b_aug submit_multi_seed.slurm  # å•ä¸ªé…ç½®

# === ç›‘æŽ§ ===
squeue --me                            # æŸ¥çœ‹ä»»åŠ¡é˜Ÿåˆ—
tail -f logs/kava_*.out                # å®žæ—¶æ—¥å¿—
scir-watch -s                          # GPU çŠ¶æ€

# === ç»“æžœ ===
python format_results.py               # ç”Ÿæˆ LaTeX è¡¨æ ¼
cat results/table1.tex                 # æŸ¥çœ‹ç»“æžœ

# === æ¸…ç† ===
scancel -u $USER                       # å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
rm -rf outputs/test                    # åˆ é™¤æµ‹è¯•è¾“å‡º
```

---

## ðŸŽ‰ å®Œæˆ

æ­å–œï¼å¦‚æžœä½ å®Œæˆäº†æ‰€æœ‰æ­¥éª¤ï¼ŒçŽ°åœ¨åº”è¯¥æœ‰ï¼š

âœ… 4 ä¸ªæ¨¡åž‹é…ç½®çš„å®Œæ•´è®­ç»ƒç»“æžœ  
âœ… æ¯ä¸ªé…ç½® 3 ä¸ªç§å­çš„ç»Ÿè®¡æ•°æ®ï¼ˆå‡å€¼ Â± æ ‡å‡†å·®ï¼‰  
âœ… æ ¼å¼åŒ–çš„ LaTeX è¡¨æ ¼ï¼ˆå¯ç›´æŽ¥ç”¨äºŽè®ºæ–‡ï¼‰  
âœ… CSV æ ¼å¼çš„ç»“æžœï¼ˆä¾¿äºŽè¿›ä¸€æ­¥åˆ†æžï¼‰  

**ä¸‹ä¸€æ­¥**ï¼šå°† `results/table1.tex` å’Œ `results/table2.tex` å¤åˆ¶åˆ°ä½ çš„è®ºæ–‡ä¸­ï¼

---

**é¢„è®¡æ€»æ—¶é—´**ï¼š
- é…ç½®æ—¶é—´ï¼š30 åˆ†é’Ÿ
- è®­ç»ƒæ—¶é—´ï¼š36-48 å°æ—¶ï¼ˆè‡ªåŠ¨è¿è¡Œï¼Œæ— éœ€äººå·¥å¹²é¢„ï¼‰
- ç”Ÿæˆç»“æžœï¼š5 åˆ†é’Ÿ

**ç¥ä½ å®žéªŒé¡ºåˆ©ï¼** ðŸš€
